{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "runs successfully on kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install opencc-python-reimplemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import re\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from opencc import OpenCC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train bert classifier(forget to use chinese bert to train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "dataset_dir = \"./Data\"  # TODO. add it when add dataset\n",
    "cc = OpenCC(\"s2t\")\n",
    "relu = re.compile(r\"-?\\d*\\.?\\d+\")  # dealwith number\n",
    "\n",
    "# bert\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "learning_rate = 5e-5\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_save_path = \"./bert-model/bert-model.pt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle input data\n",
    "dataset = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for (dirpath, dirnames, filenames) in os.walk(dataset_dir):\n",
    "    if \"mapping.txt\" in filenames:\n",
    "        print(\"adding data...\", dirpath)\n",
    "\n",
    "        mapping = []\n",
    "\n",
    "        with open(os.path.join(dirpath, \"mapping.txt\"), \"r\", encoding=\"UTF-8\") as f:\n",
    "            for line in f.readlines():\n",
    "                words = line.split(\",\")\n",
    "                mapping.append((words[0][1:], words[1].split(\"\\n\")[0][1:-1]))\n",
    "        # print(mapping) # ok\n",
    "\n",
    "        # Table1.csv\n",
    "        df1 = pd.read_csv(os.path.join(dirpath, \"Table1.csv\"), low_memory=False)\n",
    "        df1 = df1.dropna()\n",
    "        # Table2.csv\n",
    "        df2 = pd.read_csv(os.path.join(dirpath, \"Table2.csv\"), low_memory=False)\n",
    "        df2 = df2.dropna()\n",
    "\n",
    "        for col1, col2 in mapping:\n",
    "            new_group = []\n",
    "\n",
    "            for item in df1[col1].values.tolist():\n",
    "                item = str(item)\n",
    "                #                 if (len(item) >= 4 and item[:4] == \"http\")or (len(item) >= 4 and item[:4] == \"http\"): # http\n",
    "                #                     item = \"http\"\n",
    "                #                 else: # remove number include date\n",
    "                #                     item = cc.convert(item)\n",
    "                #                     item = relu.sub('', item)\n",
    "                new_group.append(cc.convert(item))\n",
    "\n",
    "            for item in df2[col2].values.tolist():\n",
    "                item = str(item)\n",
    "                #                 if (len(item) >= 4 and item[:4] == \"http\")or (len(item) >= 4 and item[:4] == \"http\"): # http\n",
    "                #                     item = \"http\"\n",
    "                #                 else: # remove number include date\n",
    "                #                     item = cc.convert(item)\n",
    "                #                     item = relu.sub('', item)\n",
    "                new_group.append(cc.convert(item))\n",
    "\n",
    "            # print(new_group) # ok # WARNING string type, content\n",
    "            dataset.append(new_group)\n",
    "\n",
    "\n",
    "print(f\"import dataset in {time.time() - start} secs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset))\n",
    "\n",
    "model_classes = len(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert\n",
    "class BertDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        #   def __init__(self, dataset_path):\n",
    "        #     self.path = dataset_path\n",
    "        self.dataset = dataset  # include indexes & data_groups\n",
    "        self.idx = None\n",
    "        self.att = None\n",
    "        # self.sent_dict = {'Positive':0, 'Negative':1, 'Neutral':2}\n",
    "        # self.sent_dict_ = {0:'Positive', 1:'Negative', 2:'Neutral'}\n",
    "        self.label = None\n",
    "\n",
    "        self.process()\n",
    "\n",
    "    def process(self):\n",
    "        self.label = []\n",
    "        self.idx = []\n",
    "        self.att = []\n",
    "\n",
    "        for i in range(len(self.dataset)):\n",
    "            data = self.dataset[i]\n",
    "            for text in tqdm(data):\n",
    "                result = tokenizer(\n",
    "                    text,\n",
    "                    padding=\"max_length\",\n",
    "                    max_length=256,\n",
    "                    truncation=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                )\n",
    "                self.idx.append(result[\"input_ids\"])\n",
    "                self.att.append(result[\"attention_mask\"])\n",
    "                self.label.append(i)  # check via tags' index\n",
    "\n",
    "        return\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        idx = self.idx[index]  # .flatten()\n",
    "        att = self.att[index]  # .flatten()\n",
    "        y = self.label[index]\n",
    "        y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "        return {\"input_ids\": idx, \"attention_mask\": att, \"y\": y}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, num_class, model_name):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.num_class = num_class\n",
    "        self.model_name = model_name\n",
    "\n",
    "        self.bert = BertModel.from_pretrained(self.model_name)\n",
    "        self.dense = nn.Linear(self.bert.config.hidden_size, self.num_class)\n",
    "\n",
    "    # Define how your model pass data\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # keys(): ['last_hidden_state', 'pooler_output']\n",
    "\n",
    "        outputs = outputs[\"pooler_output\"]  # shape: (batch, hidden_size)\n",
    "        logits = self.dense(outputs)  # shape: (batch, num_class)\n",
    "\n",
    "        return logits, F.softmax(logits, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train bert setting\n",
    "train_set = BertDataset(dataset)\n",
    "train_loader = DataLoader(train_set, num_workers=2, batch_size=batch_size)\n",
    "\n",
    "model = BertClassifier(model_classes, model_name).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir bert-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train bert\n",
    "start = time.time()\n",
    "\n",
    "for ep in range(epochs):\n",
    "    print(\"======== Epoch {:} / {:} ========\".format(ep + 1, epochs))\n",
    "\n",
    "    model.train()\n",
    "    t0 = time.time()\n",
    "    tr_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        idx = batch[\"input_ids\"].squeeze(dim=1).to(device)\n",
    "        att = batch[\"attention_mask\"].squeeze(dim=1).to(device)\n",
    "        y = batch[\"y\"].to(device)\n",
    "\n",
    "        logits, prob = model.forward(idx, att)\n",
    "        # ---------------------------------\n",
    "        #  LOSS evaluation\n",
    "        # ---------------------------------\n",
    "        loss = loss_fn(logits, y)\n",
    "\n",
    "        # ---------------------------------\n",
    "        #  OPTIMIZATION\n",
    "        # ---------------------------------\n",
    "        # Calculate weigth updates\n",
    "        loss.backward()\n",
    "        # Apply modifications\n",
    "        optimizer.step()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_tr_loss = tr_loss / len(train_loader)\n",
    "    # Measure how long this epoch took.\n",
    "    #   training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Average training loss: {0:.3f}\".format(avg_tr_loss))\n",
    "    print(\"Training epcoh took: {:}\".format(time.time() - t0))\n",
    "\n",
    "print(f\"training bert in {time.time() - start} secs\")\n",
    "\n",
    "print(\"Save model...\")\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(\"model saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test classifier in all datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "print(len(dataset))\n",
    "# 70\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same above\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataset_dir = \"./Data\"\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "model_loc = \"./bert-model/bert-model.pt\"\n",
    "model_classes = 70  # len(dataset)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "ans_dir = \"./Result\"\n",
    "cc = OpenCC(\"s2t\")\n",
    "MAX_COMPARED_WORDS = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same above\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, num_class, model_name):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.num_class = num_class\n",
    "        self.model_name = model_name\n",
    "\n",
    "        self.bert = BertModel.from_pretrained(self.model_name)\n",
    "        self.dense = nn.Linear(self.bert.config.hidden_size, self.num_class)\n",
    "\n",
    "    # Define how your model pass data\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # keys(): ['last_hidden_state', 'pooler_output']\n",
    "\n",
    "        outputs = outputs[\"pooler_output\"]  # shape: (batch, hidden_size)\n",
    "        logits = self.dense(outputs)  # shape: (batch, num_class)\n",
    "\n",
    "        return logits, F.softmax(logits, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same above\n",
    "# load model\n",
    "model = BertClassifier(model_classes, model_name).to(device)\n",
    "model.load_state_dict(torch.load(model_loc, map_location=device))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classifier_val(text):\n",
    "    encoded_input = tokenizer(\n",
    "        text,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    logits, softmaxed = model(\n",
    "        encoded_input[\"input_ids\"].squeeze(dim=1).to(device),\n",
    "        encoded_input[\"attention_mask\"].squeeze(dim=1).to(device),\n",
    "    )\n",
    "\n",
    "    return softmaxed[0].cpu().detach().numpy()\n",
    "\n",
    "\n",
    "def cal_vec(word_list):\n",
    "    _list = []\n",
    "    counter = 0\n",
    "\n",
    "    for word in word_list:\n",
    "        _list.append(get_classifier_val(cc.convert(str(word))))\n",
    "\n",
    "        counter += 1\n",
    "        if counter >= MAX_COMPARED_WORDS:\n",
    "            break\n",
    "\n",
    "    vec = np.mean(np.array(_list), axis=0)\n",
    "\n",
    "    test = np.sum(vec)\n",
    "    test_times = 1 / test\n",
    "    vec *= test_times\n",
    "\n",
    "    return vec\n",
    "\n",
    "\n",
    "def findSuccessPair(df):\n",
    "    x_tags = df.columns\n",
    "    y_tags = df.index\n",
    "    predicted_num = min(len(x_tags), len(y_tags))\n",
    "    pairs = []\n",
    "    ran_x = []\n",
    "    ran_y = []\n",
    "\n",
    "    for _ in range(predicted_num):\n",
    "        max_val = 0\n",
    "        max_x = \"\"\n",
    "        max_y = \"\"\n",
    "\n",
    "        for x in x_tags:  # O(n^2)\n",
    "            if x in ran_x:\n",
    "                continue\n",
    "            for y in y_tags:\n",
    "                if y in ran_y:\n",
    "                    continue\n",
    "\n",
    "                if max_val < df[x][y]:\n",
    "                    max_val = df[x][y]\n",
    "                    max_x = x\n",
    "                    max_y = y\n",
    "        if max_val != 0:\n",
    "            pairs.append((max_x, max_y))\n",
    "            ran_x.append(max_x)\n",
    "            ran_y.append(max_y)\n",
    "\n",
    "    return pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (dirpath, dirnames, filenames) in os.walk(dataset_dir):\n",
    "    if \"mapping.txt\" in filenames:\n",
    "        print(\"accessing...\", dirpath)\n",
    "\n",
    "        mapping = []\n",
    "\n",
    "        with open(os.path.join(dirpath, \"mapping.txt\"), \"r\", encoding=\"UTF-8\") as f:\n",
    "            for line in f.readlines():\n",
    "                words = line.split(\",\")\n",
    "                mapping.append((words[0][1:], words[1].split(\"\\n\")[0][1:-1]))\n",
    "\n",
    "        dict1 = {}\n",
    "        dict2 = {}\n",
    "\n",
    "        # Table1.csv\n",
    "        df1 = pd.read_csv(os.path.join(dirpath, \"Table1.csv\"), low_memory=False)\n",
    "        df1 = df1.dropna(how=\"all\").fillna(\"\")\n",
    "        # Table2.csv\n",
    "        df2 = pd.read_csv(os.path.join(dirpath, \"Table2.csv\"), low_memory=False)\n",
    "        df2 = df2.dropna(how=\"all\").fillna(\"\")\n",
    "\n",
    "        # my generated pd\n",
    "        cols = []\n",
    "        for col in df1.columns:\n",
    "            cols.append(col)\n",
    "\n",
    "        rows = []\n",
    "        row_index = []\n",
    "\n",
    "        for row in df2.columns:\n",
    "            a_row = []\n",
    "\n",
    "            row_index.append(row)\n",
    "            for col in df1.columns:\n",
    "                if col not in dict1.keys():\n",
    "                    vec = cal_vec(df1[col])\n",
    "                    dict1[col] = vec\n",
    "                if row not in dict2.keys():\n",
    "                    vec = cal_vec(df2[row])\n",
    "                    dict2[row] = vec\n",
    "                val = np.dot(dict1[col], dict2[row])\n",
    "                a_row.append(val)\n",
    "            rows.append(a_row)\n",
    "\n",
    "        df = pd.DataFrame(rows, columns=cols, index=row_index)\n",
    "\n",
    "        # write pair mapping\n",
    "        if len(rows) < 10:\n",
    "            base = len(rows)\n",
    "            for i in range(base, 10):\n",
    "                rows.append([\"\" for _ in range(len(cols))])\n",
    "                row_index.append(\"\")\n",
    "\n",
    "        cols.append(\"\")\n",
    "        cols.append(\"最佳配對\")\n",
    "        rows[0].append(\"\")\n",
    "        rows[0].append(\"Table1\")\n",
    "        rows[1].append(\"\")\n",
    "        rows[1].append(\"Table2\")\n",
    "        rows[3].append(\"\")\n",
    "        rows[3].append(\"successful pair\")\n",
    "        rows[4].append(\"\")\n",
    "        rows[4].append(\"Table1\")\n",
    "        rows[5].append(\"\")\n",
    "        rows[5].append(\"Table2\")\n",
    "        rows[7].append(\"\")\n",
    "        rows[7].append(\"failed pair\")\n",
    "        rows[8].append(\"\")\n",
    "        rows[8].append(\"Table1\")\n",
    "        rows[9].append(\"\")\n",
    "        rows[9].append(\"Table2\")\n",
    "\n",
    "        ## alg to find pair: get col_name and row_name\n",
    "        pairs = findSuccessPair(df)\n",
    "\n",
    "        succ_pc = 0\n",
    "        fail_pc = 0\n",
    "        for i, (col_name, row_name) in enumerate(pairs):\n",
    "            # appendSuccessPair(): row[1], row[2], col_name, row_name, col_tag(string, f\"pair_{index}\")\n",
    "            cols.append(f\"pair_{i + 1}\")\n",
    "            rows[0].append(col_name)\n",
    "            rows[1].append(row_name)\n",
    "\n",
    "            if (col_name, row_name) in mapping:\n",
    "                succ_pc += 1\n",
    "                rows[3].append(f\"pair_{succ_pc}\")\n",
    "                rows[4].append(col_name)\n",
    "                rows[5].append(row_name)\n",
    "            else:\n",
    "                fail_pc += 1\n",
    "                rows[7].append(f\"pair_{fail_pc}\")\n",
    "                rows[8].append(col_name)\n",
    "                rows[9].append(row_name)\n",
    "\n",
    "        df = pd.DataFrame(rows, columns=cols, index=row_index)\n",
    "        df.to_csv(\n",
    "            os.path.join(\n",
    "                ans_dir,\n",
    "                \"result_{}.csv\".format(\n",
    "                    re.search(\"\\d+$\", dirpath.split(\"\\\\\")[-1]).group(0)\n",
    "                ),\n",
    "            )\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "03fdeaf8a03935a32ca4d1c8d8063f7bb894f5c3a4a58b053fc2a30007e4d3d9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
