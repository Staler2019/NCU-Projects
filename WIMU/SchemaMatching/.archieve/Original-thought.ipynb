{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install opencc-python-reimplemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import jieba\n",
    "import os\n",
    "import re\n",
    "import multiprocessing\n",
    "from gensim.corpora import WikiCorpus\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from opencc import OpenCC\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Chinese word2vec via wiki(cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://dumps.wikimedia.org/zhwiki/latest/zhwiki-latest-pages-articles.xml.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_filename = \"zhwiki-latest-pages-articles.xml.bz2\"\n",
    "output_filename = (\n",
    "    \"./word2vec-model/wiki-preprocessed-raw.txt\"  # TODO. modify it when add dataset\n",
    ")\n",
    "\n",
    "wiki = WikiCorpus(input_filename, dictionary={})\n",
    "cc = OpenCC(\"s2t\")\n",
    "relu = re.compile(r\"[ a-zA-Z]\")\n",
    "\n",
    "start = time.time()\n",
    "with open(output_filename, \"w\", encoding=\"UTF-8\") as output_f:\n",
    "    for index, text in enumerate(wiki.get_texts()):\n",
    "        # an article\n",
    "        art = \" \".join(text)\n",
    "\n",
    "        # simplify to traditional\n",
    "        art = cc.convert(art)\n",
    "\n",
    "        lines = []\n",
    "        for tts in text:\n",
    "            for tt in tts.split(\"\\n\"):\n",
    "                lines.append(tt)\n",
    "\n",
    "        # delete english char and blank\n",
    "        for line in lines:\n",
    "            line = relu.sub(\"\", line)\n",
    "            seg_list = jieba.cut(line)\n",
    "            seg_res = \" \".join(seg_list)\n",
    "            # output\n",
    "            output_f.write(seg_res)\n",
    "\n",
    "        if index % 10000 == 0:\n",
    "            print(\"Saved \" + str(index) + \"articles\")\n",
    "\n",
    "print(f\"Finished preprocessed data in {time.time() - start} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train # TODO. cannot train this on kaggle because of not enough ram(not gpu)\n",
    "input_text_path = (\n",
    "    \"./word2vec-model/wiki-preprocessed-raw.txt\"  # TODO. modify it when add dataset\n",
    ")\n",
    "output_model_path = (\n",
    "    \"./word2vec-model/wiki-out-model\"  # TODO. modify it when add dataset\n",
    ")\n",
    "sentences = LineSentence(input_text_path)  # 將剛剛寫的檔案轉換成 iterable\n",
    "\n",
    "print(\"Training word2vec model\")\n",
    "# model = Word2Vec(sentences, vector_size=256, epochs=8, sg=0, window=10, workers=24)\n",
    "model = Word2Vec(sentences, workers=multiprocessing.cpu_count())\n",
    "# model = Word2Vec(sentences, size=4,window=5, min_count=5, workers=multiprocessing.cpu_count())\n",
    "print(\"Save the model...\")\n",
    "model.save(output_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "model = Word2Vec.load(\n",
    "    \"./word2vec-model/wiki-out-model\"\n",
    ")  # TODO. modify it when add dataset\n",
    "result = model.wv.most_similar(\"男人\")\n",
    "for e in result:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train bert classifier(forget to use chinese bert to train)\n",
    "\n",
    "- `Difference` use word2vec to grouping tags\n",
    "  - difficulty\n",
    "    - wiki's chinese data is too small\n",
    "    - TA's tags are not in word set that can't be compared(no effort)\n",
    "    - forget to remember O() that is would cause about O(n^2) to finish it, which is a long time\n",
    "- `Difference` handle with http(s), numbers(include date) before training\n",
    "  - difficulty\n",
    "    - after `re` many col turns into `''` that I have no time to test that is it better than not using `re`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "dataset_dir = \"./Data\"  # TODO. add it when add dataset\n",
    "cc = OpenCC(\"s2t\")\n",
    "relu = re.compile(r\"-?\\d*\\.?\\d+\")  # dealwith number\n",
    "\n",
    "# word2vec\n",
    "word2vec_model = Word2Vec.load(\n",
    "    \"./word2vec-model/wiki-out-model\"\n",
    ")  # TODO. modify it when add dataset\n",
    "word_set = set(word2vec_model.wv.index_to_key)\n",
    "\n",
    "# bert\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "learning_rate = 5e-5\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec # TODO. has no function\n",
    "def count_similarity(word1, word2):\n",
    "    if word1 == word2:\n",
    "        return 1\n",
    "    elif word1 in word_set and word2 in word_set:\n",
    "        return word2vec_model.wv.similarity(word1, word2)\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle input data\n",
    "dataset = []\n",
    "tags = []\n",
    "\n",
    "\n",
    "def add_dataset(tag_idx, df1, df2, col1, col2):\n",
    "    for item in df1[col1].values.tolist():\n",
    "        item = str(item)\n",
    "        # get out http(s) => left only http\n",
    "        if (len(item) >= 4 and item[:4] == \"http\") or (\n",
    "            len(item) >= 4 and item[:4] == \"http\"\n",
    "        ):  # http\n",
    "            item = \"http\"\n",
    "        else:  # get out number, date => left only without numbers\n",
    "            item = cc.convert(item)\n",
    "            item = relu.sub(\"\", item)\n",
    "\n",
    "        dataset[tag_idx].append(item)\n",
    "\n",
    "    for item in df2[col2].values.tolist():\n",
    "        item = str(item)\n",
    "        # get out http(s) => left only http\n",
    "        if (len(item) >= 4 and item[:4] == \"http\") or (\n",
    "            len(item) >= 4 and item[:4] == \"http\"\n",
    "        ):  # http\n",
    "            item = \"http\"\n",
    "        else:  # get out number, date => left only without numbers\n",
    "            item = cc.convert(item)\n",
    "            item = relu.sub(\"\", item)\n",
    "\n",
    "        dataset[tag_idx].append(item)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "for (dirpath, dirnames, filenames) in os.walk(dataset_dir):\n",
    "    if \"mapping.txt\" in filenames:\n",
    "        print(\"adding data...\", dirpath)\n",
    "\n",
    "        with open(os.path.join(dirpath, \"mapping.txt\"), \"r\", encoding=\"UTF-8\") as f:\n",
    "            # Table1.csv\n",
    "            df1 = pd.read_csv(os.path.join(dirpath, \"Table1.csv\"))\n",
    "            df1 = df1.dropna()\n",
    "            # Table2.csv\n",
    "            df2 = pd.read_csv(os.path.join(dirpath, \"Table2.csv\"))\n",
    "            df2 = df2.dropna()\n",
    "\n",
    "            for line in f.readlines():\n",
    "                words = line.split(\",\")\n",
    "                word1 = words[0][1:]\n",
    "                word2 = words[1].split(\"\\n\")[0][1:-1]\n",
    "                cc_word1 = cc.convert(word1)\n",
    "                cc_word2 = cc.convert(word2)\n",
    "\n",
    "                # find similarity\n",
    "                max_avg = 0\n",
    "                max_idx = -1\n",
    "\n",
    "                for i in range(len(tags)):\n",
    "                    sim = 0\n",
    "\n",
    "                    for tag in tags[i]:\n",
    "                        sim1 = count_similarity(tag, cc_word1)\n",
    "                        sim2 = count_similarity(tag, cc_word2)\n",
    "                        sim += (sim1 + sim2) / 2\n",
    "\n",
    "                    sim /= len(tags[i])\n",
    "                    if max_avg > sim:\n",
    "                        max_avg = sim\n",
    "                        max_idx = i\n",
    "\n",
    "                if max_avg >= 0.55:\n",
    "                    # add to same tag\n",
    "                    tags[max_idx].append(cc_word1)\n",
    "                    tags[max_idx].append(cc_word2)\n",
    "                    add_dataset(max_idx, df1, df2, word1, word2)\n",
    "                else:\n",
    "                    tags.append([cc_word1, cc_word2])\n",
    "                    dataset.append([])\n",
    "                    add_dataset(len(tags) - 1, df1, df2, word1, word2)\n",
    "\n",
    "print(f\"import dataset in {time.time() - start} secs\")\n",
    "print(\"tag_grouping\")\n",
    "print(tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert\n",
    "class BertDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        #   def __init__(self, dataset_path):\n",
    "        #     self.path = dataset_path\n",
    "        self.dataset = dataset  # include indexes & data_groups\n",
    "        self.idx = None\n",
    "        self.att = None\n",
    "        # self.sent_dict = {'Positive':0, 'Negative':1, 'Neutral':2}\n",
    "        # self.sent_dict_ = {0:'Positive', 1:'Negative', 2:'Neutral'}\n",
    "        self.label = None\n",
    "\n",
    "        self.process()\n",
    "\n",
    "    def process(self):\n",
    "        self.label = []\n",
    "        self.idx = []\n",
    "        self.att = []\n",
    "\n",
    "        for i in range(len(self.dataset)):\n",
    "            data = self.dataset[i]\n",
    "            for text in tqdm(data):\n",
    "                result = tokenizer(\n",
    "                    text,\n",
    "                    padding=\"max_length\",\n",
    "                    max_length=256,\n",
    "                    truncation=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                )\n",
    "                self.idx.append(result[\"input_ids\"])\n",
    "                self.att.append(result[\"attention_mask\"])\n",
    "                self.label.append(i)  # check via tags' index\n",
    "\n",
    "        return\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        idx = self.idx[index]  # .flatten()\n",
    "        att = self.att[index]  # .flatten()\n",
    "        y = self.label[index]\n",
    "        y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "        return {\"input_ids\": idx, \"attention_mask\": att, \"y\": y}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, num_class, model_name):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.num_class = num_class\n",
    "        self.model_name = model_name\n",
    "\n",
    "        self.bert = BertModel.from_pretrained(self.model_name)\n",
    "        self.dense = nn.Linear(self.bert.config.hidden_size, self.num_class)\n",
    "\n",
    "    # Define how your model pass data\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # keys(): ['last_hidden_state', 'pooler_output']\n",
    "\n",
    "        outputs = outputs[\"pooler_output\"]  # shape: (batch, hidden_size)\n",
    "        logits = self.dense(outputs)  # shape: (batch, num_class)\n",
    "\n",
    "        return logits, F.softmax(logits, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train bert setting\n",
    "train_set = BertDataset(dataset)\n",
    "train_loader = DataLoader(train_set, num_workers=2, batch_size=batch_size)\n",
    "\n",
    "model = BertClassifier(len(dataset), model_name).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train bert\n",
    "start = time.time()\n",
    "\n",
    "for ep in range(epochs):\n",
    "    print(\"======== Epoch {:} / {:} ========\".format(ep + 1, epochs))\n",
    "\n",
    "    model.train()\n",
    "    t0 = time.time()\n",
    "    tr_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        idx = batch[\"input_ids\"].squeeze(dim=1).to(device)\n",
    "        att = batch[\"attention_mask\"].squeeze(dim=1).to(device)\n",
    "        y = batch[\"y\"].to(device)\n",
    "\n",
    "        logits, prob = model.forward(idx, att)\n",
    "        # ---------------------------------\n",
    "        #  LOSS evaluation\n",
    "        # ---------------------------------\n",
    "        loss = loss_fn(logits, y)\n",
    "\n",
    "        # ---------------------------------\n",
    "        #  OPTIMIZATION\n",
    "        # ---------------------------------\n",
    "        # Calculate weigth updates\n",
    "        loss.backward()\n",
    "        # Apply modifications\n",
    "        optimizer.step()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_tr_loss = tr_loss / len(train_loader)\n",
    "    # Measure how long this epoch took.\n",
    "    #   training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Average training loss: {0:.3f}\".format(avg_tr_loss))\n",
    "    print(\"Training epcoh took: {:}\".format(time.time() - t0))\n",
    "\n",
    "print(f\"training bert in {time.time() - start} secs\")\n",
    "\n",
    "model_save_path = \"./bert-model/bert-model.pt\"\n",
    "print(\"Save model...\")\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(\"model saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test classifier in all datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "print(len(dataset))\n",
    "# 70\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same above\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataset_dir = \"./Data\"\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same above\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, num_class, model_name):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.num_class = num_class\n",
    "        self.model_name = model_name\n",
    "\n",
    "        self.bert = BertModel.from_pretrained(self.model_name)\n",
    "        self.dense = nn.Linear(self.bert.config.hidden_size, self.num_class)\n",
    "\n",
    "    # Define how your model pass data\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # keys(): ['last_hidden_state', 'pooler_output']\n",
    "\n",
    "        outputs = outputs[\"pooler_output\"]  # shape: (batch, hidden_size)\n",
    "        logits = self.dense(outputs)  # shape: (batch, num_class)\n",
    "\n",
    "        return logits, F.softmax(logits, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same above\n",
    "# load model\n",
    "model = BertClassifier(len(dataset), model_name).to(device)\n",
    "model.load_state_dict(torch.load(\"./bert-model/bert-model.pt\", map_location=device))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classifier_val(text):\n",
    "    encoded_input = tokenizer(\n",
    "        text,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    logits, softmaxed = model(\n",
    "        encoded_input[\"input_ids\"].squeeze(dim=1).to(device),\n",
    "        encoded_input[\"attention_mask\"].squeeze(dim=1).to(device),\n",
    "    )\n",
    "\n",
    "    return softmaxed[0].detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
