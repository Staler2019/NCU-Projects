2022-05-11 03:00:21
++++++++++++++++++++++++++++++++++++++++CONFIGURATION SUMMARY++++++++++++++++++++++++++++++++++++++++
 Status:
     mode                 : train
 ++++++++++++++++++++++++++++++++++++++++
 Datasets:
     datasets         fold: data/example_datasets_msra
     train            file: train.txt
     validation       file: dev.txt
     vocab             dir: data/example_datasets_msra/vocabs
     delimiter            : b
     checkpoints       dir: checkpoints/datasets_bert-bilsm
     log               dir: data/example_datasets_msra/logs
 ++++++++++++++++++++++++++++++++++++++++
Labeling Scheme:
     label          scheme: BIO
     label           level: 2
     suffixes             : ['ORG', 'PERSON', 'LOC']
     measuring     metrics: ['precision', 'recall', 'f1']
 ++++++++++++++++++++++++++++++++++++++++
Model Configuration:
     embedding         dim: 768
     max  sequence  length: 128
     hidden            dim: 200
     CUDA  VISIBLE  DEVICE: 0
     seed                 : 42
 ++++++++++++++++++++++++++++++++++++++++
 Training Settings:
     epoch                : 30
     batch            size: 8
     dropout              : 0.1
     learning         rate: 2e-05
     optimizer            : AdamW
     checkpoint       name: model
     max       checkpoints: 30
     print       per_batch: 20
     is     early     stop: False
     patient              : 10
++++++++++++++++++++++++++++++++++++++++CONFIGURATION SUMMARY END++++++++++++++++++++++++++++++++++++++++
loading label vocab...
mode: train
loading data...
loading data...
++++++++++++++++++++training starting++++++++++++++++++++
loading model
epoch:1/30
training batch:    20, loss: 92.25249, precision: 0.007 recall: 0.071 f1: 0.013 
training batch:    40, loss: 53.42588, precision: 0.014 recall: 0.062 f1: 0.022 
training batch:    60, loss: 75.61108, precision: 0.000 recall: 0.000 f1: -1.000 
training batch:    80, loss: 72.71632, precision: 0.000 recall: 0.000 f1: -1.000 
training batch:   100, loss: 52.80606, precision: 0.020 recall: 0.091 f1: 0.033 
training batch:   120, loss: 76.68044, precision: 0.000 recall: 0.000 f1: -1.000 
training batch:   140, loss: 49.77887, precision: 0.000 recall: 0.000 f1: -1.000 
training batch:   160, loss: 40.54791, precision: 0.000 recall: 0.000 f1: -1.000 
training batch:   180, loss: 27.03266, precision: 0.000 recall: 0.000 f1: -1.000 
training batch:   200, loss: 29.61011, precision: -1.000 recall: 0.000 f1: -1.000 
training batch:   220, loss: 21.48769, precision: -1.000 recall: 0.000 f1: -1.000 
training batch:   240, loss: 22.05252, precision: -1.000 recall: 0.000 f1: -1.000 
training batch:   260, loss: 20.80149, precision: -1.000 recall: 0.000 f1: -1.000 
training batch:   280, loss: 17.26889, precision: -1.000 recall: 0.000 f1: -1.000 
training batch:   300, loss: 22.33546, precision: -1.000 recall: 0.000 f1: -1.000 
training batch:   320, loss: 27.88078, precision: -1.000 recall: 0.000 f1: -1.000 
training batch:   340, loss: 32.33965, precision: -1.000 recall: 0.000 f1: -1.000 
training batch:   360, loss: 16.77236, precision: -1.000 recall: 0.000 f1: -1.000 
training batch:   380, loss: 14.79244, precision: -1.000 recall: 0.000 f1: -1.000 
training batch:   400, loss: 39.47736, precision: -1.000 recall: 0.000 f1: -1.000 
training batch:   420, loss: 22.01616, precision: -1.000 recall: 0.000 f1: -1.000 
training batch:   440, loss: 38.26123, precision: -1.000 recall: 0.000 f1: -1.000 
training batch:   460, loss: 18.42186, precision: -1.000 recall: 0.000 f1: -1.000 
training batch:   480, loss: 18.19693, precision: -1.000 recall: 0.000 f1: -1.000 
training batch:   500, loss: 19.20133, precision: -1.000 recall: 0.000 f1: -1.000 
training batch:   520, loss: 18.88934, precision: -1.000 recall: 0.000 f1: -1.000 
training batch:   540, loss: 23.82987, precision: -1.000 recall: 0.000 f1: -1.000 
training batch:   560, loss: 13.64662, precision: -1.000 recall: 0.000 f1: -1.000 
training batch:   580, loss: 13.28065, precision: -1.000 recall: 0.000 f1: -1.000 
training batch:   600, loss: 17.87882, precision: 0.000 recall: 0.000 f1: -1.000 
training batch:   620, loss: 13.23662, precision: 0.000 recall: 0.000 f1: -1.000 
training batch:   640, loss: 13.87232, precision: 0.000 recall: 0.000 f1: -1.000 
training batch:   660, loss: 8.12608, precision: -1.000 recall: 0.000 f1: -1.000 
training batch:   680, loss: 15.84465, precision: -1.000 recall: 0.000 f1: -1.000 
training batch:   700, loss: 14.43386, precision: 0.000 recall: 0.000 f1: -1.000 
training batch:   720, loss: 20.18568, precision: 1.000 recall: 0.056 f1: 0.105 
training batch:   740, loss: 11.17960, precision: 0.000 recall: 0.000 f1: -1.000 
training batch:   760, loss: 14.13932, precision: 0.250 recall: 0.062 f1: 0.100 
training batch:   780, loss: 12.22750, precision: 0.429 recall: 0.273 f1: 0.333 
training batch:   800, loss: 14.44335, precision: 0.000 recall: 0.000 f1: -1.000 
training batch:   820, loss: 7.86936, precision: 0.500 recall: 0.182 f1: 0.267 
training batch:   840, loss: 9.73001, precision: 0.000 recall: 0.000 f1: -1.000 
training batch:   860, loss: 7.29854, precision: 0.500 recall: 0.182 f1: 0.267 
training batch:   880, loss: 9.72462, precision: 0.800 recall: 0.308 f1: 0.444 
training batch:   900, loss: 6.72534, precision: 1.000 recall: 0.417 f1: 0.588 
training batch:   920, loss: 9.50332, precision: 0.667 recall: 0.429 f1: 0.522 
training batch:   940, loss: 7.47830, precision: 0.455 recall: 0.385 f1: 0.417 
training batch:   960, loss: 5.96774, precision: 1.000 recall: 0.615 f1: 0.762 
training batch:   980, loss: 5.74379, precision: 0.714 recall: 0.556 f1: 0.625 
training batch:  1000, loss: 15.99560, precision: 0.333 recall: 0.167 f1: 0.222 
training batch:  1020, loss: 8.45056, precision: 0.600 recall: 0.188 f1: 0.286 
training batch:  1040, loss: 11.56917, precision: 0.286 recall: 0.167 f1: 0.211 
training batch:  1060, loss: 9.18920, precision: 0.500 recall: 0.231 f1: 0.316 
training batch:  1080, loss: 6.04282, precision: 0.714 recall: 0.500 f1: 0.588 
training batch:  1100, loss: 4.97447, precision: 0.571 recall: 0.364 f1: 0.444 
training batch:  1120, loss: 5.99127, precision: 0.400 recall: 0.154 f1: 0.222 
training batch:  1140, loss: 7.13772, precision: 0.600 recall: 0.333 f1: 0.429 
training batch:  1160, loss: 10.30606, precision: 0.667 recall: 0.364 f1: 0.471 
training batch:  1180, loss: 8.75673, precision: 0.778 recall: 0.467 f1: 0.583 
training batch:  1200, loss: 4.28236, precision: 1.000 recall: 0.667 f1: 0.800 
training batch:  1220, loss: 7.67472, precision: 0.667 recall: 0.375 f1: 0.480 
training batch:  1240, loss: 5.85861, precision: 1.000 recall: 0.538 f1: 0.700 
training batch:  1260, loss: 5.39852, precision: 0.833 recall: 0.417 f1: 0.556 
training batch:  1280, loss: 12.74876, precision: 0.632 recall: 0.545 f1: 0.585 
training batch:  1300, loss: 6.98986, precision: 0.636 recall: 0.583 f1: 0.609 
training batch:  1320, loss: 4.73380, precision: 1.000 recall: 0.500 f1: 0.667 
training batch:  1340, loss: 3.32238, precision: 0.875 recall: 0.700 f1: 0.778 
training batch:  1360, loss: 5.58841, precision: 0.750 recall: 0.643 f1: 0.692 
training batch:  1380, loss: 4.60851, precision: 0.727 recall: 0.667 f1: 0.696 
training batch:  1400, loss: 3.28017, precision: 0.875 recall: 0.583 f1: 0.700 
training batch:  1420, loss: 12.05035, precision: 0.700 recall: 0.583 f1: 0.636 
training batch:  1440, loss: 5.20032, precision: 0.700 recall: 0.583 f1: 0.636 
training batch:  1460, loss: 7.86143, precision: 0.615 recall: 0.571 f1: 0.593 
training batch:  1480, loss: 2.71163, precision: 0.909 recall: 0.909 f1: 0.909 
training batch:  1500, loss: 8.80929, precision: 0.444 recall: 0.308 f1: 0.364 
training batch:  1520, loss: 3.03550, precision: 0.889 recall: 0.889 f1: 0.889 
training batch:  1540, loss: 3.82551, precision: 0.923 recall: 0.800 f1: 0.857 
training batch:  1560, loss: 4.51760, precision: 0.769 recall: 0.667 f1: 0.714 
training batch:  1580, loss: 6.75612, precision: 0.900 recall: 0.474 f1: 0.621 
training batch:  1600, loss: 2.52799, precision: 1.000 recall: 0.818 f1: 0.900 
training batch:  1620, loss: 3.75879, precision: 0.933 recall: 0.778 f1: 0.848 
training batch:  1640, loss: 6.13839, precision: 0.846 recall: 0.786 f1: 0.815 
training batch:  1660, loss: 3.22283, precision: 0.833 recall: 0.625 f1: 0.714 
training batch:  1680, loss: 1.19095, precision: 1.000 recall: 0.900 f1: 0.947 
training batch:  1700, loss: 1.11454, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1720, loss: 2.57006, precision: 0.917 recall: 0.846 f1: 0.880 
training batch:  1740, loss: 8.47695, precision: 0.923 recall: 0.667 f1: 0.774 
training batch:  1760, loss: 1.77246, precision: 0.846 recall: 0.733 f1: 0.786 
training batch:  1780, loss: 2.27828, precision: 1.000 recall: 0.875 f1: 0.933 
training batch:  1800, loss: 2.16189, precision: 0.818 recall: 0.818 f1: 0.818 
training batch:  1820, loss: 3.09423, precision: 0.750 recall: 0.900 f1: 0.818 
training batch:  1840, loss: 15.59362, precision: 0.615 recall: 0.533 f1: 0.571 
training batch:  1860, loss: 8.77103, precision: 0.824 recall: 0.737 f1: 0.778 
training batch:  1880, loss: 8.42917, precision: 0.688 recall: 0.611 f1: 0.647 
training batch:  1900, loss: 4.23231, precision: 0.812 recall: 0.867 f1: 0.839 
training batch:  1920, loss: 9.56689, precision: 0.857 recall: 0.800 f1: 0.828 
training batch:  1940, loss: 1.04637, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1960, loss: 4.55188, precision: 0.778 recall: 0.824 f1: 0.800 
training batch:  1980, loss: 5.64896, precision: 0.929 recall: 0.684 f1: 0.788 
training batch:  2000, loss: 2.33776, precision: 0.917 recall: 1.000 f1: 0.957 
training batch:  2020, loss: 4.53833, precision: 0.957 recall: 0.957 f1: 0.957 
training batch:  2040, loss: 6.85996, precision: 0.750 recall: 0.706 f1: 0.727 
training batch:  2060, loss: 0.64177, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2080, loss: 1.27866, precision: 0.909 recall: 0.833 f1: 0.870 
training batch:  2100, loss: 4.67173, precision: 0.667 recall: 0.714 f1: 0.690 
training batch:  2120, loss: 3.31362, precision: 0.846 recall: 0.786 f1: 0.815 
training batch:  2140, loss: 2.36667, precision: 1.000 recall: 0.857 f1: 0.923 
training batch:  2160, loss: 7.76197, precision: 0.529 recall: 0.750 f1: 0.621 
training batch:  2180, loss: 3.20953, precision: 0.833 recall: 0.909 f1: 0.870 
training batch:  2200, loss: 2.76720, precision: 1.000 recall: 0.923 f1: 0.960 
training batch:  2220, loss: 1.85057, precision: 0.889 recall: 0.889 f1: 0.889 
training batch:  2240, loss: 8.39896, precision: 1.000 recall: 0.933 f1: 0.966 
training batch:  2260, loss: 4.43927, precision: 0.889 recall: 0.889 f1: 0.889 
training batch:  2280, loss: 1.04678, precision: 0.933 recall: 0.933 f1: 0.933 
training batch:  2300, loss: 1.02400, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2320, loss: 3.34215, precision: 0.867 recall: 0.812 f1: 0.839 
training batch:  2340, loss: 3.91897, precision: 0.917 recall: 1.000 f1: 0.957 
training batch:  2360, loss: 4.35357, precision: 0.824 recall: 0.875 f1: 0.848 
training batch:  2380, loss: 0.92348, precision: 0.917 recall: 0.917 f1: 0.917 
training batch:  2400, loss: 10.66017, precision: 0.538 recall: 0.636 f1: 0.583 
training batch:  2420, loss: 6.45684, precision: 0.920 recall: 0.885 f1: 0.902 
training batch:  2440, loss: 0.51225, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2460, loss: 2.36801, precision: 1.000 recall: 0.917 f1: 0.957 
training batch:  2480, loss: 0.59654, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2500, loss: 0.94798, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2520, loss: 2.83541, precision: 1.000 recall: 0.941 f1: 0.970 
training batch:  2540, loss: 2.30585, precision: 0.867 recall: 0.867 f1: 0.867 
training batch:  2560, loss: 3.41634, precision: 0.800 recall: 0.800 f1: 0.800 
training batch:  2580, loss: 2.90469, precision: 1.000 recall: 0.952 f1: 0.976 
training batch:  2600, loss: 2.39347, precision: 1.000 recall: 0.895 f1: 0.944 
training batch:  2620, loss: 1.30413, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2640, loss: 1.54153, precision: 0.857 recall: 0.800 f1: 0.828 
training batch:  2660, loss: 1.68207, precision: 0.900 recall: 0.900 f1: 0.900 
training batch:  2680, loss: 1.08918, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2700, loss: 1.33285, precision: 0.941 recall: 1.000 f1: 0.970 
training batch:  2720, loss: 0.30571, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2740, loss: 4.45479, precision: 0.812 recall: 0.867 f1: 0.839 
training batch:  2760, loss: 7.57256, precision: 0.867 recall: 0.812 f1: 0.839 
training batch:  2780, loss: 1.98499, precision: 1.000 recall: 0.929 f1: 0.963 
training batch:  2800, loss: 4.66659, precision: 1.000 recall: 0.933 f1: 0.966 
training batch:  2820, loss: 0.75993, precision: 1.000 recall: 0.917 f1: 0.957 
training batch:  2840, loss: 2.46954, precision: 0.786 recall: 0.846 f1: 0.815 
training batch:  2860, loss: 1.78310, precision: 0.917 recall: 0.846 f1: 0.880 
training batch:  2880, loss: 4.26464, precision: 1.000 recall: 0.923 f1: 0.960 
training batch:  2900, loss: 0.46805, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2920, loss: 2.55758, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2940, loss: 2.73086, precision: 0.941 recall: 0.889 f1: 0.914 
training batch:  2960, loss: 0.49065, precision: 1.000 recall: 0.909 f1: 0.952 
training batch:  2980, loss: 0.60132, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3000, loss: 5.60435, precision: 0.929 recall: 0.929 f1: 0.929 
training batch:  3020, loss: 1.54557, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3040, loss: 2.75442, precision: 1.000 recall: 0.800 f1: 0.889 
training batch:  3060, loss: 5.04564, precision: 0.769 recall: 0.833 f1: 0.800 
training batch:  3080, loss: 1.08099, precision: 0.824 recall: 0.875 f1: 0.848 
training batch:  3100, loss: 0.64792, precision: 0.929 recall: 0.929 f1: 0.929 
training batch:  3120, loss: 5.67640, precision: 0.867 recall: 0.867 f1: 0.867 
training batch:  3140, loss: 2.23348, precision: 0.700 recall: 0.700 f1: 0.700 
training batch:  3160, loss: 0.84796, precision: 0.833 recall: 1.000 f1: 0.909 
training batch:  3180, loss: 0.89161, precision: 0.786 recall: 0.786 f1: 0.786 
training batch:  3200, loss: 2.53795, precision: 0.933 recall: 0.933 f1: 0.933 
training batch:  3220, loss: 2.27393, precision: 0.800 recall: 0.889 f1: 0.842 
training batch:  3240, loss: 3.35937, precision: 0.800 recall: 0.800 f1: 0.800 
training batch:  3260, loss: 1.01042, precision: 1.000 recall: 0.923 f1: 0.960 
training batch:  3280, loss: 0.18035, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3300, loss: 0.25064, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3320, loss: 2.74857, precision: 0.895 recall: 0.944 f1: 0.919 
training batch:  3340, loss: 3.16514, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3360, loss: 6.19569, precision: 0.917 recall: 0.846 f1: 0.880 
training batch:  3380, loss: 0.79918, precision: 0.909 recall: 0.909 f1: 0.909 
training batch:  3400, loss: 0.71587, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3420, loss: 7.99721, precision: 0.889 recall: 0.889 f1: 0.889 
training batch:  3440, loss: 0.07550, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3460, loss: 0.12699, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3480, loss: 0.90655, precision: 0.846 recall: 0.917 f1: 0.880 
training batch:  3500, loss: 8.52373, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3520, loss: 0.33596, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3540, loss: 0.18322, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3560, loss: 2.88364, precision: 0.929 recall: 0.929 f1: 0.929 
training batch:  3580, loss: 1.43270, precision: 1.000 recall: 0.900 f1: 0.947 
training batch:  3600, loss: 0.90067, precision: 0.923 recall: 0.923 f1: 0.923 
training batch:  3620, loss: 3.90177, precision: 0.733 recall: 0.786 f1: 0.759 
training batch:  3640, loss: 1.61231, precision: 0.864 recall: 1.000 f1: 0.927 
training batch:  3660, loss: 0.62924, precision: 0.917 recall: 0.846 f1: 0.880 
training batch:  3680, loss: 2.20939, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3700, loss: 1.67887, precision: 0.840 recall: 0.913 f1: 0.875 
training batch:  3720, loss: 0.07827, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3740, loss: 3.86285, precision: 0.857 recall: 0.923 f1: 0.889 
training batch:  3760, loss: 1.01298, precision: 0.909 recall: 0.909 f1: 0.909 
training batch:  3780, loss: 2.26904, precision: 0.875 recall: 0.778 f1: 0.824 
training batch:  3800, loss: 0.66981, precision: 0.909 recall: 0.909 f1: 0.909 
training batch:  3820, loss: 0.42663, precision: 0.952 recall: 0.952 f1: 0.952 
training batch:  3840, loss: 3.32039, precision: 1.000 recall: 0.769 f1: 0.870 
training batch:  3860, loss: 0.13013, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3880, loss: 0.04191, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3900, loss: 1.75194, precision: 1.000 recall: 0.778 f1: 0.875 
training batch:  3920, loss: 0.80742, precision: 0.909 recall: 0.833 f1: 0.870 
training batch:  3940, loss: 0.61456, precision: 0.923 recall: 0.923 f1: 0.923 
training batch:  3960, loss: 3.05734, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3980, loss: 6.78860, precision: 0.950 recall: 0.905 f1: 0.927 
training batch:  4000, loss: 1.20000, precision: 0.833 recall: 0.909 f1: 0.870 
training batch:  4020, loss: 0.02437, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4040, loss: 0.52974, precision: 0.895 recall: 0.944 f1: 0.919 
training batch:  4060, loss: 0.03557, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4080, loss: 6.85947, precision: 0.900 recall: 0.818 f1: 0.857 
training batch:  4100, loss: 1.42081, precision: 0.909 recall: 0.909 f1: 0.909 
training batch:  4120, loss: 2.90265, precision: 0.917 recall: 0.917 f1: 0.917 
training batch:  4140, loss: 1.05093, precision: 0.917 recall: 0.917 f1: 0.917 
training batch:  4160, loss: 1.41641, precision: 0.882 recall: 0.833 f1: 0.857 
training batch:  4180, loss: 0.02644, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4200, loss: 1.51387, precision: 0.909 recall: 1.000 f1: 0.952 
training batch:  4220, loss: 4.47257, precision: 1.000 recall: 0.833 f1: 0.909 
training batch:  4240, loss: 1.44726, precision: 0.769 recall: 0.909 f1: 0.833 
training batch:  4260, loss: 2.97510, precision: 0.923 recall: 0.923 f1: 0.923 
training batch:  4280, loss: 0.18546, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4300, loss: 0.93852, precision: 0.889 recall: 0.889 f1: 0.889 
training batch:  4320, loss: 0.06708, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4340, loss: 0.23132, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4360, loss: 0.16426, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4380, loss: 1.30352, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4400, loss: 1.40511, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4420, loss: 0.35292, precision: 0.917 recall: 1.000 f1: 0.957 
training batch:  4440, loss: 2.09027, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4460, loss: 2.35769, precision: 1.000 recall: 0.929 f1: 0.963 
training batch:  4480, loss: 0.07374, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4500, loss: 0.14865, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4520, loss: 0.05768, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4540, loss: 0.05363, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4560, loss: 1.88102, precision: 0.857 recall: 0.857 f1: 0.857 
training batch:  4580, loss: 0.02435, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4600, loss: 0.02528, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4620, loss: 1.03507, precision: 0.950 recall: 0.950 f1: 0.950 
training batch:  4640, loss: 2.02372, precision: 0.846 recall: 0.846 f1: 0.846 
training batch:  4660, loss: 1.51858, precision: 0.938 recall: 1.000 f1: 0.968 
training batch:  4680, loss: 11.95370, precision: 0.800 recall: 0.667 f1: 0.727 
training batch:  4700, loss: 0.02445, precision: 1.000 recall: 1.000 f1: 1.000 
start evaluate engines...
label: ORG, precision: 0.732 recall: 0.720 f1: 0.719 
label: PERSON, precision: 0.810 recall: 0.811 f1: 0.808 
label: LOC, precision: 0.835 recall: 0.836 f1: 0.829 
time consumption:21.54(min), precision: 0.937 recall: 0.930 f1: 0.932 
saved the new best model with f1: 0.932
loading data...
epoch:2/30
training batch:    20, loss: 1.30936, precision: 0.917 recall: 0.917 f1: 0.917 
training batch:    40, loss: 0.15673, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    60, loss: 1.66044, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    80, loss: 0.02390, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   100, loss: 0.18355, precision: 0.900 recall: 0.900 f1: 0.900 
training batch:   120, loss: 0.06391, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   140, loss: 1.66789, precision: 0.857 recall: 0.750 f1: 0.800 
training batch:   160, loss: 2.10216, precision: 0.941 recall: 1.000 f1: 0.970 
training batch:   180, loss: 3.57245, precision: 0.920 recall: 0.920 f1: 0.920 
training batch:   200, loss: 0.31843, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   220, loss: 2.03622, precision: 1.000 recall: 0.941 f1: 0.970 
training batch:   240, loss: 8.52971, precision: 0.800 recall: 0.800 f1: 0.800 
training batch:   260, loss: 0.24035, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   280, loss: 0.02041, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   300, loss: 0.96310, precision: 0.867 recall: 1.000 f1: 0.929 
training batch:   320, loss: 2.15158, precision: 0.909 recall: 1.000 f1: 0.952 
training batch:   340, loss: 1.80748, precision: 0.800 recall: 0.889 f1: 0.842 
training batch:   360, loss: 6.05127, precision: 0.769 recall: 0.909 f1: 0.833 
training batch:   380, loss: 4.97045, precision: 0.909 recall: 0.833 f1: 0.870 
training batch:   400, loss: 1.69946, precision: 0.909 recall: 0.833 f1: 0.870 
training batch:   420, loss: 0.01313, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   440, loss: 0.11865, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   460, loss: 0.26402, precision: 0.929 recall: 0.929 f1: 0.929 
training batch:   480, loss: 0.01649, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   500, loss: 0.69449, precision: 0.846 recall: 1.000 f1: 0.917 
training batch:   520, loss: 1.80503, precision: 0.938 recall: 0.882 f1: 0.909 
training batch:   540, loss: 3.46340, precision: 0.800 recall: 0.889 f1: 0.842 
training batch:   560, loss: 1.14440, precision: 0.938 recall: 0.938 f1: 0.938 
training batch:   580, loss: 0.48255, precision: 0.941 recall: 0.941 f1: 0.941 
training batch:   600, loss: 5.62445, precision: 0.889 recall: 0.889 f1: 0.889 
training batch:   620, loss: 0.01097, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   640, loss: 1.08485, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   660, loss: 1.35445, precision: 0.929 recall: 0.929 f1: 0.929 
training batch:   680, loss: 0.47571, precision: 0.900 recall: 0.900 f1: 0.900 
training batch:   700, loss: 6.71382, precision: 0.885 recall: 0.958 f1: 0.920 
training batch:   720, loss: 2.80472, precision: 0.889 recall: 0.889 f1: 0.889 
training batch:   740, loss: 0.01025, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   760, loss: 3.09022, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   780, loss: 2.01040, precision: 0.955 recall: 0.840 f1: 0.894 
training batch:   800, loss: 0.72361, precision: 0.889 recall: 0.800 f1: 0.842 
training batch:   820, loss: 3.84026, precision: 0.667 recall: 0.600 f1: 0.632 
training batch:   840, loss: 0.02091, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   860, loss: 1.00938, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   880, loss: 2.82291, precision: 0.917 recall: 1.000 f1: 0.957 
training batch:   900, loss: 0.09830, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   920, loss: 0.76119, precision: 0.950 recall: 0.950 f1: 0.950 
training batch:   940, loss: 0.88716, precision: 0.917 recall: 1.000 f1: 0.957 
training batch:   960, loss: 2.70552, precision: 0.714 recall: 0.833 f1: 0.769 
training batch:   980, loss: 0.02773, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1000, loss: 0.59403, precision: 0.923 recall: 1.000 f1: 0.960 
training batch:  1020, loss: 1.20150, precision: 0.800 recall: 0.889 f1: 0.842 
training batch:  1040, loss: 0.12244, precision: 0.909 recall: 0.909 f1: 0.909 
training batch:  1060, loss: 0.14350, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1080, loss: 0.07057, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1100, loss: 0.09729, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1120, loss: 0.86722, precision: 0.933 recall: 0.933 f1: 0.933 
training batch:  1140, loss: 5.11594, precision: 0.944 recall: 1.000 f1: 0.971 
training batch:  1160, loss: 1.44883, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1180, loss: 3.89352, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1200, loss: 0.00494, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1220, loss: 0.03860, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1240, loss: 0.04371, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1260, loss: 3.26434, precision: 0.750 recall: 0.923 f1: 0.828 
training batch:  1280, loss: 0.13509, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1300, loss: 1.86086, precision: 0.923 recall: 1.000 f1: 0.960 
training batch:  1320, loss: 0.10185, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1340, loss: 0.02109, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1360, loss: 0.22103, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1380, loss: 0.62017, precision: 0.933 recall: 1.000 f1: 0.966 
training batch:  1400, loss: 2.19382, precision: 0.889 recall: 0.889 f1: 0.889 
training batch:  1420, loss: 6.96697, precision: 0.857 recall: 0.800 f1: 0.828 
training batch:  1440, loss: 0.01369, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1460, loss: 2.82434, precision: 0.947 recall: 0.947 f1: 0.947 
training batch:  1480, loss: 0.00893, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1500, loss: 0.00640, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1520, loss: 1.09544, precision: 0.857 recall: 0.923 f1: 0.889 
training batch:  1540, loss: 0.67996, precision: 1.000 recall: 0.900 f1: 0.947 
training batch:  1560, loss: 4.95554, precision: 0.971 recall: 1.000 f1: 0.985 
training batch:  1580, loss: 3.71214, precision: 0.923 recall: 0.923 f1: 0.923 
training batch:  1600, loss: 0.00657, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1620, loss: 1.73025, precision: 0.923 recall: 1.000 f1: 0.960 
training batch:  1640, loss: 0.49086, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1660, loss: 1.56623, precision: 0.941 recall: 0.941 f1: 0.941 
training batch:  1680, loss: 3.91158, precision: 0.800 recall: 0.800 f1: 0.800 
training batch:  1700, loss: 0.89997, precision: 1.000 recall: 0.909 f1: 0.952 
training batch:  1720, loss: 0.00904, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1740, loss: 1.26978, precision: 1.000 recall: 0.957 f1: 0.978 
training batch:  1760, loss: 0.00733, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1780, loss: 0.03090, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1800, loss: 5.43945, precision: 0.900 recall: 0.900 f1: 0.900 
training batch:  1820, loss: 1.10160, precision: 0.889 recall: 0.941 f1: 0.914 
training batch:  1840, loss: 0.00508, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1860, loss: 1.94563, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1880, loss: 0.95753, precision: 0.917 recall: 0.846 f1: 0.880 
training batch:  1900, loss: 0.00437, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1920, loss: 0.09278, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1940, loss: 1.12668, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1960, loss: 0.14328, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1980, loss: 0.00792, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2000, loss: 2.98956, precision: 0.933 recall: 0.933 f1: 0.933 
training batch:  2020, loss: 0.00906, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2040, loss: 0.00503, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2060, loss: 3.18662, precision: 0.917 recall: 0.917 f1: 0.917 
training batch:  2080, loss: 4.21055, precision: 0.933 recall: 0.933 f1: 0.933 
training batch:  2100, loss: 0.00509, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2120, loss: 0.00828, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2140, loss: 0.01397, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2160, loss: 0.00400, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2180, loss: 0.96934, precision: 0.900 recall: 0.900 f1: 0.900 
training batch:  2200, loss: 4.08566, precision: 0.818 recall: 0.900 f1: 0.857 
training batch:  2220, loss: 5.16910, precision: 1.000 recall: 0.938 f1: 0.968 
training batch:  2240, loss: 0.09996, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2260, loss: 1.94635, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2280, loss: 0.00449, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2300, loss: 0.07022, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2320, loss: 0.34044, precision: 0.938 recall: 0.938 f1: 0.938 
training batch:  2340, loss: 0.06950, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2360, loss: 1.07751, precision: 0.923 recall: 0.857 f1: 0.889 
training batch:  2380, loss: 8.24209, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2400, loss: 1.46425, precision: 0.947 recall: 0.947 f1: 0.947 
training batch:  2420, loss: 1.90036, precision: 1.000 recall: 0.917 f1: 0.957 
training batch:  2440, loss: 3.09714, precision: 0.846 recall: 0.917 f1: 0.880 
training batch:  2460, loss: 0.80000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2480, loss: 0.20721, precision: 0.909 recall: 0.909 f1: 0.909 
training batch:  2500, loss: 0.00404, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2520, loss: 0.00935, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2540, loss: 5.38912, precision: 0.909 recall: 0.833 f1: 0.870 
training batch:  2560, loss: 0.00390, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2580, loss: 3.75725, precision: 0.867 recall: 0.929 f1: 0.897 
training batch:  2600, loss: 4.30791, precision: 0.929 recall: 1.000 f1: 0.963 
training batch:  2620, loss: 3.24751, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2640, loss: 0.00543, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2660, loss: 0.60853, precision: 1.000 recall: 0.917 f1: 0.957 
training batch:  2680, loss: 1.61802, precision: 0.867 recall: 0.929 f1: 0.897 
training batch:  2700, loss: 0.00806, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2720, loss: 1.17342, precision: 0.929 recall: 0.929 f1: 0.929 
training batch:  2740, loss: 0.00318, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2760, loss: 0.73600, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2780, loss: 0.42081, precision: 0.900 recall: 0.818 f1: 0.857 
training batch:  2800, loss: 0.00614, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2820, loss: 0.06634, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2840, loss: 0.87862, precision: 0.857 recall: 0.923 f1: 0.889 
training batch:  2860, loss: 0.24705, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2880, loss: 3.57271, precision: 1.000 recall: 0.923 f1: 0.960 
training batch:  2900, loss: 0.00368, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2920, loss: 0.00514, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2940, loss: 0.00577, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2960, loss: 0.00360, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2980, loss: 1.16620, precision: 0.917 recall: 0.846 f1: 0.880 
training batch:  3000, loss: 0.25301, precision: 0.833 recall: 0.909 f1: 0.870 
training batch:  3020, loss: 0.75399, precision: 0.933 recall: 0.933 f1: 0.933 
training batch:  3040, loss: 1.36583, precision: 0.933 recall: 0.933 f1: 0.933 
training batch:  3060, loss: 4.50532, precision: 0.812 recall: 0.929 f1: 0.867 
training batch:  3080, loss: 1.39717, precision: 0.818 recall: 0.900 f1: 0.857 
training batch:  3100, loss: 0.40271, precision: 0.938 recall: 1.000 f1: 0.968 
training batch:  3120, loss: 1.23394, precision: 0.960 recall: 0.960 f1: 0.960 
training batch:  3140, loss: 0.03236, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3160, loss: 0.67121, precision: 0.917 recall: 0.917 f1: 0.917 
training batch:  3180, loss: 0.32387, precision: 0.923 recall: 0.923 f1: 0.923 
training batch:  3200, loss: 0.10638, precision: 0.846 recall: 0.917 f1: 0.880 
training batch:  3220, loss: 3.49104, precision: 0.889 recall: 0.800 f1: 0.842 
training batch:  3240, loss: 0.03664, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3260, loss: 0.00823, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3280, loss: 4.61302, precision: 0.786 recall: 0.786 f1: 0.786 
training batch:  3300, loss: 0.01247, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3320, loss: 0.08689, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3340, loss: 3.56800, precision: 0.909 recall: 0.909 f1: 0.909 
training batch:  3360, loss: 0.57122, precision: 0.889 recall: 0.889 f1: 0.889 
training batch:  3380, loss: 0.52216, precision: 1.000 recall: 0.929 f1: 0.963 
training batch:  3400, loss: 0.01025, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3420, loss: 0.03350, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3440, loss: 0.00804, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3460, loss: 0.86681, precision: 1.000 recall: 0.917 f1: 0.957 
training batch:  3480, loss: 1.39658, precision: 0.800 recall: 0.923 f1: 0.857 
training batch:  3500, loss: 2.04572, precision: 0.917 recall: 1.000 f1: 0.957 
training batch:  3520, loss: 4.39348, precision: 0.933 recall: 0.875 f1: 0.903 
training batch:  3540, loss: 6.89163, precision: 0.933 recall: 0.933 f1: 0.933 
training batch:  3560, loss: 1.62612, precision: 0.945 recall: 0.963 f1: 0.954 
training batch:  3580, loss: 0.01377, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3600, loss: 0.55127, precision: 0.929 recall: 0.929 f1: 0.929 
training batch:  3620, loss: 1.23343, precision: 1.000 recall: 0.952 f1: 0.976 
training batch:  3640, loss: 0.00688, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3660, loss: 0.90083, precision: 0.889 recall: 0.889 f1: 0.889 
training batch:  3680, loss: 2.92193, precision: 0.857 recall: 0.923 f1: 0.889 
training batch:  3700, loss: 2.68165, precision: 0.920 recall: 0.958 f1: 0.939 
training batch:  3720, loss: 0.00259, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3740, loss: 0.03613, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3760, loss: 0.90142, precision: 1.000 recall: 0.941 f1: 0.970 
training batch:  3780, loss: 2.30446, precision: 0.941 recall: 0.941 f1: 0.941 
training batch:  3800, loss: 0.00313, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3820, loss: 0.01264, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3840, loss: 0.01169, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3860, loss: 0.26893, precision: 0.875 recall: 0.933 f1: 0.903 
training batch:  3880, loss: 4.64664, precision: 0.900 recall: 0.857 f1: 0.878 
training batch:  3900, loss: 0.08032, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3920, loss: 0.00385, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3940, loss: 0.92740, precision: 0.917 recall: 1.000 f1: 0.957 
training batch:  3960, loss: 3.52982, precision: 0.923 recall: 1.000 f1: 0.960 
training batch:  3980, loss: 4.22183, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4000, loss: 0.40513, precision: 0.909 recall: 1.000 f1: 0.952 
training batch:  4020, loss: 0.00863, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4040, loss: 0.00837, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4060, loss: 0.01318, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4080, loss: 14.90813, precision: 0.733 recall: 0.917 f1: 0.815 
training batch:  4100, loss: 0.00229, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4120, loss: 0.00331, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4140, loss: 0.01689, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4160, loss: 2.51633, precision: 0.867 recall: 0.722 f1: 0.788 
training batch:  4180, loss: 4.30767, precision: 0.818 recall: 0.900 f1: 0.857 
training batch:  4200, loss: 0.00332, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4220, loss: 1.56756, precision: 0.833 recall: 0.909 f1: 0.870 
training batch:  4240, loss: 1.86278, precision: 0.913 recall: 0.955 f1: 0.933 
training batch:  4260, loss: 0.03108, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4280, loss: 0.24010, precision: 0.923 recall: 0.923 f1: 0.923 
training batch:  4300, loss: 0.00348, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4320, loss: 1.75173, precision: 0.938 recall: 0.938 f1: 0.938 
training batch:  4340, loss: 1.08798, precision: 0.889 recall: 0.800 f1: 0.842 
training batch:  4360, loss: 0.00131, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4380, loss: 0.00421, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4400, loss: 1.16970, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4420, loss: 0.00485, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4440, loss: 0.01450, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4460, loss: 0.57953, precision: 0.889 recall: 0.800 f1: 0.842 
training batch:  4480, loss: 0.00357, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4500, loss: 0.63313, precision: 0.929 recall: 0.929 f1: 0.929 
training batch:  4520, loss: 0.04308, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4540, loss: 14.72505, precision: 0.846 recall: 0.786 f1: 0.815 
training batch:  4560, loss: 0.31192, precision: 0.955 recall: 0.955 f1: 0.955 
training batch:  4580, loss: 0.92632, precision: 0.800 recall: 0.889 f1: 0.842 
training batch:  4600, loss: 1.93017, precision: 1.000 recall: 0.941 f1: 0.970 
training batch:  4620, loss: 5.93283, precision: 0.909 recall: 0.909 f1: 0.909 
training batch:  4640, loss: 4.87880, precision: 0.842 recall: 0.941 f1: 0.889 
training batch:  4660, loss: 1.82539, precision: 0.933 recall: 1.000 f1: 0.966 
training batch:  4680, loss: 0.04038, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4700, loss: 1.69962, precision: 0.923 recall: 0.923 f1: 0.923 
start evaluate engines...
label: ORG, precision: 0.748 recall: 0.771 f1: 0.754 
label: PERSON, precision: 0.817 recall: 0.816 f1: 0.814 
label: LOC, precision: 0.855 recall: 0.848 f1: 0.847 
time consumption:21.31(min), precision: 0.946 recall: 0.949 f1: 0.947 
saved the new best model with f1: 0.947
loading data...
epoch:3/30
training batch:    20, loss: 0.01552, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    40, loss: 0.00968, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    60, loss: 0.00136, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    80, loss: 0.38887, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   100, loss: 0.07163, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   120, loss: 0.24863, precision: 1.000 recall: 0.923 f1: 0.960 
training batch:   140, loss: 0.00252, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   160, loss: 0.00423, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   180, loss: 0.01286, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   200, loss: 4.28129, precision: 0.778 recall: 0.933 f1: 0.848 
training batch:   220, loss: 0.00360, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   240, loss: 0.00224, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   260, loss: 1.19019, precision: 0.917 recall: 0.917 f1: 0.917 
training batch:   280, loss: 0.51225, precision: 0.923 recall: 0.857 f1: 0.889 
training batch:   300, loss: 0.00152, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   320, loss: 2.32106, precision: 0.933 recall: 0.933 f1: 0.933 
training batch:   340, loss: 2.57935, precision: 1.000 recall: 0.909 f1: 0.952 
training batch:   360, loss: 0.01434, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   380, loss: 0.00298, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   400, loss: 0.00154, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   420, loss: 0.45870, precision: 1.000 recall: 0.923 f1: 0.960 
training batch:   440, loss: 2.21511, precision: 0.941 recall: 0.941 f1: 0.941 
training batch:   460, loss: 4.44994, precision: 0.950 recall: 0.905 f1: 0.927 
training batch:   480, loss: 0.00141, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   500, loss: 0.00214, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   520, loss: 2.72586, precision: 0.933 recall: 0.933 f1: 0.933 
training batch:   540, loss: 0.60850, precision: 0.962 recall: 0.962 f1: 0.962 
training batch:   560, loss: 1.82349, precision: 0.938 recall: 0.938 f1: 0.938 
training batch:   580, loss: 0.00681, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   600, loss: 0.01886, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   620, loss: 0.91861, precision: 0.917 recall: 1.000 f1: 0.957 
training batch:   640, loss: 2.39294, precision: 1.000 recall: 0.929 f1: 0.963 
training batch:   660, loss: 0.02154, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   680, loss: 0.00815, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   700, loss: 0.00276, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   720, loss: 0.00102, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   740, loss: 0.00323, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   760, loss: 0.00299, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   780, loss: 0.11700, precision: 1.000 recall: 0.955 f1: 0.977 
training batch:   800, loss: 0.00117, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   820, loss: 0.00095, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   840, loss: 0.00143, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   860, loss: 2.95390, precision: 0.857 recall: 0.947 f1: 0.900 
training batch:   880, loss: 0.75709, precision: 1.000 recall: 0.900 f1: 0.947 
training batch:   900, loss: 0.01569, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   920, loss: 0.69788, precision: 0.909 recall: 1.000 f1: 0.952 
training batch:   940, loss: 0.34529, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   960, loss: 6.65277, precision: 0.929 recall: 0.867 f1: 0.897 
training batch:   980, loss: 0.38749, precision: 0.900 recall: 0.900 f1: 0.900 
training batch:  1000, loss: 0.01553, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1020, loss: 0.00489, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1040, loss: 0.00150, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1060, loss: 0.84125, precision: 1.000 recall: 0.933 f1: 0.966 
training batch:  1080, loss: 0.00829, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1100, loss: 0.00387, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1120, loss: 0.00152, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1140, loss: 0.11127, precision: 0.938 recall: 1.000 f1: 0.968 
training batch:  1160, loss: 0.00416, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1180, loss: 1.63150, precision: 1.000 recall: 0.909 f1: 0.952 
training batch:  1200, loss: 0.00329, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1220, loss: 0.99174, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1240, loss: 0.00161, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1260, loss: 0.00142, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1280, loss: 0.63853, precision: 0.846 recall: 0.846 f1: 0.846 
training batch:  1300, loss: 0.00767, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1320, loss: 0.74831, precision: 1.000 recall: 0.917 f1: 0.957 
training batch:  1340, loss: 0.04566, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1360, loss: 0.00290, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1380, loss: 4.45480, precision: 0.909 recall: 0.909 f1: 0.909 
training batch:  1400, loss: 0.04793, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1420, loss: 0.00120, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1440, loss: 0.49444, precision: 1.000 recall: 0.900 f1: 0.947 
training batch:  1460, loss: 1.29101, precision: 0.947 recall: 0.947 f1: 0.947 
training batch:  1480, loss: 1.23375, precision: 0.889 recall: 0.889 f1: 0.889 
training batch:  1500, loss: 0.00100, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1520, loss: 0.77603, precision: 1.000 recall: 0.933 f1: 0.966 
training batch:  1540, loss: 0.00182, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1560, loss: 0.00237, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1580, loss: 0.00241, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1600, loss: 0.00231, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1620, loss: 0.25276, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1640, loss: 2.77213, precision: 0.875 recall: 0.933 f1: 0.903 
training batch:  1660, loss: 2.07776, precision: 0.917 recall: 1.000 f1: 0.957 
training batch:  1680, loss: 0.70222, precision: 0.938 recall: 0.882 f1: 0.909 
training batch:  1700, loss: 0.00979, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1720, loss: 0.00963, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1740, loss: 0.00263, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1760, loss: 0.41018, precision: 0.846 recall: 0.917 f1: 0.880 
training batch:  1780, loss: 0.00207, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1800, loss: 0.01156, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1820, loss: 1.24237, precision: 0.889 recall: 0.889 f1: 0.889 
training batch:  1840, loss: 0.00313, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1860, loss: 0.00152, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1880, loss: 0.00209, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1900, loss: 2.34693, precision: 0.786 recall: 0.846 f1: 0.815 
training batch:  1920, loss: 0.01787, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1940, loss: 0.00368, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1960, loss: 0.00724, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1980, loss: 4.63065, precision: 0.727 recall: 0.889 f1: 0.800 
training batch:  2000, loss: 0.13927, precision: 1.000 recall: 0.947 f1: 0.973 
training batch:  2020, loss: 0.74602, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2040, loss: 0.00174, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2060, loss: 0.03506, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2080, loss: 0.11895, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2100, loss: 2.69360, precision: 0.909 recall: 0.833 f1: 0.870 
training batch:  2120, loss: 4.59019, precision: 0.947 recall: 0.947 f1: 0.947 
training batch:  2140, loss: 0.00386, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2160, loss: 0.00613, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2180, loss: 0.02432, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2200, loss: 0.04700, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2220, loss: 0.00252, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2240, loss: 0.01963, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2260, loss: 0.00298, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2280, loss: 0.00425, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2300, loss: 0.00519, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2320, loss: 0.00126, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2340, loss: 0.00137, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2360, loss: 0.00496, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2380, loss: 0.00873, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2400, loss: 2.17207, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2420, loss: 1.26887, precision: 0.875 recall: 0.875 f1: 0.875 
training batch:  2440, loss: 0.01109, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2460, loss: 5.69288, precision: 0.895 recall: 0.810 f1: 0.850 
training batch:  2480, loss: 0.01687, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2500, loss: 1.46410, precision: 1.000 recall: 0.929 f1: 0.963 
training batch:  2520, loss: 5.48060, precision: 0.950 recall: 1.000 f1: 0.974 
training batch:  2540, loss: 0.28576, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2560, loss: 0.54556, precision: 0.867 recall: 0.929 f1: 0.897 
training batch:  2580, loss: 0.01980, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2600, loss: 0.04338, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2620, loss: 0.00268, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2640, loss: 0.00572, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2660, loss: 0.00096, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2680, loss: 7.41959, precision: 0.933 recall: 0.933 f1: 0.933 
training batch:  2700, loss: 0.00243, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2720, loss: 0.24336, precision: 0.833 recall: 0.909 f1: 0.870 
training batch:  2740, loss: 0.00140, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2760, loss: 0.00289, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2780, loss: 1.92912, precision: 0.909 recall: 0.909 f1: 0.909 
training batch:  2800, loss: 0.00173, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2820, loss: 0.00068, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2840, loss: 0.00140, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2860, loss: 0.01314, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2880, loss: 8.61940, precision: 0.857 recall: 0.800 f1: 0.828 
training batch:  2900, loss: 0.01276, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2920, loss: 0.41306, precision: 1.000 recall: 0.958 f1: 0.979 
training batch:  2940, loss: 5.39478, precision: 0.897 recall: 0.897 f1: 0.897 
training batch:  2960, loss: 0.24006, precision: 0.833 recall: 0.909 f1: 0.870 
training batch:  2980, loss: 2.87254, precision: 0.917 recall: 1.000 f1: 0.957 
training batch:  3000, loss: 1.30921, precision: 1.000 recall: 0.923 f1: 0.960 
training batch:  3020, loss: 0.02926, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3040, loss: 1.00299, precision: 0.917 recall: 0.917 f1: 0.917 
training batch:  3060, loss: 0.00370, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3080, loss: 0.26545, precision: 0.941 recall: 0.941 f1: 0.941 
training batch:  3100, loss: 0.03185, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3120, loss: 0.00208, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3140, loss: 2.19283, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3160, loss: 0.26139, precision: 0.933 recall: 0.933 f1: 0.933 
training batch:  3180, loss: 0.00303, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3200, loss: 0.00178, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3220, loss: 0.09393, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3240, loss: 0.00203, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3260, loss: 1.49310, precision: 0.875 recall: 0.933 f1: 0.903 
training batch:  3280, loss: 0.00421, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3300, loss: 0.00072, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3320, loss: 0.08724, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3340, loss: 0.06527, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3360, loss: 0.00401, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3380, loss: 0.00123, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3400, loss: 0.00477, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3420, loss: 1.00852, precision: 0.929 recall: 0.867 f1: 0.897 
training batch:  3440, loss: 0.00271, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3460, loss: 0.00368, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3480, loss: 0.00171, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3500, loss: 0.00292, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3520, loss: 0.00157, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3540, loss: 0.34914, precision: 0.900 recall: 1.000 f1: 0.947 
training batch:  3560, loss: 0.03487, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3580, loss: 0.01041, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3600, loss: 1.70769, precision: 0.882 recall: 0.938 f1: 0.909 
training batch:  3620, loss: 0.84676, precision: 1.000 recall: 0.952 f1: 0.976 
training batch:  3640, loss: 1.72036, precision: 0.923 recall: 0.857 f1: 0.889 
training batch:  3660, loss: 0.01299, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3680, loss: 0.00109, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3700, loss: 0.48032, precision: 0.900 recall: 1.000 f1: 0.947 
training batch:  3720, loss: 0.00271, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3740, loss: 0.01200, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3760, loss: 1.58966, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3780, loss: 0.43783, precision: 0.909 recall: 1.000 f1: 0.952 
training batch:  3800, loss: 0.00300, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3820, loss: 0.00213, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3840, loss: 4.02246, precision: 0.917 recall: 0.917 f1: 0.917 
training batch:  3860, loss: 2.04237, precision: 0.889 recall: 0.941 f1: 0.914 
training batch:  3880, loss: 4.25281, precision: 1.000 recall: 0.938 f1: 0.968 
training batch:  3900, loss: 0.02833, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3920, loss: 0.00373, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3940, loss: 0.00328, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3960, loss: 1.53950, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3980, loss: 1.57315, precision: 0.857 recall: 0.923 f1: 0.889 
training batch:  4000, loss: 0.07130, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4020, loss: 2.40248, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4040, loss: 0.00275, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4060, loss: 0.00188, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4080, loss: 0.04174, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4100, loss: 0.03666, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4120, loss: 0.00187, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4140, loss: 10.79856, precision: 0.938 recall: 0.938 f1: 0.938 
training batch:  4160, loss: 0.00167, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4180, loss: 0.72128, precision: 0.909 recall: 0.909 f1: 0.909 
training batch:  4200, loss: 0.91924, precision: 0.826 recall: 0.905 f1: 0.864 
training batch:  4220, loss: 1.96866, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4240, loss: 0.89899, precision: 0.933 recall: 1.000 f1: 0.966 
training batch:  4260, loss: 1.64093, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4280, loss: 0.00375, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4300, loss: 6.70453, precision: 0.769 recall: 0.833 f1: 0.800 
training batch:  4320, loss: 0.05762, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4340, loss: 0.00104, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4360, loss: 0.00161, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4380, loss: 0.00268, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4400, loss: 0.00592, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4420, loss: 0.00207, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4440, loss: 3.70293, precision: 0.857 recall: 0.857 f1: 0.857 
training batch:  4460, loss: 0.11650, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4480, loss: 0.04275, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4500, loss: 0.00149, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4520, loss: 1.99070, precision: 0.950 recall: 0.950 f1: 0.950 
training batch:  4540, loss: 0.03462, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4560, loss: 0.00620, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4580, loss: 1.34905, precision: 0.909 recall: 0.909 f1: 0.909 
training batch:  4600, loss: 2.08836, precision: 0.875 recall: 0.875 f1: 0.875 
training batch:  4620, loss: 0.00491, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4640, loss: 0.00333, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4660, loss: 0.00128, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4680, loss: 0.32449, precision: 0.933 recall: 0.933 f1: 0.933 
training batch:  4700, loss: 0.02564, precision: 1.000 recall: 1.000 f1: 1.000 
start evaluate engines...
label: ORG, precision: 0.753 recall: 0.773 f1: 0.758 
label: PERSON, precision: 0.814 recall: 0.816 f1: 0.813 
label: LOC, precision: 0.853 recall: 0.863 f1: 0.854 
time consumption:21.36(min), precision: 0.944 recall: 0.955 f1: 0.948 
saved the new best model with f1: 0.948
loading data...
epoch:4/30
training batch:    20, loss: 0.00713, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    40, loss: 0.00132, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    60, loss: 2.49281, precision: 0.733 recall: 0.846 f1: 0.786 
training batch:    80, loss: 0.00124, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   100, loss: 0.00372, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   120, loss: 0.00277, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   140, loss: 4.75619, precision: 1.000 recall: 0.941 f1: 0.970 
training batch:   160, loss: 0.00157, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   180, loss: 0.28206, precision: 0.933 recall: 0.933 f1: 0.933 
training batch:   200, loss: 0.94433, precision: 0.909 recall: 0.833 f1: 0.870 
training batch:   220, loss: 0.00202, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   240, loss: 0.00060, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   260, loss: 0.11854, precision: 1.000 recall: 0.917 f1: 0.957 
training batch:   280, loss: 0.01114, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   300, loss: 0.00181, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   320, loss: 2.25973, precision: 0.909 recall: 0.909 f1: 0.909 
training batch:   340, loss: 0.22895, precision: 0.929 recall: 0.929 f1: 0.929 
training batch:   360, loss: 0.00443, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   380, loss: 0.01813, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   400, loss: 0.00112, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   420, loss: 0.00084, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   440, loss: 0.00150, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   460, loss: 8.70659, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   480, loss: 0.05924, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   500, loss: 0.31893, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   520, loss: 6.82298, precision: 0.818 recall: 0.900 f1: 0.857 
training batch:   540, loss: 0.00666, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   560, loss: 0.40878, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   580, loss: 13.47006, precision: 0.929 recall: 0.929 f1: 0.929 
training batch:   600, loss: 0.00272, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   620, loss: 2.95589, precision: 0.900 recall: 0.900 f1: 0.900 
training batch:   640, loss: 0.02490, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   660, loss: 0.00167, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   680, loss: 0.42472, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   700, loss: 0.00049, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   720, loss: 0.00171, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   740, loss: 0.02511, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   760, loss: 0.00627, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   780, loss: 0.00184, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   800, loss: 2.31377, precision: 1.000 recall: 0.933 f1: 0.966 
training batch:   820, loss: 3.98343, precision: 0.833 recall: 0.909 f1: 0.870 
training batch:   840, loss: 0.00170, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   860, loss: 2.05489, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   880, loss: 0.00088, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   900, loss: 0.00268, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   920, loss: 3.97901, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   940, loss: 0.78065, precision: 1.000 recall: 0.941 f1: 0.970 
training batch:   960, loss: 0.00165, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   980, loss: 0.00423, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1000, loss: 0.00279, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1020, loss: 0.00246, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1040, loss: 0.00594, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1060, loss: 0.55441, precision: 0.929 recall: 0.867 f1: 0.897 
training batch:  1080, loss: 0.05371, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1100, loss: 0.00103, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1120, loss: 0.00199, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1140, loss: 0.00079, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1160, loss: 0.00908, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1180, loss: 4.99439, precision: 1.000 recall: 0.900 f1: 0.947 
training batch:  1200, loss: 0.00069, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1220, loss: 0.03513, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1240, loss: 0.00280, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1260, loss: 0.00203, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1280, loss: 0.00063, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1300, loss: 0.00173, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1320, loss: 0.00073, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1340, loss: 0.00476, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1360, loss: 0.04822, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1380, loss: 0.00144, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1400, loss: 0.00189, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1420, loss: 0.00139, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1440, loss: 0.00052, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1460, loss: 0.00340, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1480, loss: 0.00053, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1500, loss: 0.11299, precision: 0.938 recall: 0.882 f1: 0.909 
training batch:  1520, loss: 0.19654, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1540, loss: 0.00067, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1560, loss: 0.00100, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1580, loss: 0.00053, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1600, loss: 1.59872, precision: 0.889 recall: 0.889 f1: 0.889 
training batch:  1620, loss: 0.00037, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1640, loss: 0.03786, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1660, loss: 0.00437, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1680, loss: 0.00068, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1700, loss: 3.87764, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1720, loss: 0.00083, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1740, loss: 0.00943, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1760, loss: 0.47206, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1780, loss: 0.00261, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1800, loss: 0.01556, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1820, loss: 0.97606, precision: 0.973 recall: 0.947 f1: 0.960 
training batch:  1840, loss: 0.00424, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1860, loss: 0.00099, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1880, loss: 0.00086, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1900, loss: 0.00149, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1920, loss: 0.00097, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1940, loss: 2.58124, precision: 1.000 recall: 0.900 f1: 0.947 
training batch:  1960, loss: 0.00128, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1980, loss: 0.00664, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2000, loss: 0.00141, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2020, loss: 0.00733, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2040, loss: 0.00077, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2060, loss: 5.50084, precision: 0.846 recall: 0.917 f1: 0.880 
training batch:  2080, loss: 1.17061, precision: 0.917 recall: 1.000 f1: 0.957 
training batch:  2100, loss: 0.00155, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2120, loss: 0.00312, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2140, loss: 0.00133, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2160, loss: 10.41308, precision: 0.846 recall: 0.917 f1: 0.880 
training batch:  2180, loss: 0.00325, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2200, loss: 0.24358, precision: 1.000 recall: 0.938 f1: 0.968 
training batch:  2220, loss: 2.58092, precision: 0.909 recall: 0.909 f1: 0.909 
training batch:  2240, loss: 0.00351, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2260, loss: 1.34931, precision: 1.000 recall: 0.900 f1: 0.947 
training batch:  2280, loss: 1.16522, precision: 0.909 recall: 0.909 f1: 0.909 
training batch:  2300, loss: 0.00105, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2320, loss: 4.00990, precision: 0.978 recall: 0.978 f1: 0.978 
training batch:  2340, loss: 1.14355, precision: 1.000 recall: 0.938 f1: 0.968 
training batch:  2360, loss: 0.00191, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2380, loss: 1.56330, precision: 0.917 recall: 1.000 f1: 0.957 
training batch:  2400, loss: 0.68929, precision: 0.923 recall: 0.857 f1: 0.889 
training batch:  2420, loss: 0.00160, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2440, loss: 0.00099, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2460, loss: 0.00288, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2480, loss: 0.00655, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2500, loss: 0.00130, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2520, loss: 2.79245, precision: 0.882 recall: 0.938 f1: 0.909 
training batch:  2540, loss: 0.88237, precision: 0.950 recall: 0.905 f1: 0.927 
training batch:  2560, loss: 9.78575, precision: 0.941 recall: 0.889 f1: 0.914 
training batch:  2580, loss: 2.83632, precision: 0.923 recall: 0.923 f1: 0.923 
training batch:  2600, loss: 0.00736, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2620, loss: 0.00395, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2640, loss: 0.00133, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2660, loss: 0.00095, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2680, loss: 0.00076, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2700, loss: 0.00074, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2720, loss: 0.27367, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2740, loss: 3.41716, precision: 0.923 recall: 0.923 f1: 0.923 
training batch:  2760, loss: 0.16070, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2780, loss: 0.01694, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2800, loss: 0.00064, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2820, loss: 6.84908, precision: 0.818 recall: 0.818 f1: 0.818 
training batch:  2840, loss: 0.00222, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2860, loss: 0.00121, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2880, loss: 0.33739, precision: 0.900 recall: 1.000 f1: 0.947 
training batch:  2900, loss: 2.15690, precision: 0.900 recall: 0.900 f1: 0.900 
training batch:  2920, loss: 0.00332, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2940, loss: 0.56456, precision: 0.857 recall: 0.923 f1: 0.889 
training batch:  2960, loss: 0.64806, precision: 0.909 recall: 0.909 f1: 0.909 
training batch:  2980, loss: 0.00261, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3000, loss: 0.00313, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3020, loss: 0.01712, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3040, loss: 5.84191, precision: 0.917 recall: 0.917 f1: 0.917 
training batch:  3060, loss: 0.00039, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3080, loss: 0.00179, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3100, loss: 0.00178, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3120, loss: 1.58858, precision: 0.917 recall: 0.917 f1: 0.917 
training batch:  3140, loss: 1.59828, precision: 1.000 recall: 0.933 f1: 0.966 
training batch:  3160, loss: 0.00165, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3180, loss: 0.00059, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3200, loss: 0.00144, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3220, loss: 0.17296, precision: 1.000 recall: 0.917 f1: 0.957 
training batch:  3240, loss: 0.00566, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3260, loss: 0.00141, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3280, loss: 0.00087, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3300, loss: 0.08290, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3320, loss: 0.00105, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3340, loss: 0.00259, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3360, loss: 0.54651, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3380, loss: 0.00107, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3400, loss: 0.00314, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3420, loss: 0.00086, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3440, loss: 1.14570, precision: 0.917 recall: 0.917 f1: 0.917 
training batch:  3460, loss: 0.38941, precision: 1.000 recall: 0.955 f1: 0.977 
training batch:  3480, loss: 8.22778, precision: 0.909 recall: 0.769 f1: 0.833 
training batch:  3500, loss: 0.07990, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3520, loss: 0.05835, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3540, loss: 0.05699, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3560, loss: 0.98599, precision: 0.917 recall: 1.000 f1: 0.957 
training batch:  3580, loss: 0.00208, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3600, loss: 1.10704, precision: 0.923 recall: 0.923 f1: 0.923 
training batch:  3620, loss: 0.00539, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3640, loss: 0.00684, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3660, loss: 0.00220, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3680, loss: 1.52349, precision: 0.900 recall: 0.900 f1: 0.900 
training batch:  3700, loss: 9.39585, precision: 1.000 recall: 0.857 f1: 0.923 
training batch:  3720, loss: 0.00114, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3740, loss: 0.12642, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3760, loss: 0.00085, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3780, loss: 0.00088, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3800, loss: 0.00095, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3820, loss: 0.00065, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3840, loss: 0.00079, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3860, loss: 0.00169, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3880, loss: 0.06969, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3900, loss: 0.00108, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3920, loss: 0.00108, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3940, loss: 2.40103, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3960, loss: 0.02403, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3980, loss: 0.04965, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4000, loss: 0.90174, precision: 0.933 recall: 1.000 f1: 0.966 
training batch:  4020, loss: 0.00065, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4040, loss: 0.00425, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4060, loss: 1.68640, precision: 0.929 recall: 0.812 f1: 0.867 
training batch:  4080, loss: 0.00210, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4100, loss: 0.53149, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4120, loss: 0.00389, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4140, loss: 0.00122, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4160, loss: 1.07533, precision: 0.938 recall: 0.938 f1: 0.938 
training batch:  4180, loss: 0.00069, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4200, loss: 0.00146, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4220, loss: 0.78486, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4240, loss: 0.00076, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4260, loss: 0.00866, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4280, loss: 0.05044, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4300, loss: 2.14914, precision: 1.000 recall: 0.909 f1: 0.952 
training batch:  4320, loss: 0.74313, precision: 0.933 recall: 0.933 f1: 0.933 
training batch:  4340, loss: 0.00112, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4360, loss: 0.00144, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4380, loss: 0.00465, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4400, loss: 0.00189, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4420, loss: 0.72567, precision: 0.938 recall: 0.882 f1: 0.909 
training batch:  4440, loss: 0.00041, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4460, loss: 0.00583, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4480, loss: 0.00151, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4500, loss: 0.00108, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4520, loss: 0.13130, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4540, loss: 0.04521, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4560, loss: 0.00155, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4580, loss: 0.00099, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4600, loss: 1.09940, precision: 0.917 recall: 0.917 f1: 0.917 
training batch:  4620, loss: 0.00063, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4640, loss: 2.20885, precision: 0.929 recall: 0.867 f1: 0.897 
training batch:  4660, loss: 2.99139, precision: 0.909 recall: 0.909 f1: 0.909 
training batch:  4680, loss: 2.10579, precision: 0.917 recall: 0.846 f1: 0.880 
training batch:  4700, loss: 0.85871, precision: 0.909 recall: 0.909 f1: 0.909 
start evaluate engines...
label: ORG, precision: 0.756 recall: 0.776 f1: 0.760 
label: PERSON, precision: 0.822 recall: 0.810 f1: 0.814 
label: LOC, precision: 0.850 recall: 0.872 f1: 0.855 
time consumption:21.36(min), precision: 0.950 recall: 0.960 f1: 0.954 
saved the new best model with f1: 0.954
loading data...
epoch:5/30
training batch:    20, loss: 0.05108, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    40, loss: 0.00054, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    60, loss: 0.00034, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    80, loss: 0.00054, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   100, loss: 0.00284, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   120, loss: 0.06896, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   140, loss: 0.00147, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   160, loss: 0.00088, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   180, loss: 0.00067, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   200, loss: 0.00064, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   220, loss: 1.37813, precision: 0.882 recall: 0.938 f1: 0.909 
training batch:   240, loss: 0.26405, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   260, loss: 0.01814, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   280, loss: 0.74223, precision: 1.000 recall: 0.900 f1: 0.947 
training batch:   300, loss: 0.00070, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   320, loss: 0.00152, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   340, loss: 0.00046, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   360, loss: 0.00102, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   380, loss: 1.33900, precision: 1.000 recall: 0.941 f1: 0.970 
training batch:   400, loss: 0.00070, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   420, loss: 0.00152, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   440, loss: 0.93519, precision: 1.000 recall: 0.947 f1: 0.973 
training batch:   460, loss: 0.00095, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   480, loss: 0.95863, precision: 0.909 recall: 0.909 f1: 0.909 
training batch:   500, loss: 5.47591, precision: 1.000 recall: 0.938 f1: 0.968 
training batch:   520, loss: 0.41826, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   540, loss: 0.38282, precision: 0.889 recall: 0.889 f1: 0.889 
training batch:   560, loss: 0.00054, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   580, loss: 9.28735, precision: 0.955 recall: 0.955 f1: 0.955 
training batch:   600, loss: 0.00042, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   620, loss: 0.00099, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   640, loss: 0.00130, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   660, loss: 0.00027, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   680, loss: 0.00025, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   700, loss: 0.00034, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   720, loss: 0.00046, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   740, loss: 0.00123, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   760, loss: 0.00043, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   780, loss: 0.00027, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   800, loss: 0.00077, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   820, loss: 0.00042, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   840, loss: 0.34353, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   860, loss: 0.00029, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   880, loss: 8.05264, precision: 0.846 recall: 0.786 f1: 0.815 
training batch:   900, loss: 0.00087, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   920, loss: 0.00637, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   940, loss: 0.00076, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   960, loss: 0.00122, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   980, loss: 0.00472, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1000, loss: 0.00089, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1020, loss: 0.00015, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1040, loss: 0.00240, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1060, loss: 0.00119, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1080, loss: 0.00115, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1100, loss: 0.00046, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1120, loss: 4.61398, precision: 0.933 recall: 0.933 f1: 0.933 
training batch:  1140, loss: 0.00065, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1160, loss: 0.01988, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1180, loss: 0.00044, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1200, loss: 0.00115, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1220, loss: 0.05885, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1240, loss: 0.02051, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1260, loss: 0.51058, precision: 0.926 recall: 0.962 f1: 0.943 
training batch:  1280, loss: 0.00126, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1300, loss: 0.00058, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1320, loss: 0.00074, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1340, loss: 0.00050, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1360, loss: 0.00111, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1380, loss: 0.04196, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1400, loss: 3.92939, precision: 1.000 recall: 0.917 f1: 0.957 
training batch:  1420, loss: 0.00086, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1440, loss: 0.00715, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1460, loss: 0.00156, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1480, loss: 1.26386, precision: 0.952 recall: 1.000 f1: 0.976 
training batch:  1500, loss: 0.00173, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1520, loss: 0.03516, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1540, loss: 0.00088, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1560, loss: 0.00033, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1580, loss: 3.30298, precision: 0.923 recall: 0.923 f1: 0.923 
training batch:  1600, loss: 0.00048, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1620, loss: 0.00153, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1640, loss: 0.01381, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1660, loss: 0.00071, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1680, loss: 0.00046, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1700, loss: 0.13931, precision: 0.929 recall: 0.929 f1: 0.929 
training batch:  1720, loss: 0.75050, precision: 1.000 recall: 0.917 f1: 0.957 
training batch:  1740, loss: 1.06607, precision: 0.900 recall: 1.000 f1: 0.947 
training batch:  1760, loss: 0.00192, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1780, loss: 0.00048, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1800, loss: 0.00039, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1820, loss: 0.00038, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1840, loss: 0.00025, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1860, loss: 0.00116, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1880, loss: 14.11176, precision: 0.889 recall: 0.800 f1: 0.842 
training batch:  1900, loss: 2.09447, precision: 0.938 recall: 0.938 f1: 0.938 
training batch:  1920, loss: 1.83621, precision: 0.933 recall: 0.933 f1: 0.933 
training batch:  1940, loss: 0.00116, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1960, loss: 0.00060, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1980, loss: 1.93283, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2000, loss: 0.00061, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2020, loss: 0.00044, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2040, loss: 5.04777, precision: 1.000 recall: 0.857 f1: 0.923 
training batch:  2060, loss: 0.00116, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2080, loss: 1.50046, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2100, loss: 0.00081, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2120, loss: 1.43309, precision: 0.857 recall: 0.857 f1: 0.857 
training batch:  2140, loss: 0.00059, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2160, loss: 0.00211, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2180, loss: 0.00073, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2200, loss: 2.87817, precision: 0.882 recall: 0.938 f1: 0.909 
training batch:  2220, loss: 1.72313, precision: 0.950 recall: 1.000 f1: 0.974 
training batch:  2240, loss: 0.00080, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2260, loss: 0.00031, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2280, loss: 0.03925, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2300, loss: 0.09241, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2320, loss: 0.00035, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2340, loss: 0.00067, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2360, loss: 0.00038, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2380, loss: 0.01997, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2400, loss: 0.35211, precision: 0.882 recall: 0.938 f1: 0.909 
training batch:  2420, loss: 0.00183, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2440, loss: 0.00271, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2460, loss: 0.00078, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2480, loss: 0.00098, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2500, loss: 0.04022, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2520, loss: 0.03942, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2540, loss: 0.00047, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2560, loss: 0.00110, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2580, loss: 1.09775, precision: 0.938 recall: 0.882 f1: 0.909 
training batch:  2600, loss: 0.00618, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2620, loss: 0.00036, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2640, loss: 0.00348, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2660, loss: 0.00132, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2680, loss: 0.00149, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2700, loss: 0.00034, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2720, loss: 0.00154, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2740, loss: 0.00103, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2760, loss: 0.04133, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2780, loss: 0.73294, precision: 1.000 recall: 0.941 f1: 0.970 
training batch:  2800, loss: 0.00187, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2820, loss: 0.00201, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2840, loss: 0.00433, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2860, loss: 0.00223, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2880, loss: 0.00338, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2900, loss: 0.00030, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2920, loss: 0.00026, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2940, loss: 0.00040, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2960, loss: 0.00119, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2980, loss: 0.00088, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3000, loss: 0.08473, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3020, loss: 0.00156, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3040, loss: 0.00060, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3060, loss: 0.03131, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3080, loss: 0.26511, precision: 0.933 recall: 0.933 f1: 0.933 
training batch:  3100, loss: 0.00035, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3120, loss: 0.00210, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3140, loss: 1.25249, precision: 1.000 recall: 0.933 f1: 0.966 
training batch:  3160, loss: 0.00468, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3180, loss: 0.05710, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3200, loss: 0.00140, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3220, loss: 0.00055, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3240, loss: 0.00960, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3260, loss: 0.25257, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3280, loss: 0.00075, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3300, loss: 0.00550, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3320, loss: 0.02468, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3340, loss: 0.34169, precision: 1.000 recall: 0.938 f1: 0.968 
training batch:  3360, loss: 0.00107, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3380, loss: 0.00044, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3400, loss: 0.00071, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3420, loss: 0.41165, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3440, loss: 0.00760, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3460, loss: 0.00769, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3480, loss: 0.00060, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3500, loss: 0.00025, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3520, loss: 0.00065, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3540, loss: 0.00075, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3560, loss: 0.00088, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3580, loss: 0.00035, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3600, loss: 0.00035, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3620, loss: 0.04315, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3640, loss: 0.01070, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3660, loss: 10.59779, precision: 0.778 recall: 0.778 f1: 0.778 
training batch:  3680, loss: 0.00066, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3700, loss: 0.02160, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3720, loss: 0.00057, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3740, loss: 0.83361, precision: 0.900 recall: 0.900 f1: 0.900 
training batch:  3760, loss: 1.31600, precision: 0.941 recall: 0.889 f1: 0.914 
training batch:  3780, loss: 0.00505, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3800, loss: 0.00055, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3820, loss: 0.00863, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3840, loss: 0.00029, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3860, loss: 0.98927, precision: 0.929 recall: 1.000 f1: 0.963 
training batch:  3880, loss: 0.00029, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3900, loss: 0.00160, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3920, loss: 0.00061, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3940, loss: 0.00030, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3960, loss: 0.00228, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3980, loss: 0.14118, precision: 0.947 recall: 0.947 f1: 0.947 
training batch:  4000, loss: 0.00202, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4020, loss: 0.00088, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4040, loss: 0.00149, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4060, loss: 0.40740, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4080, loss: 0.00025, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4100, loss: 0.00257, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4120, loss: 0.00044, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4140, loss: 0.00049, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4160, loss: 4.90072, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4180, loss: 0.00044, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4200, loss: 0.00031, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4220, loss: 0.00050, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4240, loss: 0.00350, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4260, loss: 0.14356, precision: 0.867 recall: 0.929 f1: 0.897 
training batch:  4280, loss: 8.75203, precision: 0.941 recall: 0.941 f1: 0.941 
training batch:  4300, loss: 0.00073, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4320, loss: 0.00139, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4340, loss: 0.00026, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4360, loss: 0.00138, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4380, loss: 0.80154, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4400, loss: 0.00055, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4420, loss: 0.00023, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4440, loss: 1.87029, precision: 0.955 recall: 0.955 f1: 0.955 
training batch:  4460, loss: 0.00024, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4480, loss: 0.00036, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4500, loss: 0.00083, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4520, loss: 0.00065, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4540, loss: 0.00105, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4560, loss: 0.00084, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4580, loss: 0.24434, precision: 0.941 recall: 1.000 f1: 0.970 
training batch:  4600, loss: 0.00083, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4620, loss: 0.00028, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4640, loss: 9.15729, precision: 0.786 recall: 0.917 f1: 0.846 
training batch:  4660, loss: 0.00064, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4680, loss: 0.00042, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4700, loss: 0.00080, precision: 1.000 recall: 1.000 f1: 1.000 
start evaluate engines...
label: ORG, precision: 0.764 recall: 0.770 f1: 0.763 
label: PERSON, precision: 0.820 recall: 0.819 f1: 0.817 
label: LOC, precision: 0.861 recall: 0.873 f1: 0.862 
time consumption:21.42(min), precision: 0.952 recall: 0.959 f1: 0.955 
saved the new best model with f1: 0.955
loading data...
epoch:6/30
training batch:    20, loss: 2.16884, precision: 0.933 recall: 0.933 f1: 0.933 
training batch:    40, loss: 0.00159, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    60, loss: 0.00027, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    80, loss: 0.00028, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   100, loss: 0.90139, precision: 0.857 recall: 0.923 f1: 0.889 
training batch:   120, loss: 0.00083, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   140, loss: 0.00242, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   160, loss: 0.00313, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   180, loss: 0.00048, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   200, loss: 4.36319, precision: 0.909 recall: 0.909 f1: 0.909 
training batch:   220, loss: 0.00052, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   240, loss: 0.00258, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   260, loss: 0.00059, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   280, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   300, loss: 0.00029, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   320, loss: 0.00013, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   340, loss: 0.94653, precision: 0.923 recall: 1.000 f1: 0.960 
training batch:   360, loss: 0.00039, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   380, loss: 0.10796, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   400, loss: 0.00018, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   420, loss: 0.00031, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   440, loss: 5.33189, precision: 1.000 recall: 0.941 f1: 0.970 
training batch:   460, loss: 0.00026, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   480, loss: 0.00039, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   500, loss: 0.00021, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   520, loss: 1.73837, precision: 0.833 recall: 0.909 f1: 0.870 
training batch:   540, loss: 0.00026, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   560, loss: 0.23576, precision: 0.929 recall: 0.929 f1: 0.929 
training batch:   580, loss: 0.00131, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   600, loss: 0.00057, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   620, loss: 1.59443, precision: 0.941 recall: 1.000 f1: 0.970 
training batch:   640, loss: 0.00063, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   660, loss: 0.00015, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   680, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   700, loss: 0.00013, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   720, loss: 0.00981, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   740, loss: 0.00097, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   760, loss: 0.29113, precision: 0.909 recall: 0.909 f1: 0.909 
training batch:   780, loss: 3.99090, precision: 0.917 recall: 1.000 f1: 0.957 
training batch:   800, loss: 0.00021, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   820, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   840, loss: 0.00018, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   860, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   880, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   900, loss: 0.00024, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   920, loss: 0.00026, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   940, loss: 0.02372, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   960, loss: 0.00031, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   980, loss: 0.00024, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1000, loss: 0.00018, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1020, loss: 0.02266, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1040, loss: 0.00088, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1060, loss: 0.00043, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1080, loss: 0.00036, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1100, loss: 0.00036, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1120, loss: 0.01974, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1140, loss: 0.00048, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1160, loss: 0.00114, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1180, loss: 1.02004, precision: 0.933 recall: 0.933 f1: 0.933 
training batch:  1200, loss: 0.04345, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1220, loss: 0.25534, precision: 0.929 recall: 1.000 f1: 0.963 
training batch:  1240, loss: 0.00025, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1260, loss: 0.00097, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1280, loss: 0.00103, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1300, loss: 1.07588, precision: 0.909 recall: 0.909 f1: 0.909 
training batch:  1320, loss: 2.26840, precision: 0.900 recall: 0.900 f1: 0.900 
training batch:  1340, loss: 0.00168, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1360, loss: 0.00038, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1380, loss: 0.00029, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1400, loss: 2.74112, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1420, loss: 0.00013, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1440, loss: 0.00212, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1460, loss: 7.37181, precision: 0.800 recall: 0.889 f1: 0.842 
training batch:  1480, loss: 0.00021, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1500, loss: 0.00109, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1520, loss: 0.00315, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1540, loss: 0.00145, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1560, loss: 0.00023, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1580, loss: 0.00021, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1600, loss: 0.08481, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1620, loss: 0.03400, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1640, loss: 0.00023, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1660, loss: 0.00038, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1680, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1700, loss: 0.00062, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1720, loss: 0.00053, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1740, loss: 0.00036, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1760, loss: 0.49022, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1780, loss: 0.00018, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1800, loss: 0.00026, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1820, loss: 0.00072, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1840, loss: 0.00019, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1860, loss: 0.00213, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1880, loss: 0.00036, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1900, loss: 0.00924, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1920, loss: 0.98891, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1940, loss: 2.15566, precision: 0.846 recall: 0.917 f1: 0.880 
training batch:  1960, loss: 0.00027, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1980, loss: 0.00307, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2000, loss: 0.00540, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2020, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2040, loss: 0.00120, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2060, loss: 0.00042, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2080, loss: 0.10610, precision: 1.000 recall: 0.938 f1: 0.968 
training batch:  2100, loss: 0.00024, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2120, loss: 3.60366, precision: 0.933 recall: 1.000 f1: 0.966 
training batch:  2140, loss: 0.00031, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2160, loss: 0.00013, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2180, loss: 0.00032, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2200, loss: 0.00021, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2220, loss: 0.00039, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2240, loss: 14.87249, precision: 0.833 recall: 0.909 f1: 0.870 
training batch:  2260, loss: 0.00033, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2280, loss: 0.00020, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2300, loss: 0.00124, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2320, loss: 0.00034, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2340, loss: 1.31410, precision: 1.000 recall: 0.952 f1: 0.976 
training batch:  2360, loss: 0.00018, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2380, loss: 0.00089, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2400, loss: 0.00015, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2420, loss: 0.00212, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2440, loss: 0.00030, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2460, loss: 0.00252, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2480, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2500, loss: 0.00102, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2520, loss: 0.00036, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2540, loss: 0.00056, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2560, loss: 0.00041, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2580, loss: 0.03754, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2600, loss: 0.00069, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2620, loss: 0.00020, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2640, loss: 0.00030, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2660, loss: 0.00040, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2680, loss: 0.00049, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2700, loss: 0.00054, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2720, loss: 0.00048, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2740, loss: 0.00245, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2760, loss: 0.00026, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2780, loss: 5.41396, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2800, loss: 0.00017, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2820, loss: 0.02004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2840, loss: 0.64078, precision: 0.917 recall: 0.917 f1: 0.917 
training batch:  2860, loss: 0.13826, precision: 0.909 recall: 0.909 f1: 0.909 
training batch:  2880, loss: 0.00024, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2900, loss: 0.12798, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2920, loss: 0.00021, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2940, loss: 0.79241, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2960, loss: 0.00025, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2980, loss: 0.00018, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3000, loss: 0.00032, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3020, loss: 0.00103, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3040, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3060, loss: 0.00023, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3080, loss: 0.00064, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3100, loss: 0.00332, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3120, loss: 0.00094, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3140, loss: 0.01782, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3160, loss: 0.00020, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3180, loss: 0.00021, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3200, loss: 0.00062, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3220, loss: 0.10769, precision: 0.944 recall: 0.944 f1: 0.944 
training batch:  3240, loss: 0.45912, precision: 0.900 recall: 0.900 f1: 0.900 
training batch:  3260, loss: 0.00705, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3280, loss: 0.00036, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3300, loss: 1.04769, precision: 0.889 recall: 0.727 f1: 0.800 
training batch:  3320, loss: 0.00021, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3340, loss: 3.25340, precision: 0.857 recall: 0.923 f1: 0.889 
training batch:  3360, loss: 0.00215, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3380, loss: 0.00266, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3400, loss: 1.93259, precision: 0.955 recall: 0.955 f1: 0.955 
training batch:  3420, loss: 2.00713, precision: 0.857 recall: 0.923 f1: 0.889 
training batch:  3440, loss: 0.00023, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3460, loss: 0.00082, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3480, loss: 0.00083, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3500, loss: 0.00495, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3520, loss: 1.41183, precision: 0.938 recall: 0.938 f1: 0.938 
training batch:  3540, loss: 1.27715, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3560, loss: 0.97490, precision: 1.000 recall: 0.867 f1: 0.929 
training batch:  3580, loss: 0.00846, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3600, loss: 0.00035, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3620, loss: 0.00092, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3640, loss: 0.00097, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3660, loss: 0.00076, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3680, loss: 3.76241, precision: 0.900 recall: 0.900 f1: 0.900 
training batch:  3700, loss: 9.77151, precision: 0.944 recall: 0.944 f1: 0.944 
training batch:  3720, loss: 0.00053, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3740, loss: 0.00095, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3760, loss: 0.00057, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3780, loss: 0.00514, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3800, loss: 0.00027, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3820, loss: 0.00037, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3840, loss: 0.00021, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3860, loss: 0.00100, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3880, loss: 1.09066, precision: 0.900 recall: 0.900 f1: 0.900 
training batch:  3900, loss: 0.00103, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3920, loss: 0.00023, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3940, loss: 0.00029, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3960, loss: 0.00051, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3980, loss: 0.00154, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4000, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4020, loss: 0.00031, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4040, loss: 0.61955, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4060, loss: 0.00045, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4080, loss: 0.00033, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4100, loss: 0.02687, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4120, loss: 0.00218, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4140, loss: 0.26756, precision: 0.944 recall: 0.944 f1: 0.944 
training batch:  4160, loss: 0.00756, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4180, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4200, loss: 0.00017, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4220, loss: 0.00027, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4240, loss: 0.00022, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4260, loss: 0.00171, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4280, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4300, loss: 0.01638, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4320, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4340, loss: 1.57334, precision: 0.727 recall: 0.889 f1: 0.800 
training batch:  4360, loss: 0.05014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4380, loss: 0.00070, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4400, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4420, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4440, loss: 0.00079, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4460, loss: 0.00019, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4480, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4500, loss: 0.00024, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4520, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4540, loss: 0.00061, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4560, loss: 0.00028, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4580, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4600, loss: 3.43150, precision: 1.000 recall: 0.944 f1: 0.971 
training batch:  4620, loss: 0.00017, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4640, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4660, loss: 0.00020, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4680, loss: 1.13883, precision: 0.938 recall: 0.882 f1: 0.909 
training batch:  4700, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
start evaluate engines...
label: ORG, precision: 0.758 recall: 0.775 f1: 0.760 
label: PERSON, precision: 0.823 recall: 0.822 f1: 0.821 
label: LOC, precision: 0.863 recall: 0.872 f1: 0.863 
time consumption:21.38(min), precision: 0.947 recall: 0.959 f1: 0.952 
loading data...
epoch:7/30
training batch:    20, loss: 15.20632, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    40, loss: 0.00045, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    60, loss: 0.00023, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    80, loss: 0.02152, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   100, loss: 0.00972, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   120, loss: 0.84160, precision: 0.944 recall: 0.944 f1: 0.944 
training batch:   140, loss: 0.00026, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   160, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   180, loss: 0.00065, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   200, loss: 0.00063, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   220, loss: 0.00017, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   240, loss: 0.00117, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   260, loss: 0.00023, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   280, loss: 0.00099, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   300, loss: 0.01336, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   320, loss: -0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   340, loss: 0.00025, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   360, loss: 0.00039, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   380, loss: 0.00036, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   400, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   420, loss: 0.00159, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   440, loss: 2.27724, precision: 1.000 recall: 0.960 f1: 0.980 
training batch:   460, loss: 0.00039, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   480, loss: 0.01741, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   500, loss: 0.00026, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   520, loss: 0.05083, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   540, loss: 0.00098, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   560, loss: 0.00061, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   580, loss: 0.00013, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   600, loss: 0.00022, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   620, loss: 0.00023, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   640, loss: 0.00020, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   660, loss: 0.00039, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   680, loss: 0.00852, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   700, loss: 0.00083, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   720, loss: 0.00013, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   740, loss: 0.00077, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   760, loss: 0.00022, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   780, loss: 0.00051, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   800, loss: 0.00157, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   820, loss: 2.35198, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   840, loss: 7.23724, precision: 0.929 recall: 0.929 f1: 0.929 
training batch:   860, loss: 0.00185, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   880, loss: 0.00017, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   900, loss: 0.00024, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   920, loss: 0.00017, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   940, loss: 0.00390, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   960, loss: 0.00039, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   980, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1000, loss: 0.00096, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1020, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1040, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1060, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1080, loss: 0.00034, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1100, loss: 0.00017, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1120, loss: 1.68569, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1140, loss: 0.00078, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1160, loss: 0.00019, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1180, loss: 0.00038, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1200, loss: 0.00027, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1220, loss: 3.53656, precision: 0.917 recall: 0.786 f1: 0.846 
training batch:  1240, loss: 0.00104, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1260, loss: 0.00667, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1280, loss: 0.00017, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1300, loss: 0.00023, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1320, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1340, loss: 0.00032, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1360, loss: 0.00025, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1380, loss: 0.00021, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1400, loss: 0.00029, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1420, loss: 0.00992, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1440, loss: 0.00024, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1460, loss: 0.00024, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1480, loss: 0.67209, precision: 0.917 recall: 0.846 f1: 0.880 
training batch:  1500, loss: 1.33437, precision: 0.909 recall: 0.909 f1: 0.909 
training batch:  1520, loss: 0.09307, precision: 0.900 recall: 0.818 f1: 0.857 
training batch:  1540, loss: 0.00074, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1560, loss: 0.00054, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1580, loss: 0.00067, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1600, loss: 0.00062, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1620, loss: 1.52405, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1640, loss: 0.00054, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1660, loss: 0.00036, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1680, loss: 0.00020, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1700, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1720, loss: 0.00668, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1740, loss: 0.00046, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1760, loss: 0.00020, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1780, loss: 0.00030, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1800, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1820, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1840, loss: 0.00024, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1860, loss: 0.22276, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1880, loss: 0.00037, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1900, loss: 0.73279, precision: 0.882 recall: 0.938 f1: 0.909 
training batch:  1920, loss: 0.00080, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1940, loss: 0.00037, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1960, loss: 0.00029, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1980, loss: 0.00023, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2000, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2020, loss: 0.00017, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2040, loss: 0.01740, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2060, loss: 0.00030, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2080, loss: 0.00030, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2100, loss: 2.21596, precision: 0.944 recall: 0.944 f1: 0.944 
training batch:  2120, loss: 6.13048, precision: 0.882 recall: 0.833 f1: 0.857 
training batch:  2140, loss: 0.06190, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2160, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2180, loss: 0.00016, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2200, loss: 0.00042, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2220, loss: 0.00037, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2240, loss: 0.00017, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2260, loss: 1.00250, precision: 0.929 recall: 0.929 f1: 0.929 
training batch:  2280, loss: 0.00019, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2300, loss: 0.00029, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2320, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2340, loss: 0.00021, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2360, loss: 0.00056, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2380, loss: 0.00019, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2400, loss: 0.04025, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2420, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2440, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2460, loss: 0.00118, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2480, loss: 0.00347, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2500, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2520, loss: 0.00023, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2540, loss: 0.00023, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2560, loss: 0.00024, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2580, loss: 0.00044, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2600, loss: 0.00043, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2620, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2640, loss: 0.00031, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2660, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2680, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2700, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2720, loss: 0.00026, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2740, loss: 0.00016, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2760, loss: 0.00410, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2780, loss: 0.88198, precision: 0.875 recall: 0.933 f1: 0.903 
training batch:  2800, loss: 0.02952, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2820, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2840, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2860, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2880, loss: 0.00032, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2900, loss: 0.00128, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2920, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2940, loss: 0.00340, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2960, loss: 0.38058, precision: 0.933 recall: 1.000 f1: 0.966 
training batch:  2980, loss: 0.00067, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3000, loss: 0.00079, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3020, loss: 0.00016, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3040, loss: 0.00020, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3060, loss: 0.00104, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3080, loss: 0.00031, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3100, loss: 0.00016, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3120, loss: 0.00340, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3140, loss: 0.00013, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3160, loss: 0.00105, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3180, loss: 0.00030, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3200, loss: 0.01226, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3220, loss: 0.00025, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3240, loss: 0.00050, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3260, loss: 0.00036, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3280, loss: 0.00026, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3300, loss: 0.00027, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3320, loss: 0.00015, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3340, loss: 0.00019, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3360, loss: 4.15386, precision: 0.944 recall: 0.944 f1: 0.944 
training batch:  3380, loss: 0.00037, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3400, loss: 0.00045, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3420, loss: 0.00038, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3440, loss: 5.33487, precision: 0.889 recall: 0.889 f1: 0.889 
training batch:  3460, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3480, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3500, loss: 0.99714, precision: 0.900 recall: 1.000 f1: 0.947 
training batch:  3520, loss: 7.03134, precision: 0.889 recall: 0.889 f1: 0.889 
training batch:  3540, loss: 0.00029, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3560, loss: 0.00023, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3580, loss: 0.00020, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3600, loss: 0.00019, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3620, loss: 0.00073, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3640, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3660, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3680, loss: 0.00019, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3700, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3720, loss: 0.00093, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3740, loss: 0.02241, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3760, loss: 2.16583, precision: 0.950 recall: 1.000 f1: 0.974 
training batch:  3780, loss: 0.55327, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3800, loss: 0.00035, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3820, loss: 0.00025, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3840, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3860, loss: 0.00016, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3880, loss: 0.34347, precision: 1.000 recall: 0.917 f1: 0.957 
training batch:  3900, loss: 0.00913, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3920, loss: 0.00129, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3940, loss: 1.32780, precision: 1.000 recall: 0.929 f1: 0.963 
training batch:  3960, loss: 0.00018, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3980, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4000, loss: 0.02878, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4020, loss: 0.15321, precision: 0.900 recall: 0.900 f1: 0.900 
training batch:  4040, loss: 0.00033, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4060, loss: 0.00052, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4080, loss: 2.70343, precision: 1.000 recall: 0.833 f1: 0.909 
training batch:  4100, loss: 0.00017, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4120, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4140, loss: 0.00017, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4160, loss: 0.00145, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4180, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4200, loss: 0.00063, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4220, loss: 0.00017, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4240, loss: 0.00186, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4260, loss: 0.00075, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4280, loss: 0.00039, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4300, loss: 0.00099, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4320, loss: 0.00022, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4340, loss: 0.00040, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4360, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4380, loss: 2.77816, precision: 1.000 recall: 0.923 f1: 0.960 
training batch:  4400, loss: 0.00099, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4420, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4440, loss: 0.00034, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4460, loss: 0.00044, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4480, loss: 0.00307, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4500, loss: 0.00016, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4520, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4540, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4560, loss: 0.00122, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4580, loss: 0.00024, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4600, loss: 0.00029, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4620, loss: 0.00018, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4640, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4660, loss: 0.00028, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4680, loss: 0.00132, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4700, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
start evaluate engines...
label: ORG, precision: 0.759 recall: 0.772 f1: 0.759 
label: PERSON, precision: 0.828 recall: 0.811 f1: 0.817 
label: LOC, precision: 0.864 recall: 0.865 f1: 0.860 
time consumption:21.35(min), precision: 0.956 recall: 0.953 f1: 0.954 
loading data...
epoch:8/30
training batch:    20, loss: 0.00027, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    40, loss: 0.00021, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    60, loss: 0.00013, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    80, loss: 0.00019, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   100, loss: 0.00013, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   120, loss: 0.00303, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   140, loss: 0.00021, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   160, loss: 0.00037, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   180, loss: 0.00029, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   200, loss: 0.00085, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   220, loss: 0.00037, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   240, loss: 0.00115, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   260, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   280, loss: 0.00027, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   300, loss: 0.00050, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   320, loss: 0.87273, precision: 0.941 recall: 1.000 f1: 0.970 
training batch:   340, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   360, loss: 2.33473, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   380, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   400, loss: 0.00022, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   420, loss: 0.00033, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   440, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   460, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   480, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   500, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   520, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   540, loss: 0.00023, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   560, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   580, loss: 0.00019, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   600, loss: 0.00070, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   620, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   640, loss: 1.00150, precision: 0.909 recall: 0.909 f1: 0.909 
training batch:   660, loss: 0.00026, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   680, loss: 0.00079, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   700, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   720, loss: 0.00013, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   740, loss: 7.51132, precision: 0.846 recall: 0.917 f1: 0.880 
training batch:   760, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   780, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   800, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   820, loss: 0.00048, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   840, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   860, loss: 0.25931, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   880, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   900, loss: 0.00048, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   920, loss: 0.00134, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   940, loss: 0.00025, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   960, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   980, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1000, loss: 0.00015, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1020, loss: 0.00025, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1040, loss: 0.00025, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1060, loss: 0.00096, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1080, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1100, loss: 0.00066, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1120, loss: 0.00016, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1140, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1160, loss: 0.00083, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1180, loss: 0.00041, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1200, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1220, loss: 0.51948, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1240, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1260, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1280, loss: 0.00134, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1300, loss: 0.00020, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1320, loss: 1.01679, precision: 0.938 recall: 0.938 f1: 0.938 
training batch:  1340, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1360, loss: 0.00032, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1380, loss: 0.00022, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1400, loss: 0.00017, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1420, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1440, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1460, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1480, loss: 0.00017, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1500, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1520, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1540, loss: 0.00040, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1560, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1580, loss: 0.00123, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1600, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1620, loss: 0.00017, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1640, loss: 0.00058, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1660, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1680, loss: 0.00035, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1700, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1720, loss: 0.00023, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1740, loss: 0.00022, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1760, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1780, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1800, loss: 0.00060, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1820, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1840, loss: 0.00100, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1860, loss: 0.00039, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1880, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1900, loss: 0.62716, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1920, loss: 5.89279, precision: 1.000 recall: 0.933 f1: 0.966 
training batch:  1940, loss: 0.00115, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1960, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1980, loss: 0.00013, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2000, loss: 0.00170, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2020, loss: 0.00025, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2040, loss: 0.00054, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2060, loss: 1.04535, precision: 1.000 recall: 0.962 f1: 0.980 
training batch:  2080, loss: 0.00041, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2100, loss: 1.20540, precision: 0.875 recall: 0.875 f1: 0.875 
training batch:  2120, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2140, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2160, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2180, loss: 0.00112, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2200, loss: 0.00018, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2220, loss: 0.00013, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2240, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2260, loss: 0.00535, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2280, loss: 0.00030, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2300, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2320, loss: 0.00021, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2340, loss: 0.00018, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2360, loss: 0.00015, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2380, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2400, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2420, loss: 0.00021, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2440, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2460, loss: 0.00013, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2480, loss: 0.00027, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2500, loss: 1.05396, precision: 0.867 recall: 0.929 f1: 0.897 
training batch:  2520, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2540, loss: 0.00121, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2560, loss: 0.00087, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2580, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2600, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2620, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2640, loss: 0.00232, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2660, loss: 0.83240, precision: 0.929 recall: 0.867 f1: 0.897 
training batch:  2680, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2700, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2720, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2740, loss: 0.00038, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2760, loss: 0.00052, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2780, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2800, loss: 0.00023, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2820, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2840, loss: 5.69799, precision: 0.938 recall: 0.938 f1: 0.938 
training batch:  2860, loss: 0.00161, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2880, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2900, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2920, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2940, loss: 0.00154, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2960, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2980, loss: 0.00013, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3000, loss: 0.00220, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3020, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3040, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3060, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3080, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3100, loss: 0.00019, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3120, loss: 0.00039, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3140, loss: 0.00048, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3160, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3180, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3200, loss: 0.00035, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3220, loss: 0.00016, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3240, loss: 0.73254, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3260, loss: 0.00256, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3280, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3300, loss: 12.29128, precision: 1.000 recall: 0.950 f1: 0.974 
training batch:  3320, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3340, loss: 0.19440, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3360, loss: 0.00038, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3380, loss: 0.00019, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3400, loss: 0.00022, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3420, loss: 0.00017, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3440, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3460, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3480, loss: 0.00024, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3500, loss: 4.35765, precision: 0.938 recall: 0.938 f1: 0.938 
training batch:  3520, loss: 0.06096, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3540, loss: 0.07954, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3560, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3580, loss: 0.00484, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3600, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3620, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3640, loss: -0.00018, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3660, loss: 0.11784, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3680, loss: 0.82274, precision: 0.950 recall: 0.950 f1: 0.950 
training batch:  3700, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3720, loss: 0.00048, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3740, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3760, loss: 0.00050, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3780, loss: 0.01266, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3800, loss: 0.00019, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3820, loss: 2.02505, precision: 0.929 recall: 1.000 f1: 0.963 
training batch:  3840, loss: 0.00029, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3860, loss: 0.00019, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3880, loss: 0.00090, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3900, loss: 1.20158, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3920, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3940, loss: 0.00013, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3960, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3980, loss: 0.00070, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4000, loss: 0.00024, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4020, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4040, loss: 0.00055, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4060, loss: 0.00030, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4080, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4100, loss: 0.00049, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4120, loss: 0.58611, precision: 0.981 recall: 0.962 f1: 0.971 
training batch:  4140, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4160, loss: 0.00115, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4180, loss: 2.79418, precision: 0.929 recall: 1.000 f1: 0.963 
training batch:  4200, loss: 0.05207, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4220, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4240, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4260, loss: 0.96525, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4280, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4300, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4320, loss: 0.00020, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4340, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4360, loss: 0.00165, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4380, loss: 0.66062, precision: 0.929 recall: 0.929 f1: 0.929 
training batch:  4400, loss: 1.63917, precision: 0.909 recall: 1.000 f1: 0.952 
training batch:  4420, loss: 0.00056, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4440, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4460, loss: 0.18287, precision: 0.900 recall: 0.900 f1: 0.900 
training batch:  4480, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4500, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4520, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4540, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4560, loss: 6.82945, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4580, loss: 0.00026, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4600, loss: 2.73524, precision: 0.917 recall: 1.000 f1: 0.957 
training batch:  4620, loss: 0.27360, precision: 0.917 recall: 0.917 f1: 0.917 
training batch:  4640, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4660, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4680, loss: 0.00019, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4700, loss: 1.89270, precision: 1.000 recall: 1.000 f1: 1.000 
start evaluate engines...
label: ORG, precision: 0.756 recall: 0.768 f1: 0.757 
label: PERSON, precision: 0.827 recall: 0.821 f1: 0.822 
label: LOC, precision: 0.861 recall: 0.866 f1: 0.859 
time consumption:21.35(min), precision: 0.953 recall: 0.956 f1: 0.954 
loading data...
epoch:9/30
training batch:    20, loss: 0.00016, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    40, loss: 0.00019, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    60, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    80, loss: 0.00445, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   100, loss: 0.00168, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   120, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   140, loss: 0.00039, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   160, loss: 0.00020, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   180, loss: 0.00023, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   200, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   220, loss: 0.00028, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   240, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   260, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   280, loss: 0.00018, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   300, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   320, loss: 0.05938, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   340, loss: 0.00043, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   360, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   380, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   400, loss: 0.00026, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   420, loss: 0.00030, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   440, loss: 0.00023, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   460, loss: 0.00044, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   480, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   500, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   520, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   540, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   560, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   580, loss: 2.40383, precision: 0.933 recall: 1.000 f1: 0.966 
training batch:   600, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   620, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   640, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   660, loss: 0.04937, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   680, loss: 0.00031, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   700, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   720, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   740, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   760, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   780, loss: 0.00021, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   800, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   820, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   840, loss: 0.00058, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   860, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   880, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   900, loss: 0.01517, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   920, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   940, loss: 0.00033, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   960, loss: 0.00028, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   980, loss: 6.66203, precision: 0.933 recall: 0.933 f1: 0.933 
training batch:  1000, loss: 0.00024, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1020, loss: 0.46941, precision: 0.944 recall: 0.895 f1: 0.919 
training batch:  1040, loss: 0.00021, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1060, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1080, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1100, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1120, loss: 0.00017, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1140, loss: 0.00015, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1160, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1180, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1200, loss: 0.00023, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1220, loss: 2.37297, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1240, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1260, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1280, loss: 3.20125, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1300, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1320, loss: 0.00015, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1340, loss: 0.86049, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1360, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1380, loss: 0.00026, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1400, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1420, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1440, loss: 5.06437, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1460, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1480, loss: 10.30441, precision: 0.941 recall: 0.889 f1: 0.914 
training batch:  1500, loss: 0.00030, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1520, loss: 0.00031, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1540, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1560, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1580, loss: 0.00035, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1600, loss: 0.00895, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1620, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1640, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1660, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1680, loss: 0.00053, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1700, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1720, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1740, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1760, loss: 0.00015, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1780, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1800, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1820, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1840, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1860, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1880, loss: 0.00015, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1900, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1920, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1940, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1960, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1980, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2000, loss: 0.00027, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2020, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2040, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2060, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2080, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2100, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2120, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2140, loss: 0.00017, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2160, loss: 0.00023, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2180, loss: 0.02442, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2200, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2220, loss: 1.30765, precision: 0.917 recall: 0.917 f1: 0.917 
training batch:  2240, loss: 0.00103, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2260, loss: 0.00067, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2280, loss: 0.00015, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2300, loss: 0.00015, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2320, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2340, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2360, loss: 0.02080, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2380, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2400, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2420, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2440, loss: 0.00158, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2460, loss: 0.00104, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2480, loss: 0.00020, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2500, loss: 0.00135, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2520, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2540, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2560, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2580, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2600, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2620, loss: 0.03765, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2640, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2660, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2680, loss: 0.00035, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2700, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2720, loss: 0.00020, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2740, loss: 0.00030, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2760, loss: 0.00045, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2780, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2800, loss: 0.00036, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2820, loss: 0.00015, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2840, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2860, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2880, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2900, loss: 0.98581, precision: 1.000 recall: 0.947 f1: 0.973 
training batch:  2920, loss: 0.52377, precision: 0.929 recall: 0.929 f1: 0.929 
training batch:  2940, loss: 0.80412, precision: 0.944 recall: 1.000 f1: 0.971 
training batch:  2960, loss: 0.00443, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2980, loss: 0.00028, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3000, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3020, loss: 2.13117, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3040, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3060, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3080, loss: 0.00037, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3100, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3120, loss: 0.00055, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3140, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3160, loss: 0.00027, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3180, loss: 4.29189, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3200, loss: 0.00018, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3220, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3240, loss: 0.00020, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3260, loss: 0.92045, precision: 0.833 recall: 0.909 f1: 0.870 
training batch:  3280, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3300, loss: 0.00020, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3320, loss: 0.00015, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3340, loss: 0.00018, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3360, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3380, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3400, loss: 0.00024, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3420, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3440, loss: 2.07210, precision: 0.947 recall: 1.000 f1: 0.973 
training batch:  3460, loss: 0.02883, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3480, loss: 0.00018, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3500, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3520, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3540, loss: 0.00159, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3560, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3580, loss: 0.00020, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3600, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3620, loss: 0.00018, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3640, loss: 0.00020, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3660, loss: 0.20154, precision: 0.900 recall: 0.900 f1: 0.900 
training batch:  3680, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3700, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3720, loss: 1.43890, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3740, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3760, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3780, loss: 0.00037, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3800, loss: 0.00015, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3820, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3840, loss: 0.00019, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3860, loss: 0.00027, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3880, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3900, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3920, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3940, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3960, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3980, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4000, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4020, loss: 0.00018, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4040, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4060, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4080, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4100, loss: 0.00018, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4120, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4140, loss: 0.09955, precision: 0.933 recall: 0.933 f1: 0.933 
training batch:  4160, loss: 0.00122, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4180, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4200, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4220, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4240, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4260, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4280, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4300, loss: 0.00022, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4320, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4340, loss: 1.95089, precision: 0.917 recall: 0.846 f1: 0.880 
training batch:  4360, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4380, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4400, loss: 0.00030, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4420, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4440, loss: 0.00030, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4460, loss: 0.00024, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4480, loss: 0.00023, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4500, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4520, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4540, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4560, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4580, loss: 0.00038, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4600, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4620, loss: 0.00189, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4640, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4660, loss: 0.00036, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4680, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4700, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
start evaluate engines...
label: ORG, precision: 0.748 recall: 0.775 f1: 0.756 
label: PERSON, precision: 0.824 recall: 0.818 f1: 0.819 
label: LOC, precision: 0.849 recall: 0.865 f1: 0.852 
time consumption:21.37(min), precision: 0.943 recall: 0.959 f1: 0.950 
loading data...
epoch:10/30
training batch:    20, loss: 0.00013, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    40, loss: 0.00024, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    60, loss: 0.00021, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    80, loss: 0.00057, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   100, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   120, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   140, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   160, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   180, loss: 0.00020, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   200, loss: 1.39636, precision: 0.933 recall: 1.000 f1: 0.966 
training batch:   220, loss: 0.00013, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   240, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   260, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   280, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   300, loss: 0.00016, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   320, loss: 0.00309, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   340, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   360, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   380, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   400, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   420, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   440, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   460, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   480, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   500, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   520, loss: 0.00027, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   540, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   560, loss: -0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   580, loss: 1.77897, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   600, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   620, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   640, loss: 0.00023, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   660, loss: 0.00020, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   680, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   700, loss: 0.00016, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   720, loss: 0.00013, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   740, loss: 0.00013, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   760, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   780, loss: 0.00786, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   800, loss: 0.00018, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   820, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   840, loss: 0.00508, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   860, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   880, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   900, loss: 0.93587, precision: 0.917 recall: 0.917 f1: 0.917 
training batch:   920, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   940, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   960, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   980, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1000, loss: 0.00017, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1020, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1040, loss: 0.00035, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1060, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1080, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1100, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1120, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1140, loss: 0.00026, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1160, loss: 0.00015, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1180, loss: 0.00095, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1200, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1220, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1240, loss: 0.00015, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1260, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1280, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1300, loss: 2.20161, precision: 0.909 recall: 0.909 f1: 0.909 
training batch:  1320, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1340, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1360, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1380, loss: -0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1400, loss: 0.00045, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1420, loss: 0.00018, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1440, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1460, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1480, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1500, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1520, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1540, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1560, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1580, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1600, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1620, loss: 0.00016, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1640, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1660, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1680, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1700, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1720, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1740, loss: 0.00026, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1760, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1780, loss: 0.00068, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1800, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1820, loss: 0.02758, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1840, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1860, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1880, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1900, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1920, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1940, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1960, loss: 9.17758, precision: 0.889 recall: 0.889 f1: 0.889 
training batch:  1980, loss: 0.00021, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2000, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2020, loss: 0.00019, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2040, loss: 0.00046, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2060, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2080, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2100, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2120, loss: 0.01067, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2140, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2160, loss: 2.64224, precision: 1.000 recall: 0.917 f1: 0.957 
training batch:  2180, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2200, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2220, loss: 0.00016, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2240, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2260, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2280, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2300, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2320, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2340, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2360, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2380, loss: 0.71515, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2400, loss: 6.51025, precision: 1.000 recall: 0.950 f1: 0.974 
training batch:  2420, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2440, loss: 2.26121, precision: 0.923 recall: 0.923 f1: 0.923 
training batch:  2460, loss: 0.00013, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2480, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2500, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2520, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2540, loss: 0.00029, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2560, loss: 0.00041, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2580, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2600, loss: 0.00063, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2620, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2640, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2660, loss: 0.14366, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2680, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2700, loss: 0.26681, precision: 0.889 recall: 0.889 f1: 0.889 
training batch:  2720, loss: 3.72342, precision: 1.000 recall: 0.929 f1: 0.963 
training batch:  2740, loss: 6.75110, precision: 0.923 recall: 0.923 f1: 0.923 
training batch:  2760, loss: 0.55234, precision: 0.941 recall: 1.000 f1: 0.970 
training batch:  2780, loss: 0.03174, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2800, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2820, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2840, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2860, loss: 0.00039, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2880, loss: 0.00023, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2900, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2920, loss: 0.01695, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2940, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2960, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2980, loss: 0.00027, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3000, loss: 1.14346, precision: 0.909 recall: 0.909 f1: 0.909 
training batch:  3020, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3040, loss: 0.01200, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3060, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3080, loss: 2.87771, precision: 0.947 recall: 0.900 f1: 0.923 
training batch:  3100, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3120, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3140, loss: 0.00033, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3160, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3180, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3200, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3220, loss: 0.00045, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3240, loss: 0.00015, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3260, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3280, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3300, loss: 0.48431, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3320, loss: 0.00022, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3340, loss: 0.00013, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3360, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3380, loss: 0.00041, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3400, loss: 0.00029, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3420, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3440, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3460, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3480, loss: 0.55091, precision: 1.000 recall: 0.929 f1: 0.963 
training batch:  3500, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3520, loss: 0.00862, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3540, loss: 0.02338, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3560, loss: 0.00033, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3580, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3600, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3620, loss: 0.00060, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3640, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3660, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3680, loss: 0.07764, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3700, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3720, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3740, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3760, loss: 0.00034, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3780, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3800, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3820, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3840, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3860, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3880, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3900, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3920, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3940, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3960, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3980, loss: 0.00023, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4000, loss: 0.00016, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4020, loss: 0.00015, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4040, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4060, loss: 0.00113, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4080, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4100, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4120, loss: 0.00013, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4140, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4160, loss: 0.00026, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4180, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4200, loss: 6.26257, precision: 0.947 recall: 0.947 f1: 0.947 
training batch:  4220, loss: 0.00052, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4240, loss: 0.00273, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4260, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4280, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4300, loss: 0.00217, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4320, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4340, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4360, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4380, loss: 0.00020, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4400, loss: 0.00018, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4420, loss: 3.30757, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4440, loss: 0.45335, precision: 1.000 recall: 0.933 f1: 0.966 
training batch:  4460, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4480, loss: 0.00025, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4500, loss: 0.21091, precision: 0.900 recall: 0.947 f1: 0.923 
training batch:  4520, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4540, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4560, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4580, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4600, loss: 0.00016, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4620, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4640, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4660, loss: 0.00023, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4680, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4700, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
start evaluate engines...
label: ORG, precision: 0.759 recall: 0.774 f1: 0.761 
label: PERSON, precision: 0.825 recall: 0.821 f1: 0.821 
label: LOC, precision: 0.867 recall: 0.867 f1: 0.863 
time consumption:21.51(min), precision: 0.952 recall: 0.958 f1: 0.954 
loading data...
epoch:11/30
training batch:    20, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    40, loss: 0.00029, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    60, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    80, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   100, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   120, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   140, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   160, loss: 0.00051, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   180, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   200, loss: 0.00017, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   220, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   240, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   260, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   280, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   300, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   320, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   340, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   360, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   380, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   400, loss: 0.00033, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   420, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   440, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   460, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   480, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   500, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   520, loss: 0.03201, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   540, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   560, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   580, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   600, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   620, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   640, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   660, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   680, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   700, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   720, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   740, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   760, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   780, loss: 0.00034, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   800, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   820, loss: 0.00020, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   840, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   860, loss: 1.34268, precision: 0.909 recall: 0.909 f1: 0.909 
training batch:   880, loss: 0.00058, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   900, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   920, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   940, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   960, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   980, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1000, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1020, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1040, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1060, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1080, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1100, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1120, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1140, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1160, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1180, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1200, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1220, loss: 0.00962, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1240, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1260, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1280, loss: 0.00023, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1300, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1320, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1340, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1360, loss: 0.00058, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1380, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1400, loss: 0.00336, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1420, loss: 0.00033, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1440, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1460, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1480, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1500, loss: 4.22716, precision: 0.909 recall: 1.000 f1: 0.952 
training batch:  1520, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1540, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1560, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1580, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1600, loss: 0.04497, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1620, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1640, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1660, loss: 2.02623, precision: 0.923 recall: 1.000 f1: 0.960 
training batch:  1680, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1700, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1720, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1740, loss: 0.00028, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1760, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1780, loss: 0.03248, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1800, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1820, loss: 3.23357, precision: 1.000 recall: 0.941 f1: 0.970 
training batch:  1840, loss: 0.02197, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1860, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1880, loss: 0.00015, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1900, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1920, loss: 0.00086, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1940, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1960, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1980, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2000, loss: 0.00758, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2020, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2040, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2060, loss: -0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2080, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2100, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2120, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2140, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2160, loss: 0.00027, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2180, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2200, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2220, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2240, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2260, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2280, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2300, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2320, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2340, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2360, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2380, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2400, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2420, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2440, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2460, loss: 0.00029, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2480, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2500, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2520, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2540, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2560, loss: 0.00069, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2580, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2600, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2620, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2640, loss: 0.00056, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2660, loss: 0.00042, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2680, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2700, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2720, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2740, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2760, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2780, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2800, loss: 0.00292, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2820, loss: 0.50119, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2840, loss: 0.00051, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2860, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2880, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2900, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2920, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2940, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2960, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2980, loss: 0.00020, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3000, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3020, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3040, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3060, loss: 0.00013, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3080, loss: 2.14050, precision: 0.929 recall: 0.929 f1: 0.929 
training batch:  3100, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3120, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3140, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3160, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3180, loss: 1.44654, precision: 0.900 recall: 0.900 f1: 0.900 
training batch:  3200, loss: 0.00026, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3220, loss: 0.00013, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3240, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3260, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3280, loss: 0.00249, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3300, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3320, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3340, loss: 0.00017, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3360, loss: 0.00089, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3380, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3400, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3420, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3440, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3460, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3480, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3500, loss: 0.00086, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3520, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3540, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3560, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3580, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3600, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3620, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3640, loss: 0.00017, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3660, loss: 0.00042, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3680, loss: 0.00139, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3700, loss: 0.37293, precision: 0.933 recall: 0.933 f1: 0.933 
training batch:  3720, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3740, loss: 10.42907, precision: 0.889 recall: 0.889 f1: 0.889 
training batch:  3760, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3780, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3800, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3820, loss: 0.00028, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3840, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3860, loss: 0.00072, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3880, loss: 0.00019, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3900, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3920, loss: 0.00045, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3940, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3960, loss: 0.00043, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3980, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4000, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4020, loss: 1.16267, precision: 0.909 recall: 0.833 f1: 0.870 
training batch:  4040, loss: 0.00085, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4060, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4080, loss: 0.00016, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4100, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4120, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4140, loss: 0.00229, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4160, loss: 0.00452, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4180, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4200, loss: 0.04720, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4220, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4240, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4260, loss: 0.00032, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4280, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4300, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4320, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4340, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4360, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4380, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4400, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4420, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4440, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4460, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4480, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4500, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4520, loss: 0.00017, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4540, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4560, loss: 0.00018, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4580, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4600, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4620, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4640, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4660, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4680, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4700, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
start evaluate engines...
label: ORG, precision: 0.768 recall: 0.779 f1: 0.768 
label: PERSON, precision: 0.829 recall: 0.818 f1: 0.821 
label: LOC, precision: 0.862 recall: 0.874 f1: 0.863 
time consumption:21.51(min), precision: 0.955 recall: 0.959 f1: 0.956 
saved the new best model with f1: 0.956
loading data...
epoch:12/30
training batch:    20, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    40, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    60, loss: 0.00020, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    80, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   100, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   120, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   140, loss: 0.00013, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   160, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   180, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   200, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   220, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   240, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   260, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   280, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   300, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   320, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   340, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   360, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   380, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   400, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   420, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   440, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   460, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   480, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   500, loss: 0.00464, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   520, loss: 0.00029, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   540, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   560, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   580, loss: 0.00015, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   600, loss: 0.00706, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   620, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   640, loss: 0.00026, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   660, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   680, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   700, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   720, loss: 0.00044, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   740, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   760, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   780, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   800, loss: 0.00032, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   820, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   840, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   860, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   880, loss: -0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   900, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   920, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   940, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   960, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   980, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1000, loss: 0.00098, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1020, loss: 0.00418, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1040, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1060, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1080, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1100, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1120, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1140, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1160, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1180, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1200, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1220, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1240, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1260, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1280, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1300, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1320, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1340, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1360, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1380, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1400, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1420, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1440, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1460, loss: 0.00439, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1480, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1500, loss: 0.21854, precision: 0.950 recall: 0.950 f1: 0.950 
training batch:  1520, loss: 2.30241, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1540, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1560, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1580, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1600, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1620, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1640, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1660, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1680, loss: 0.00041, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1700, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1720, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1740, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1760, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1780, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1800, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1820, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1840, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1860, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1880, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1900, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1920, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1940, loss: 1.00825, precision: 0.944 recall: 0.944 f1: 0.944 
training batch:  1960, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1980, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2000, loss: -0.00025, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2020, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2040, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2060, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2080, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2100, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2120, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2140, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2160, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2180, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2200, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2220, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2240, loss: 0.00048, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2260, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2280, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2300, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2320, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2340, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2360, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2380, loss: 0.00062, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2400, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2420, loss: 0.00025, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2440, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2460, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2480, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2500, loss: 1.17470, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2520, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2540, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2560, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2580, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2600, loss: 0.03189, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2620, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2640, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2660, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2680, loss: 0.79645, precision: 0.889 recall: 0.889 f1: 0.889 
training batch:  2700, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2720, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2740, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2760, loss: 0.00023, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2780, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2800, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2820, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2840, loss: 0.00016, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2860, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2880, loss: 0.00023, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2900, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2920, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2940, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2960, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2980, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3000, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3020, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3040, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3060, loss: 0.00013, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3080, loss: 0.97680, precision: 0.857 recall: 0.923 f1: 0.889 
training batch:  3100, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3120, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3140, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3160, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3180, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3200, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3220, loss: 0.00039, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3240, loss: 0.00099, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3260, loss: 0.00116, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3280, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3300, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3320, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3340, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3360, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3380, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3400, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3420, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3440, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3460, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3480, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3500, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3520, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3540, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3560, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3580, loss: 2.29560, precision: 0.947 recall: 1.000 f1: 0.973 
training batch:  3600, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3620, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3640, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3660, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3680, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3700, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3720, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3740, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3760, loss: 0.00013, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3780, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3800, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3820, loss: 0.36319, precision: 0.923 recall: 0.923 f1: 0.923 
training batch:  3840, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3860, loss: -0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3880, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3900, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3920, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3940, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3960, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3980, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4000, loss: -0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4020, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4040, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4060, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4080, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4100, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4120, loss: 0.00037, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4140, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4160, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4180, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4200, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4220, loss: 0.00016, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4240, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4260, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4280, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4300, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4320, loss: 0.00013, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4340, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4360, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4380, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4400, loss: 0.00030, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4420, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4440, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4460, loss: 0.00413, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4480, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4500, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4520, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4540, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4560, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4580, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4600, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4620, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4640, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4660, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4680, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4700, loss: 0.00015, precision: 1.000 recall: 1.000 f1: 1.000 
start evaluate engines...
label: ORG, precision: 0.765 recall: 0.770 f1: 0.764 
label: PERSON, precision: 0.831 recall: 0.823 f1: 0.825 
label: LOC, precision: 0.857 recall: 0.876 f1: 0.862 
time consumption:21.54(min), precision: 0.955 recall: 0.960 f1: 0.957 
saved the new best model with f1: 0.957
loading data...
epoch:13/30
training batch:    20, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    40, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    60, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    80, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   100, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   120, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   140, loss: 0.00028, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   160, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   180, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   200, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   220, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   240, loss: -0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   260, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   280, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   300, loss: 1.13496, precision: 0.917 recall: 0.917 f1: 0.917 
training batch:   320, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   340, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   360, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   380, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   400, loss: 0.01828, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   420, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   440, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   460, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   480, loss: 0.27624, precision: 0.889 recall: 0.941 f1: 0.914 
training batch:   500, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   520, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   540, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   560, loss: 0.04750, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   580, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   600, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   620, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   640, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   660, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   680, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   700, loss: 7.11506, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   720, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   740, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   760, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   780, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   800, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   820, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   840, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   860, loss: 0.12466, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   880, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   900, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   920, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   940, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   960, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   980, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1000, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1020, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1040, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1060, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1080, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1100, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1120, loss: 0.00203, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1140, loss: 0.00013, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1160, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1180, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1200, loss: 0.00021, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1220, loss: 0.59536, precision: 0.944 recall: 1.000 f1: 0.971 
training batch:  1240, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1260, loss: 0.00020, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1280, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1300, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1320, loss: 0.00013, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1340, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1360, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1380, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1400, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1420, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1440, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1460, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1480, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1500, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1520, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1540, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1560, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1580, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1600, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1620, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1640, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1660, loss: 0.00017, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1680, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1700, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1720, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1740, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1760, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1780, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1800, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1820, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1840, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1860, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1880, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1900, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1920, loss: -0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1940, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1960, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1980, loss: 0.00386, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2000, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2020, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2040, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2060, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2080, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2100, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2120, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2140, loss: 0.00031, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2160, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2180, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2200, loss: 0.00218, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2220, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2240, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2260, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2280, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2300, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2320, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2340, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2360, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2380, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2400, loss: 0.03463, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2420, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2440, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2460, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2480, loss: 0.00020, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2500, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2520, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2540, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2560, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2580, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2600, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2620, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2640, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2660, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2680, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2700, loss: 0.23023, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2720, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2740, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2760, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2780, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2800, loss: 0.00030, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2820, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2840, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2860, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2880, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2900, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2920, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2940, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2960, loss: 0.00027, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2980, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3000, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3020, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3040, loss: 1.09953, precision: 0.909 recall: 0.909 f1: 0.909 
training batch:  3060, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3080, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3100, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3120, loss: 0.00020, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3140, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3160, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3180, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3200, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3220, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3240, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3260, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3280, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3300, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3320, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3340, loss: 0.00038, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3360, loss: 0.00017, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3380, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3400, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3420, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3440, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3460, loss: 0.00019, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3480, loss: 0.00016, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3500, loss: 0.00034, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3520, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3540, loss: -0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3560, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3580, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3600, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3620, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3640, loss: 0.00022, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3660, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3680, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3700, loss: 4.65826, precision: 0.933 recall: 0.933 f1: 0.933 
training batch:  3720, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3740, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3760, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3780, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3800, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3820, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3840, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3860, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3880, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3900, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3920, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3940, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3960, loss: -0.00056, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3980, loss: 0.00052, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4000, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4020, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4040, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4060, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4080, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4100, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4120, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4140, loss: 0.00371, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4160, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4180, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4200, loss: 0.00019, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4220, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4240, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4260, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4280, loss: 0.00015, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4300, loss: 2.87679, precision: 0.929 recall: 0.867 f1: 0.897 
training batch:  4320, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4340, loss: 0.03749, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4360, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4380, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4400, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4420, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4440, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4460, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4480, loss: 0.00033, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4500, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4520, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4540, loss: 0.00013, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4560, loss: 1.21835, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4580, loss: 0.07708, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4600, loss: 0.00022, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4620, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4640, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4660, loss: 5.15149, precision: 0.938 recall: 0.938 f1: 0.938 
training batch:  4680, loss: 0.00018, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4700, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
start evaluate engines...
label: ORG, precision: 0.767 recall: 0.762 f1: 0.760 
label: PERSON, precision: 0.824 recall: 0.823 f1: 0.822 
label: LOC, precision: 0.858 recall: 0.877 f1: 0.862 
time consumption:21.53(min), precision: 0.953 recall: 0.958 f1: 0.955 
loading data...
epoch:14/30
training batch:    20, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    40, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    60, loss: 0.00072, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    80, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   100, loss: 0.00071, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   120, loss: 1.84369, precision: 0.900 recall: 0.818 f1: 0.857 
training batch:   140, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   160, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   180, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   200, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   220, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   240, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   260, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   280, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   300, loss: 3.47223, precision: 0.889 recall: 1.000 f1: 0.941 
training batch:   320, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   340, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   360, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   380, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   400, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   420, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   440, loss: 0.05122, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   460, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   480, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   500, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   520, loss: -0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   540, loss: 0.00025, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   560, loss: 0.00015, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   580, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   600, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   620, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   640, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   660, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   680, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   700, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   720, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   740, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   760, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   780, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   800, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   820, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   840, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   860, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   880, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   900, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   920, loss: 2.78210, precision: 0.938 recall: 0.938 f1: 0.938 
training batch:   940, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   960, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   980, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1000, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1020, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1040, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1060, loss: -0.00023, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1080, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1100, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1120, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1140, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1160, loss: 0.01160, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1180, loss: 0.00066, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1200, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1220, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1240, loss: 0.00022, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1260, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1280, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1300, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1320, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1340, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1360, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1380, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1400, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1420, loss: -0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1440, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1460, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1480, loss: 0.00019, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1500, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1520, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1540, loss: 0.28466, precision: 0.957 recall: 1.000 f1: 0.978 
training batch:  1560, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1580, loss: 0.00013, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1600, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1620, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1640, loss: 0.00031, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1660, loss: 0.00020, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1680, loss: 0.00040, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1700, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1720, loss: 0.00038, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1740, loss: 0.00047, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1760, loss: 0.98761, precision: 0.947 recall: 1.000 f1: 0.973 
training batch:  1780, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1800, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1820, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1840, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1860, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1880, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1900, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1920, loss: 0.00017, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1940, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1960, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1980, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2000, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2020, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2040, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2060, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2080, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2100, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2120, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2140, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2160, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2180, loss: 0.00053, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2200, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2220, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2240, loss: 0.01807, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2260, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2280, loss: 0.00042, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2300, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2320, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2340, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2360, loss: 0.00016, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2380, loss: 0.00019, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2400, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2420, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2440, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2460, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2480, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2500, loss: 0.00073, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2520, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2540, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2560, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2580, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2600, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2620, loss: 2.89174, precision: 0.929 recall: 1.000 f1: 0.963 
training batch:  2640, loss: 0.00019, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2660, loss: 0.00019, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2680, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2700, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2720, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2740, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2760, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2780, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2800, loss: 3.43513, precision: 0.938 recall: 1.000 f1: 0.968 
training batch:  2820, loss: 0.00646, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2840, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2860, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2880, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2900, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2920, loss: 0.00016, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2940, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2960, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2980, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3000, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3020, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3040, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3060, loss: 0.00013, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3080, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3100, loss: 0.00023, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3120, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3140, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3160, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3180, loss: 5.64732, precision: 0.929 recall: 1.000 f1: 0.963 
training batch:  3200, loss: 0.00015, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3220, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3240, loss: 5.10796, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3260, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3280, loss: 0.00025, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3300, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3320, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3340, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3360, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3380, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3400, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3420, loss: 0.00501, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3440, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3460, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3480, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3500, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3520, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3540, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3560, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3580, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3600, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3620, loss: 0.00059, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3640, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3660, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3680, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3700, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3720, loss: 8.43746, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3740, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3760, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3780, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3800, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3820, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3840, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3860, loss: 0.66002, precision: 0.923 recall: 1.000 f1: 0.960 
training batch:  3880, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3900, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3920, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3940, loss: 0.00023, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3960, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3980, loss: 0.00272, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4000, loss: 0.36638, precision: 0.933 recall: 0.875 f1: 0.903 
training batch:  4020, loss: 1.57491, precision: 0.923 recall: 1.000 f1: 0.960 
training batch:  4040, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4060, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4080, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4100, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4120, loss: 0.00024, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4140, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4160, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4180, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4200, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4220, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4240, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4260, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4280, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4300, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4320, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4340, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4360, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4380, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4400, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4420, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4440, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4460, loss: 0.01685, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4480, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4500, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4520, loss: -0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4540, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4560, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4580, loss: 4.53984, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4600, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4620, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4640, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4660, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4680, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4700, loss: 6.87905, precision: 1.000 recall: 1.000 f1: 1.000 
start evaluate engines...
label: ORG, precision: 0.755 recall: 0.773 f1: 0.759 
label: PERSON, precision: 0.828 recall: 0.821 f1: 0.822 
label: LOC, precision: 0.867 recall: 0.868 f1: 0.864 
time consumption:21.24(min), precision: 0.954 recall: 0.958 f1: 0.955 
loading data...
epoch:15/30
training batch:    20, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    40, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    60, loss: -0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    80, loss: -0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   100, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   120, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   140, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   160, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   180, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   200, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   220, loss: 0.00020, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   240, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   260, loss: 0.00035, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   280, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   300, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   320, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   340, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   360, loss: 0.00056, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   380, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   400, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   420, loss: 0.00188, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   440, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   460, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   480, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   500, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   520, loss: 0.00013, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   540, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   560, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   580, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   600, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   620, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   640, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   660, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   680, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   700, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   720, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   740, loss: 0.00136, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   760, loss: 2.68106, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   780, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   800, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   820, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   840, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   860, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   880, loss: 0.00019, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   900, loss: 0.00071, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   920, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   940, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   960, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   980, loss: -0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1000, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1020, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1040, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1060, loss: 0.17584, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1080, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1100, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1120, loss: 0.00016, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1140, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1160, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1180, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1200, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1220, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1240, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1260, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1280, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1300, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1320, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1340, loss: 0.00023, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1360, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1380, loss: 0.00025, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1400, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1420, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1440, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1460, loss: 0.00015, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1480, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1500, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1520, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1540, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1560, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1580, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1600, loss: 0.02061, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1620, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1640, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1660, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1680, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1700, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1720, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1740, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1760, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1780, loss: 0.00031, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1800, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1820, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1840, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1860, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1880, loss: 0.02109, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1900, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1920, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1940, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1960, loss: -0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1980, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2000, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2020, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2040, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2060, loss: 0.01388, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2080, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2100, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2120, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2140, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2160, loss: 1.78441, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2180, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2200, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2220, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2240, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2260, loss: 0.00931, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2280, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2300, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2320, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2340, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2360, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2380, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2400, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2420, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2440, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2460, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2480, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2500, loss: 0.00026, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2520, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2540, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2560, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2580, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2600, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2620, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2640, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2660, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2680, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2700, loss: 0.03640, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2720, loss: 0.00020, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2740, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2760, loss: 3.20096, precision: 0.933 recall: 1.000 f1: 0.966 
training batch:  2780, loss: 0.01348, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2800, loss: -0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2820, loss: 2.07732, precision: 0.957 recall: 0.957 f1: 0.957 
training batch:  2840, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2860, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2880, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2900, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2920, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2940, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2960, loss: 1.88463, precision: 1.000 recall: 0.941 f1: 0.970 
training batch:  2980, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3000, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3020, loss: 0.00025, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3040, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3060, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3080, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3100, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3120, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3140, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3160, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3180, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3200, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3220, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3240, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3260, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3280, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3300, loss: 0.00062, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3320, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3340, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3360, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3380, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3400, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3420, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3440, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3460, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3480, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3500, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3520, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3540, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3560, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3580, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3600, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3620, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3640, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3660, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3680, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3700, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3720, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3740, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3760, loss: 0.16733, precision: 0.917 recall: 1.000 f1: 0.957 
training batch:  3780, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3800, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3820, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3840, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3860, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3880, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3900, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3920, loss: 0.00488, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3940, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3960, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3980, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4000, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4020, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4040, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4060, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4080, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4100, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4120, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4140, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4160, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4180, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4200, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4220, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4240, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4260, loss: 0.00017, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4280, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4300, loss: 0.00244, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4320, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4340, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4360, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4380, loss: 0.00013, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4400, loss: -0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4420, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4440, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4460, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4480, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4500, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4520, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4540, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4560, loss: 0.00031, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4580, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4600, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4620, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4640, loss: 0.00030, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4660, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4680, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4700, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
start evaluate engines...
label: ORG, precision: 0.769 recall: 0.772 f1: 0.765 
label: PERSON, precision: 0.824 recall: 0.826 f1: 0.823 
label: LOC, precision: 0.867 recall: 0.871 f1: 0.865 
time consumption:21.38(min), precision: 0.957 recall: 0.959 f1: 0.958 
saved the new best model with f1: 0.958
loading data...
epoch:16/30
training batch:    20, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    40, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    60, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    80, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   100, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   120, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   140, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   160, loss: 1.57640, precision: 1.000 recall: 0.917 f1: 0.957 
training batch:   180, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   200, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   220, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   240, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   260, loss: 0.00389, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   280, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   300, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   320, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   340, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   360, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   380, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   400, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   420, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   440, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   460, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   480, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   500, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   520, loss: 0.49900, precision: 0.933 recall: 0.933 f1: 0.933 
training batch:   540, loss: 0.51117, precision: 0.917 recall: 0.917 f1: 0.917 
training batch:   560, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   580, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   600, loss: 0.00052, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   620, loss: 0.00037, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   640, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   660, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   680, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   700, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   720, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   740, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   760, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   780, loss: 0.00013, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   800, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   820, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   840, loss: -0.00019, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   860, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   880, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   900, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   920, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   940, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   960, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   980, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1000, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1020, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1040, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1060, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1080, loss: 2.08698, precision: 1.000 recall: 0.955 f1: 0.977 
training batch:  1100, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1120, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1140, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1160, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1180, loss: 0.01204, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1200, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1220, loss: 0.00027, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1240, loss: 3.74446, precision: 0.857 recall: 0.923 f1: 0.889 
training batch:  1260, loss: 0.09011, precision: 0.889 recall: 1.000 f1: 0.941 
training batch:  1280, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1300, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1320, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1340, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1360, loss: 7.82509, precision: 0.917 recall: 1.000 f1: 0.957 
training batch:  1380, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1400, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1420, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1440, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1460, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1480, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1500, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1520, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1540, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1560, loss: -0.00013, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1580, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1600, loss: 0.00018, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1620, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1640, loss: 0.01752, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1660, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1680, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1700, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1720, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1740, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1760, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1780, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1800, loss: 2.57606, precision: 0.917 recall: 0.917 f1: 0.917 
training batch:  1820, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1840, loss: -0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1860, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1880, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1900, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1920, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1940, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1960, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1980, loss: 0.00022, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2000, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2020, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2040, loss: 0.04139, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2060, loss: 1.39459, precision: 0.933 recall: 0.933 f1: 0.933 
training batch:  2080, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2100, loss: 0.00013, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2120, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2140, loss: 0.00018, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2160, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2180, loss: -0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2200, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2220, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2240, loss: -0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2260, loss: 0.00015, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2280, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2300, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2320, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2340, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2360, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2380, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2400, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2420, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2440, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2460, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2480, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2500, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2520, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2540, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2560, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2580, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2600, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2620, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2640, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2660, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2680, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2700, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2720, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2740, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2760, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2780, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2800, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2820, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2840, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2860, loss: 0.00062, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2880, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2900, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2920, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2940, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2960, loss: 1.47929, precision: 0.929 recall: 0.929 f1: 0.929 
training batch:  2980, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3000, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3020, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3040, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3060, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3080, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3100, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3120, loss: -0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3140, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3160, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3180, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3200, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3220, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3240, loss: 0.00487, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3260, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3280, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3300, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3320, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3340, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3360, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3380, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3400, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3420, loss: 0.00033, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3440, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3460, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3480, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3500, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3520, loss: 0.23667, precision: 1.000 recall: 0.950 f1: 0.974 
training batch:  3540, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3560, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3580, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3600, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3620, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3640, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3660, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3680, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3700, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3720, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3740, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3760, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3780, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3800, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3820, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3840, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3860, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3880, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3900, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3920, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3940, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3960, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3980, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4000, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4020, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4040, loss: 0.00024, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4060, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4080, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4100, loss: 0.00025, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4120, loss: 0.00142, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4140, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4160, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4180, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4200, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4220, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4240, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4260, loss: 0.00026, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4280, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4300, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4320, loss: 0.17465, precision: 1.000 recall: 0.923 f1: 0.960 
training batch:  4340, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4360, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4380, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4400, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4420, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4440, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4460, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4480, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4500, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4520, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4540, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4560, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4580, loss: 0.00019, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4600, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4620, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4640, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4660, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4680, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4700, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
start evaluate engines...
label: ORG, precision: 0.770 recall: 0.774 f1: 0.768 
label: PERSON, precision: 0.821 recall: 0.825 f1: 0.821 
label: LOC, precision: 0.865 recall: 0.876 f1: 0.867 
time consumption:21.50(min), precision: 0.955 recall: 0.963 f1: 0.958 
saved the new best model with f1: 0.958
loading data...
epoch:17/30
training batch:    20, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    40, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    60, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    80, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   100, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   120, loss: 0.43237, precision: 0.909 recall: 0.909 f1: 0.909 
training batch:   140, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   160, loss: -0.00025, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   180, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   200, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   220, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   240, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   260, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   280, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   300, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   320, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   340, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   360, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   380, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   400, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   420, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   440, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   460, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   480, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   500, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   520, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   540, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   560, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   580, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   600, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   620, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   640, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   660, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   680, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   700, loss: 4.53477, precision: 0.900 recall: 0.900 f1: 0.900 
training batch:   720, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   740, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   760, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   780, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   800, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   820, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   840, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   860, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   880, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   900, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   920, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   940, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   960, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   980, loss: -0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1000, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1020, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1040, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1060, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1080, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1100, loss: 0.46950, precision: 0.909 recall: 0.833 f1: 0.870 
training batch:  1120, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1140, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1160, loss: -0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1180, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1200, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1220, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1240, loss: 0.00026, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1260, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1280, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1300, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1320, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1340, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1360, loss: 0.00015, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1380, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1400, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1420, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1440, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1460, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1480, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1500, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1520, loss: 0.03678, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1540, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1560, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1580, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1600, loss: -0.00016, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1620, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1640, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1660, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1680, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1700, loss: 0.00160, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1720, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1740, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1760, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1780, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1800, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1820, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1840, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1860, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1880, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1900, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1920, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1940, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1960, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1980, loss: -0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2000, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2020, loss: 0.41856, precision: 1.000 recall: 0.923 f1: 0.960 
training batch:  2040, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2060, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2080, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2100, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2120, loss: 0.00018, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2140, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2160, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2180, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2200, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2220, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2240, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2260, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2280, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2300, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2320, loss: 4.23443, precision: 1.000 recall: 0.933 f1: 0.966 
training batch:  2340, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2360, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2380, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2400, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2420, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2440, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2460, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2480, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2500, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2520, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2540, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2560, loss: 0.00021, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2580, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2600, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2620, loss: -0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2640, loss: 0.00044, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2660, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2680, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2700, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2720, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2740, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2760, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2780, loss: 6.01711, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2800, loss: 0.00018, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2820, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2840, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2860, loss: -0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2880, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2900, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2920, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2940, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2960, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2980, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3000, loss: 0.01271, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3020, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3040, loss: -0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3060, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3080, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3100, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3120, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3140, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3160, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3180, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3200, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3220, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3240, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3260, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3280, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3300, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3320, loss: 1.57261, precision: 0.938 recall: 0.938 f1: 0.938 
training batch:  3340, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3360, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3380, loss: 0.00016, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3400, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3420, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3440, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3460, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3480, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3500, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3520, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3540, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3560, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3580, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3600, loss: 0.00021, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3620, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3640, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3660, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3680, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3700, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3720, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3740, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3760, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3780, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3800, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3820, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3840, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3860, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3880, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3900, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3920, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3940, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3960, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3980, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4000, loss: 0.00019, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4020, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4040, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4060, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4080, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4100, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4120, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4140, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4160, loss: 0.97614, precision: 0.923 recall: 0.923 f1: 0.923 
training batch:  4180, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4200, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4220, loss: 0.00022, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4240, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4260, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4280, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4300, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4320, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4340, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4360, loss: -0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4380, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4400, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4420, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4440, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4460, loss: 0.00303, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4480, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4500, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4520, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4540, loss: 1.56059, precision: 0.909 recall: 1.000 f1: 0.952 
training batch:  4560, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4580, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4600, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4620, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4640, loss: 0.00013, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4660, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4680, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4700, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
start evaluate engines...
label: ORG, precision: 0.771 recall: 0.776 f1: 0.769 
label: PERSON, precision: 0.827 recall: 0.812 f1: 0.818 
label: LOC, precision: 0.857 recall: 0.877 f1: 0.863 
time consumption:21.50(min), precision: 0.956 recall: 0.960 f1: 0.957 
loading data...
epoch:18/30
training batch:    20, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    40, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    60, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    80, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   100, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   120, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   140, loss: 0.00032, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   160, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   180, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   200, loss: -0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   220, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   240, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   260, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   280, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   300, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   320, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   340, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   360, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   380, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   400, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   420, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   440, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   460, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   480, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   500, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   520, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   540, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   560, loss: 0.00891, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   580, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   600, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   620, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   640, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   660, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   680, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   700, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   720, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   740, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   760, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   780, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   800, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   820, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   840, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   860, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   880, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   900, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   920, loss: -0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   940, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   960, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   980, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1000, loss: -0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1020, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1040, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1060, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1080, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1100, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1120, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1140, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1160, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1180, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1200, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1220, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1240, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1260, loss: 1.46093, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1280, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1300, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1320, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1340, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1360, loss: 0.00285, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1380, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1400, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1420, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1440, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1460, loss: 0.00013, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1480, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1500, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1520, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1540, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1560, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1580, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1600, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1620, loss: 0.42260, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1640, loss: 0.00013, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1660, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1680, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1700, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1720, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1740, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1760, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1780, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1800, loss: 0.00668, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1820, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1840, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1860, loss: 0.00013, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1880, loss: 0.00018, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1900, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1920, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1940, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1960, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1980, loss: 1.02378, precision: 0.962 recall: 1.000 f1: 0.980 
training batch:  2000, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2020, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2040, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2060, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2080, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2100, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2120, loss: 0.64729, precision: 0.929 recall: 0.929 f1: 0.929 
training batch:  2140, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2160, loss: 0.71704, precision: 0.818 recall: 0.900 f1: 0.857 
training batch:  2180, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2200, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2220, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2240, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2260, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2280, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2300, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2320, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2340, loss: 0.00270, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2360, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2380, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2400, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2420, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2440, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2460, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2480, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2500, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2520, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2540, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2560, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2580, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2600, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2620, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2640, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2660, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2680, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2700, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2720, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2740, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2760, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2780, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2800, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2820, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2840, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2860, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2880, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2900, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2920, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2940, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2960, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2980, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3000, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3020, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3040, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3060, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3080, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3100, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3120, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3140, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3160, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3180, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3200, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3220, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3240, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3260, loss: 0.05872, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3280, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3300, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3320, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3340, loss: -0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3360, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3380, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3400, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3420, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3440, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3460, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3480, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3500, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3520, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3540, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3560, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3580, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3600, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3620, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3640, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3660, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3680, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3700, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3720, loss: 0.00013, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3740, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3760, loss: 0.00025, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3780, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3800, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3820, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3840, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3860, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3880, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3900, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3920, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3940, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3960, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3980, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4000, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4020, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4040, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4060, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4080, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4100, loss: -0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4120, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4140, loss: -0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4160, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4180, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4200, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4220, loss: -0.00015, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4240, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4260, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4280, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4300, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4320, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4340, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4360, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4380, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4400, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4420, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4440, loss: 7.49728, precision: 1.000 recall: 0.909 f1: 0.952 
training batch:  4460, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4480, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4500, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4520, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4540, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4560, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4580, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4600, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4620, loss: 1.00903, precision: 0.909 recall: 0.909 f1: 0.909 
training batch:  4640, loss: -0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4660, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4680, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4700, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
start evaluate engines...
label: ORG, precision: 0.774 recall: 0.777 f1: 0.771 
label: PERSON, precision: 0.828 recall: 0.822 f1: 0.823 
label: LOC, precision: 0.867 recall: 0.875 f1: 0.867 
time consumption:21.44(min), precision: 0.960 recall: 0.961 f1: 0.959 
saved the new best model with f1: 0.959
loading data...
epoch:19/30
training batch:    20, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    40, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    60, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    80, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   100, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   120, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   140, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   160, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   180, loss: 0.00241, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   200, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   220, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   240, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   260, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   280, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   300, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   320, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   340, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   360, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   380, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   400, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   420, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   440, loss: -0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   460, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   480, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   500, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   520, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   540, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   560, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   580, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   600, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   620, loss: 0.00052, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   640, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   660, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   680, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   700, loss: -0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   720, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   740, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   760, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   780, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   800, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   820, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   840, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   860, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   880, loss: 2.65727, precision: 0.952 recall: 0.952 f1: 0.952 
training batch:   900, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   920, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   940, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   960, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   980, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1000, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1020, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1040, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1060, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1080, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1100, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1120, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1140, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1160, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1180, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1200, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1220, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1240, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1260, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1280, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1300, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1320, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1340, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1360, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1380, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1400, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1420, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1440, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1460, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1480, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1500, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1520, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1540, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1560, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1580, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1600, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1620, loss: 0.00020, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1640, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1660, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1680, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1700, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1720, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1740, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1760, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1780, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1800, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1820, loss: -0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1840, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1860, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1880, loss: -0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1900, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1920, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1940, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1960, loss: -0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1980, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2000, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2020, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2040, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2060, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2080, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2100, loss: -0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2120, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2140, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2160, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2180, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2200, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2220, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2240, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2260, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2280, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2300, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2320, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2340, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2360, loss: -0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2380, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2400, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2420, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2440, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2460, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2480, loss: 0.00022, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2500, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2520, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2540, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2560, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2580, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2600, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2620, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2640, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2660, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2680, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2700, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2720, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2740, loss: 0.00045, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2760, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2780, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2800, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2820, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2840, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2860, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2880, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2900, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2920, loss: 2.42133, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2940, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2960, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2980, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3000, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3020, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3040, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3060, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3080, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3100, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3120, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3140, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3160, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3180, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3200, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3220, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3240, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3260, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3280, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3300, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3320, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3340, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3360, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3380, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3400, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3420, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3440, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3460, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3480, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3500, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3520, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3540, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3560, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3580, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3600, loss: -0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3620, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3640, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3660, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3680, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3700, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3720, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3740, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3760, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3780, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3800, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3820, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3840, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3860, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3880, loss: -0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3900, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3920, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3940, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3960, loss: 0.00028, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3980, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4000, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4020, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4040, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4060, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4080, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4100, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4120, loss: 4.12418, precision: 0.938 recall: 0.938 f1: 0.938 
training batch:  4140, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4160, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4180, loss: 0.00022, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4200, loss: 6.22967, precision: 0.933 recall: 0.933 f1: 0.933 
training batch:  4220, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4240, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4260, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4280, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4300, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4320, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4340, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4360, loss: 0.00017, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4380, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4400, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4420, loss: 2.23553, precision: 1.000 recall: 0.909 f1: 0.952 
training batch:  4440, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4460, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4480, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4500, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4520, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4540, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4560, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4580, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4600, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4620, loss: 3.09067, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4640, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4660, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4680, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4700, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
start evaluate engines...
label: ORG, precision: 0.767 recall: 0.778 f1: 0.767 
label: PERSON, precision: 0.822 recall: 0.819 f1: 0.819 
label: LOC, precision: 0.863 recall: 0.872 f1: 0.863 
time consumption:21.43(min), precision: 0.954 recall: 0.961 f1: 0.957 
loading data...
epoch:20/30
training batch:    20, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    40, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    60, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    80, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   100, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   120, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   140, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   160, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   180, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   200, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   220, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   240, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   260, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   280, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   300, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   320, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   340, loss: 0.00031, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   360, loss: 0.00028, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   380, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   400, loss: -0.00019, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   420, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   440, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   460, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   480, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   500, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   520, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   540, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   560, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   580, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   600, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   620, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   640, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   660, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   680, loss: -0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   700, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   720, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   740, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   760, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   780, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   800, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   820, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   840, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   860, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   880, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   900, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   920, loss: 0.27199, precision: 1.000 recall: 0.923 f1: 0.960 
training batch:   940, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   960, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   980, loss: 0.00070, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1000, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1020, loss: 0.00026, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1040, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1060, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1080, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1100, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1120, loss: -0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1140, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1160, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1180, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1200, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1220, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1240, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1260, loss: 0.00375, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1280, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1300, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1320, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1340, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1360, loss: 0.90187, precision: 0.857 recall: 0.923 f1: 0.889 
training batch:  1380, loss: 1.17757, precision: 0.917 recall: 1.000 f1: 0.957 
training batch:  1400, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1420, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1440, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1460, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1480, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1500, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1520, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1540, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1560, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1580, loss: 0.00057, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1600, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1620, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1640, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1660, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1680, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1700, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1720, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1740, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1760, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1780, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1800, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1820, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1840, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1860, loss: -0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1880, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1900, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1920, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1940, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1960, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1980, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2000, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2020, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2040, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2060, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2080, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2100, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2120, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2140, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2160, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2180, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2200, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2220, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2240, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2260, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2280, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2300, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2320, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2340, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2360, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2380, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2400, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2420, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2440, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2460, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2480, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2500, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2520, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2540, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2560, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2580, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2600, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2620, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2640, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2660, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2680, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2700, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2720, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2740, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2760, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2780, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2800, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2820, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2840, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2860, loss: 0.00041, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2880, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2900, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2920, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2940, loss: -0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2960, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2980, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3000, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3020, loss: -0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3040, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3060, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3080, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3100, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3120, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3140, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3160, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3180, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3200, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3220, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3240, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3260, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3280, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3300, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3320, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3340, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3360, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3380, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3400, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3420, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3440, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3460, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3480, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3500, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3520, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3540, loss: -0.00031, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3560, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3580, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3600, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3620, loss: -0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3640, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3660, loss: 0.00018, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3680, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3700, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3720, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3740, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3760, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3780, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3800, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3820, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3840, loss: 0.00023, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3860, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3880, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3900, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3920, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3940, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3960, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3980, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4000, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4020, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4040, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4060, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4080, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4100, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4120, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4140, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4160, loss: 0.84582, precision: 0.963 recall: 1.000 f1: 0.981 
training batch:  4180, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4200, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4220, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4240, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4260, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4280, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4300, loss: -0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4320, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4340, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4360, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4380, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4400, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4420, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4440, loss: 0.00039, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4460, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4480, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4500, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4520, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4540, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4560, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4580, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4600, loss: -0.00016, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4620, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4640, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4660, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4680, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4700, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
start evaluate engines...
label: ORG, precision: 0.768 recall: 0.777 f1: 0.769 
label: PERSON, precision: 0.825 recall: 0.820 f1: 0.821 
label: LOC, precision: 0.862 recall: 0.875 f1: 0.864 
time consumption:21.50(min), precision: 0.956 recall: 0.961 f1: 0.958 
loading data...
epoch:21/30
training batch:    20, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    40, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    60, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    80, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   100, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   120, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   140, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   160, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   180, loss: -0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   200, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   220, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   240, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   260, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   280, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   300, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   320, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   340, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   360, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   380, loss: 2.80546, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   400, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   420, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   440, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   460, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   480, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   500, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   520, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   540, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   560, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   580, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   600, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   620, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   640, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   660, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   680, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   700, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   720, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   740, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   760, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   780, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   800, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   820, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   840, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   860, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   880, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   900, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   920, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   940, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   960, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   980, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1000, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1020, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1040, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1060, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1080, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1100, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1120, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1140, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1160, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1180, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1200, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1220, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1240, loss: 0.00036, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1260, loss: 0.63593, precision: 0.944 recall: 1.000 f1: 0.971 
training batch:  1280, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1300, loss: 0.00015, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1320, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1340, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1360, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1380, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1400, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1420, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1440, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1460, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1480, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1500, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1520, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1540, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1560, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1580, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1600, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1620, loss: -0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1640, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1660, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1680, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1700, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1720, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1740, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1760, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1780, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1800, loss: -0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1820, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1840, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1860, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1880, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1900, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1920, loss: 0.00043, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1940, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1960, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1980, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2000, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2020, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2040, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2060, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2080, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2100, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2120, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2140, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2160, loss: 0.00022, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2180, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2200, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2220, loss: -0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2240, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2260, loss: -0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2280, loss: -0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2300, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2320, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2340, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2360, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2380, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2400, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2420, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2440, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2460, loss: -0.00020, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2480, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2500, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2520, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2540, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2560, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2580, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2600, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2620, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2640, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2660, loss: -0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2680, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2700, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2720, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2740, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2760, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2780, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2800, loss: 0.64717, precision: 0.955 recall: 1.000 f1: 0.977 
training batch:  2820, loss: -0.00015, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2840, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2860, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2880, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2900, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2920, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2940, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2960, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2980, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3000, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3020, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3040, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3060, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3080, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3100, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3120, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3140, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3160, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3180, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3200, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3220, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3240, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3260, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3280, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3300, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3320, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3340, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3360, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3380, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3400, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3420, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3440, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3460, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3480, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3500, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3520, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3540, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3560, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3580, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3600, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3620, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3640, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3660, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3680, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3700, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3720, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3740, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3760, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3780, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3800, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3820, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3840, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3860, loss: 1.34265, precision: 0.917 recall: 0.917 f1: 0.917 
training batch:  3880, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3900, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3920, loss: 0.06184, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3940, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3960, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3980, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4000, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4020, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4040, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4060, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4080, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4100, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4120, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4140, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4160, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4180, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4200, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4220, loss: -0.00025, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4240, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4260, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4280, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4300, loss: 0.00029, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4320, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4340, loss: 0.00149, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4360, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4380, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4400, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4420, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4440, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4460, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4480, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4500, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4520, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4540, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4560, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4580, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4600, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4620, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4640, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4660, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4680, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4700, loss: -0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
start evaluate engines...
label: ORG, precision: 0.768 recall: 0.773 f1: 0.767 
label: PERSON, precision: 0.829 recall: 0.826 f1: 0.826 
label: LOC, precision: 0.866 recall: 0.876 f1: 0.867 
time consumption:21.52(min), precision: 0.958 recall: 0.962 f1: 0.959 
loading data...
epoch:22/30
training batch:    20, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    40, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    60, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    80, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   100, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   120, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   140, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   160, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   180, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   200, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   220, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   240, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   260, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   280, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   300, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   320, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   340, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   360, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   380, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   400, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   420, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   440, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   460, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   480, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   500, loss: 0.00023, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   520, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   540, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   560, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   580, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   600, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   620, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   640, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   660, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   680, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   700, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   720, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   740, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   760, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   780, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   800, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   820, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   840, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   860, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   880, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   900, loss: -0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   920, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   940, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   960, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   980, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1000, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1020, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1040, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1060, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1080, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1100, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1120, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1140, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1160, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1180, loss: -0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1200, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1220, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1240, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1260, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1280, loss: 0.65785, precision: 0.929 recall: 0.929 f1: 0.929 
training batch:  1300, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1320, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1340, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1360, loss: 0.00077, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1380, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1400, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1420, loss: -0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1440, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1460, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1480, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1500, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1520, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1540, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1560, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1580, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1600, loss: -0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1620, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1640, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1660, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1680, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1700, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1720, loss: -0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1740, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1760, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1780, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1800, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1820, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1840, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1860, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1880, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1900, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1920, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1940, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1960, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1980, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2000, loss: 0.00013, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2020, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2040, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2060, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2080, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2100, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2120, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2140, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2160, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2180, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2200, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2220, loss: -0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2240, loss: -0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2260, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2280, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2300, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2320, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2340, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2360, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2380, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2400, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2420, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2440, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2460, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2480, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2500, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2520, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2540, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2560, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2580, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2600, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2620, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2640, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2660, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2680, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2700, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2720, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2740, loss: -0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2760, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2780, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2800, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2820, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2840, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2860, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2880, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2900, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2920, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2940, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2960, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2980, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3000, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3020, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3040, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3060, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3080, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3100, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3120, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3140, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3160, loss: -0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3180, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3200, loss: -0.00013, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3220, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3240, loss: 0.00218, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3260, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3280, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3300, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3320, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3340, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3360, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3380, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3400, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3420, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3440, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3460, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3480, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3500, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3520, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3540, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3560, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3580, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3600, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3620, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3640, loss: 0.00022, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3660, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3680, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3700, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3720, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3740, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3760, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3780, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3800, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3820, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3840, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3860, loss: -0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3880, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3900, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3920, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3940, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3960, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3980, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4000, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4020, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4040, loss: 0.81934, precision: 0.944 recall: 0.944 f1: 0.944 
training batch:  4060, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4080, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4100, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4120, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4140, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4160, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4180, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4200, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4220, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4240, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4260, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4280, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4300, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4320, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4340, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4360, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4380, loss: -0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4400, loss: 0.00160, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4420, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4440, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4460, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4480, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4500, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4520, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4540, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4560, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4580, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4600, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4620, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4640, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4660, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4680, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4700, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
start evaluate engines...
label: ORG, precision: 0.765 recall: 0.771 f1: 0.764 
label: PERSON, precision: 0.828 recall: 0.822 f1: 0.823 
label: LOC, precision: 0.864 recall: 0.874 f1: 0.864 
time consumption:21.49(min), precision: 0.959 recall: 0.959 f1: 0.959 
loading data...
epoch:23/30
training batch:    20, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    40, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    60, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    80, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   100, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   120, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   140, loss: -0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   160, loss: -0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   180, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   200, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   220, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   240, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   260, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   280, loss: 0.00106, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   300, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   320, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   340, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   360, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   380, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   400, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   420, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   440, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   460, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   480, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   500, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   520, loss: -0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   540, loss: -0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   560, loss: -0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   580, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   600, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   620, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   640, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   660, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   680, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   700, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   720, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   740, loss: 7.31590, precision: 0.846 recall: 0.917 f1: 0.880 
training batch:   760, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   780, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   800, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   820, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   840, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   860, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   880, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   900, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   920, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   940, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   960, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   980, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1000, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1020, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1040, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1060, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1080, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1100, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1120, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1140, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1160, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1180, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1200, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1220, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1240, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1260, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1280, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1300, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1320, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1340, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1360, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1380, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1400, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1420, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1440, loss: 0.42862, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1460, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1480, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1500, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1520, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1540, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1560, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1580, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1600, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1620, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1640, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1660, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1680, loss: 0.00680, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1700, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1720, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1740, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1760, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1780, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1800, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1820, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1840, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1860, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1880, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1900, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1920, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1940, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1960, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1980, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2000, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2020, loss: 0.00013, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2040, loss: -0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2060, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2080, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2100, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2120, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2140, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2160, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2180, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2200, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2220, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2240, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2260, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2280, loss: -0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2300, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2320, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2340, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2360, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2380, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2400, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2420, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2440, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2460, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2480, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2500, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2520, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2540, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2560, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2580, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2600, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2620, loss: 0.00782, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2640, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2660, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2680, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2700, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2720, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2740, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2760, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2780, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2800, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2820, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2840, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2860, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2880, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2900, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2920, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2940, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2960, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2980, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3000, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3020, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3040, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3060, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3080, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3100, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3120, loss: 0.00086, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3140, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3160, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3180, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3200, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3220, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3240, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3260, loss: 0.00019, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3280, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3300, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3320, loss: 1.11833, precision: 0.962 recall: 1.000 f1: 0.980 
training batch:  3340, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3360, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3380, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3400, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3420, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3440, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3460, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3480, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3500, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3520, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3540, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3560, loss: 2.61388, precision: 0.917 recall: 0.917 f1: 0.917 
training batch:  3580, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3600, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3620, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3640, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3660, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3680, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3700, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3720, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3740, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3760, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3780, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3800, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3820, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3840, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3860, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3880, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3900, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3920, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3940, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3960, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3980, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4000, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4020, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4040, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4060, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4080, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4100, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4120, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4140, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4160, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4180, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4200, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4220, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4240, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4260, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4280, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4300, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4320, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4340, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4360, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4380, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4400, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4420, loss: -0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4440, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4460, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4480, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4500, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4520, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4540, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4560, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4580, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4600, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4620, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4640, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4660, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4680, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4700, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
start evaluate engines...
label: ORG, precision: 0.763 recall: 0.777 f1: 0.765 
label: PERSON, precision: 0.825 recall: 0.823 f1: 0.822 
label: LOC, precision: 0.867 recall: 0.872 f1: 0.865 
time consumption:21.50(min), precision: 0.956 recall: 0.960 f1: 0.957 
loading data...
epoch:24/30
training batch:    20, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    40, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    60, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    80, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   100, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   120, loss: -0.00013, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   140, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   160, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   180, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   200, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   220, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   240, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   260, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   280, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   300, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   320, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   340, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   360, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   380, loss: -0.00013, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   400, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   420, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   440, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   460, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   480, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   500, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   520, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   540, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   560, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   580, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   600, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   620, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   640, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   660, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   680, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   700, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   720, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   740, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   760, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   780, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   800, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   820, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   840, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   860, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   880, loss: -0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   900, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   920, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   940, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   960, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   980, loss: 0.00013, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1000, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1020, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1040, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1060, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1080, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1100, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1120, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1140, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1160, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1180, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1200, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1220, loss: 0.00026, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1240, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1260, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1280, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1300, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1320, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1340, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1360, loss: -0.00016, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1380, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1400, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1420, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1440, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1460, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1480, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1500, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1520, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1540, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1560, loss: 0.00013, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1580, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1600, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1620, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1640, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1660, loss: 0.00016, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1680, loss: 2.81209, precision: 0.938 recall: 1.000 f1: 0.968 
training batch:  1700, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1720, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1740, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1760, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1780, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1800, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1820, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1840, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1860, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1880, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1900, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1920, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1940, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1960, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1980, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2000, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2020, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2040, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2060, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2080, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2100, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2120, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2140, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2160, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2180, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2200, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2220, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2240, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2260, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2280, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2300, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2320, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2340, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2360, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2380, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2400, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2420, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2440, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2460, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2480, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2500, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2520, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2540, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2560, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2580, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2600, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2620, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2640, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2660, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2680, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2700, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2720, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2740, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2760, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2780, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2800, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2820, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2840, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2860, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2880, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2900, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2920, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2940, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2960, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2980, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3000, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3020, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3040, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3060, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3080, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3100, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3120, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3140, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3160, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3180, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3200, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3220, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3240, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3260, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3280, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3300, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3320, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3340, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3360, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3380, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3400, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3420, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3440, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3460, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3480, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3500, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3520, loss: 0.00040, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3540, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3560, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3580, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3600, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3620, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3640, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3660, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3680, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3700, loss: -0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3720, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3740, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3760, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3780, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3800, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3820, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3840, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3860, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3880, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3900, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3920, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3940, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3960, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3980, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4000, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4020, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4040, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4060, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4080, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4100, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4120, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4140, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4160, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4180, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4200, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4220, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4240, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4260, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4280, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4300, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4320, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4340, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4360, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4380, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4400, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4420, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4440, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4460, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4480, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4500, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4520, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4540, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4560, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4580, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4600, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4620, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4640, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4660, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4680, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4700, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
start evaluate engines...
label: ORG, precision: 0.767 recall: 0.773 f1: 0.766 
label: PERSON, precision: 0.824 recall: 0.823 f1: 0.821 
label: LOC, precision: 0.862 recall: 0.876 f1: 0.865 
time consumption:21.48(min), precision: 0.955 recall: 0.962 f1: 0.958 
loading data...
epoch:25/30
training batch:    20, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    40, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    60, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    80, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   100, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   120, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   140, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   160, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   180, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   200, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   220, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   240, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   260, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   280, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   300, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   320, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   340, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   360, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   380, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   400, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   420, loss: -0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   440, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   460, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   480, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   500, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   520, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   540, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   560, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   580, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   600, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   620, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   640, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   660, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   680, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   700, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   720, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   740, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   760, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   780, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   800, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   820, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   840, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   860, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   880, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   900, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   920, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   940, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   960, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   980, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1000, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1020, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1040, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1060, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1080, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1100, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1120, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1140, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1160, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1180, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1200, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1220, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1240, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1260, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1280, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1300, loss: 0.00018, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1320, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1340, loss: 14.48856, precision: 1.000 recall: 0.952 f1: 0.976 
training batch:  1360, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1380, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1400, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1420, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1440, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1460, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1480, loss: -0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1500, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1520, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1540, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1560, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1580, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1600, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1620, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1640, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1660, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1680, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1700, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1720, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1740, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1760, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1780, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1800, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1820, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1840, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1860, loss: -0.00017, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1880, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1900, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1920, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1940, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1960, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1980, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2000, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2020, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2040, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2060, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2080, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2100, loss: 0.00019, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2120, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2140, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2160, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2180, loss: -0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2200, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2220, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2240, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2260, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2280, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2300, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2320, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2340, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2360, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2380, loss: -0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2400, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2420, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2440, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2460, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2480, loss: 0.00013, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2500, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2520, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2540, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2560, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2580, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2600, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2620, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2640, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2660, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2680, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2700, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2720, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2740, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2760, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2780, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2800, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2820, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2840, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2860, loss: 0.02653, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2880, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2900, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2920, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2940, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2960, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2980, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3000, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3020, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3040, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3060, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3080, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3100, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3120, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3140, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3160, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3180, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3200, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3220, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3240, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3260, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3280, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3300, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3320, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3340, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3360, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3380, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3400, loss: 0.75806, precision: 0.889 recall: 1.000 f1: 0.941 
training batch:  3420, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3440, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3460, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3480, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3500, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3520, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3540, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3560, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3580, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3600, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3620, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3640, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3660, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3680, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3700, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3720, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3740, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3760, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3780, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3800, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3820, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3840, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3860, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3880, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3900, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3920, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3940, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3960, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3980, loss: 2.30020, precision: 0.962 recall: 0.962 f1: 0.962 
training batch:  4000, loss: -0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4020, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4040, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4060, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4080, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4100, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4120, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4140, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4160, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4180, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4200, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4220, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4240, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4260, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4280, loss: -0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4300, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4320, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4340, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4360, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4380, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4400, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4420, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4440, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4460, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4480, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4500, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4520, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4540, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4560, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4580, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4600, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4620, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4640, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4660, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4680, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4700, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
start evaluate engines...
label: ORG, precision: 0.766 recall: 0.777 f1: 0.768 
label: PERSON, precision: 0.826 recall: 0.823 f1: 0.822 
label: LOC, precision: 0.869 recall: 0.874 f1: 0.867 
time consumption:21.51(min), precision: 0.958 recall: 0.963 f1: 0.960 
saved the new best model with f1: 0.960
loading data...
epoch:26/30
training batch:    20, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    40, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    60, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    80, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   100, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   120, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   140, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   160, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   180, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   200, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   220, loss: 0.00028, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   240, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   260, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   280, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   300, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   320, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   340, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   360, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   380, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   400, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   420, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   440, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   460, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   480, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   500, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   520, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   540, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   560, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   580, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   600, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   620, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   640, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   660, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   680, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   700, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   720, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   740, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   760, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   780, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   800, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   820, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   840, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   860, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   880, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   900, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   920, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   940, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   960, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   980, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1000, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1020, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1040, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1060, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1080, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1100, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1120, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1140, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1160, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1180, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1200, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1220, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1240, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1260, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1280, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1300, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1320, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1340, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1360, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1380, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1400, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1420, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1440, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1460, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1480, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1500, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1520, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1540, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1560, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1580, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1600, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1620, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1640, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1660, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1680, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1700, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1720, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1740, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1760, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1780, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1800, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1820, loss: -0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1840, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1860, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1880, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1900, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1920, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1940, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1960, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1980, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2000, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2020, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2040, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2060, loss: -0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2080, loss: -0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2100, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2120, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2140, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2160, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2180, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2200, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2220, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2240, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2260, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2280, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2300, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2320, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2340, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2360, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2380, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2400, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2420, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2440, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2460, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2480, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2500, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2520, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2540, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2560, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2580, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2600, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2620, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2640, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2660, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2680, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2700, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2720, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2740, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2760, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2780, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2800, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2820, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2840, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2860, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2880, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2900, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2920, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2940, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2960, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2980, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3000, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3020, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3040, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3060, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3080, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3100, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3120, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3140, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3160, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3180, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3200, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3220, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3240, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3260, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3280, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3300, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3320, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3340, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3360, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3380, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3400, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3420, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3440, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3460, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3480, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3500, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3520, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3540, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3560, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3580, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3600, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3620, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3640, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3660, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3680, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3700, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3720, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3740, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3760, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3780, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3800, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3820, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3840, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3860, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3880, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3900, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3920, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3940, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3960, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3980, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4000, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4020, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4040, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4060, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4080, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4100, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4120, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4140, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4160, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4180, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4200, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4220, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4240, loss: -0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4260, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4280, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4300, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4320, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4340, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4360, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4380, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4400, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4420, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4440, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4460, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4480, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4500, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4520, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4540, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4560, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4580, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4600, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4620, loss: 0.85460, precision: 0.944 recall: 0.944 f1: 0.944 
training batch:  4640, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4660, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4680, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4700, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
start evaluate engines...
label: ORG, precision: 0.768 recall: 0.779 f1: 0.769 
label: PERSON, precision: 0.829 recall: 0.818 f1: 0.821 
label: LOC, precision: 0.863 recall: 0.872 f1: 0.863 
time consumption:21.45(min), precision: 0.959 recall: 0.961 f1: 0.959 
loading data...
epoch:27/30
training batch:    20, loss: 1.57550, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    40, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    60, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    80, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   100, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   120, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   140, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   160, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   180, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   200, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   220, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   240, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   260, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   280, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   300, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   320, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   340, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   360, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   380, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   400, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   420, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   440, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   460, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   480, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   500, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   520, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   540, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   560, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   580, loss: -0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   600, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   620, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   640, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   660, loss: -0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   680, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   700, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   720, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   740, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   760, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   780, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   800, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   820, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   840, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   860, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   880, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   900, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   920, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   940, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   960, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   980, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1000, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1020, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1040, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1060, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1080, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1100, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1120, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1140, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1160, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1180, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1200, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1220, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1240, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1260, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1280, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1300, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1320, loss: -0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1340, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1360, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1380, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1400, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1420, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1440, loss: -0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1460, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1480, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1500, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1520, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1540, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1560, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1580, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1600, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1620, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1640, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1660, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1680, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1700, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1720, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1740, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1760, loss: 0.78394, precision: 0.923 recall: 0.923 f1: 0.923 
training batch:  1780, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1800, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1820, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1840, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1860, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1880, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1900, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1920, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1940, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1960, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1980, loss: -0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2000, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2020, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2040, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2060, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2080, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2100, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2120, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2140, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2160, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2180, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2200, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2220, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2240, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2260, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2280, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2300, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2320, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2340, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2360, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2380, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2400, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2420, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2440, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2460, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2480, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2500, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2520, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2540, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2560, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2580, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2600, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2620, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2640, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2660, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2680, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2700, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2720, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2740, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2760, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2780, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2800, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2820, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2840, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2860, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2880, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2900, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2920, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2940, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2960, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2980, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3000, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3020, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3040, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3060, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3080, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3100, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3120, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3140, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3160, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3180, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3200, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3220, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3240, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3260, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3280, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3300, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3320, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3340, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3360, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3380, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3400, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3420, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3440, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3460, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3480, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3500, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3520, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3540, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3560, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3580, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3600, loss: -0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3620, loss: -0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3640, loss: -0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3660, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3680, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3700, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3720, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3740, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3760, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3780, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3800, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3820, loss: -0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3840, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3860, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3880, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3900, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3920, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3940, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3960, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3980, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4000, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4020, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4040, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4060, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4080, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4100, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4120, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4140, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4160, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4180, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4200, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4220, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4240, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4260, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4280, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4300, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4320, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4340, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4360, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4380, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4400, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4420, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4440, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4460, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4480, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4500, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4520, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4540, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4560, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4580, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4600, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4620, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4640, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4660, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4680, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4700, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
start evaluate engines...
label: ORG, precision: 0.766 recall: 0.780 f1: 0.769 
label: PERSON, precision: 0.826 recall: 0.822 f1: 0.822 
label: LOC, precision: 0.868 recall: 0.871 f1: 0.866 
time consumption:21.44(min), precision: 0.960 recall: 0.962 f1: 0.960 
saved the new best model with f1: 0.960
loading data...
epoch:28/30
training batch:    20, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    40, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    60, loss: -0.00013, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    80, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   100, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   120, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   140, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   160, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   180, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   200, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   220, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   240, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   260, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   280, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   300, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   320, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   340, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   360, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   380, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   400, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   420, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   440, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   460, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   480, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   500, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   520, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   540, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   560, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   580, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   600, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   620, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   640, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   660, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   680, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   700, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   720, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   740, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   760, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   780, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   800, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   820, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   840, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   860, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   880, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   900, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   920, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   940, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   960, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   980, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1000, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1020, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1040, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1060, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1080, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1100, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1120, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1140, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1160, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1180, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1200, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1220, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1240, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1260, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1280, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1300, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1320, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1340, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1360, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1380, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1400, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1420, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1440, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1460, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1480, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1500, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1520, loss: -0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1540, loss: -0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1560, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1580, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1600, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1620, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1640, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1660, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1680, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1700, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1720, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1740, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1760, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1780, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1800, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1820, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1840, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1860, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1880, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1900, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1920, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1940, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1960, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1980, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2000, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2020, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2040, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2060, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2080, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2100, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2120, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2140, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2160, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2180, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2200, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2220, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2240, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2260, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2280, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2300, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2320, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2340, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2360, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2380, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2400, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2420, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2440, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2460, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2480, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2500, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2520, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2540, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2560, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2580, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2600, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2620, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2640, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2660, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2680, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2700, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2720, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2740, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2760, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2780, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2800, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2820, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2840, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2860, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2880, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2900, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2920, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2940, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2960, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2980, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3000, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3020, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3040, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3060, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3080, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3100, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3120, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3140, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3160, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3180, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3200, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3220, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3240, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3260, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3280, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3300, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3320, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3340, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3360, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3380, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3400, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3420, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3440, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3460, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3480, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3500, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3520, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3540, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3560, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3580, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3600, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3620, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3640, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3660, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3680, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3700, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3720, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3740, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3760, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3780, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3800, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3820, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3840, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3860, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3880, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3900, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3920, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3940, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3960, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3980, loss: 0.00017, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4000, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4020, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4040, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4060, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4080, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4100, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4120, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4140, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4160, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4180, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4200, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4220, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4240, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4260, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4280, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4300, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4320, loss: 1.84343, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4340, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4360, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4380, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4400, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4420, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4440, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4460, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4480, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4500, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4520, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4540, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4560, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4580, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4600, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4620, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4640, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4660, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4680, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4700, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
start evaluate engines...
label: ORG, precision: 0.765 recall: 0.778 f1: 0.768 
label: PERSON, precision: 0.826 recall: 0.822 f1: 0.822 
label: LOC, precision: 0.860 recall: 0.872 f1: 0.862 
time consumption:21.50(min), precision: 0.955 recall: 0.962 f1: 0.958 
loading data...
epoch:29/30
training batch:    20, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    40, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    60, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    80, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   100, loss: 0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   120, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   140, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   160, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   180, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   200, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   220, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   240, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   260, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   280, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   300, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   320, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   340, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   360, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   380, loss: 0.00021, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   400, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   420, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   440, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   460, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   480, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   500, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   520, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   540, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   560, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   580, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   600, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   620, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   640, loss: -0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   660, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   680, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   700, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   720, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   740, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   760, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   780, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   800, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   820, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   840, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   860, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   880, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   900, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   920, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   940, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   960, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   980, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1000, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1020, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1040, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1060, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1080, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1100, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1120, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1140, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1160, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1180, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1200, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1220, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1240, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1260, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1280, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1300, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1320, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1340, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1360, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1380, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1400, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1420, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1440, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1460, loss: -0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1480, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1500, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1520, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1540, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1560, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1580, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1600, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1620, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1640, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1660, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1680, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1700, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1720, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1740, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1760, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1780, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1800, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1820, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1840, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1860, loss: 0.00014, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1880, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1900, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1920, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1940, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1960, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1980, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2000, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2020, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2040, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2060, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2080, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2100, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2120, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2140, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2160, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2180, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2200, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2220, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2240, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2260, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2280, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2300, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2320, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2340, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2360, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2380, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2400, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2420, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2440, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2460, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2480, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2500, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2520, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2540, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2560, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2580, loss: 0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2600, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2620, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2640, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2660, loss: 0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2680, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2700, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2720, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2740, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2760, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2780, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2800, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2820, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2840, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2860, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2880, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2900, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2920, loss: -0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2940, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2960, loss: -0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2980, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3000, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3020, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3040, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3060, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3080, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3100, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3120, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3140, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3160, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3180, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3200, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3220, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3240, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3260, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3280, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3300, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3320, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3340, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3360, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3380, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3400, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3420, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3440, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3460, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3480, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3500, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3520, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3540, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3560, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3580, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3600, loss: -0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3620, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3640, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3660, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3680, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3700, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3720, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3740, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3760, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3780, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3800, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3820, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3840, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3860, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3880, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3900, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3920, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3940, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3960, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3980, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4000, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4020, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4040, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4060, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4080, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4100, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4120, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4140, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4160, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4180, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4200, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4220, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4240, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4260, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4280, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4300, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4320, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4340, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4360, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4380, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4400, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4420, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4440, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4460, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4480, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4500, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4520, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4540, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4560, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4580, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4600, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4620, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4640, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4660, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4680, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4700, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
start evaluate engines...
label: ORG, precision: 0.770 recall: 0.776 f1: 0.769 
label: PERSON, precision: 0.827 recall: 0.822 f1: 0.823 
label: LOC, precision: 0.861 recall: 0.875 f1: 0.864 
time consumption:21.49(min), precision: 0.957 recall: 0.962 f1: 0.959 
loading data...
epoch:30/30
training batch:    20, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    40, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    60, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:    80, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   100, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   120, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   140, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   160, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   180, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   200, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   220, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   240, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   260, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   280, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   300, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   320, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   340, loss: -0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   360, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   380, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   400, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   420, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   440, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   460, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   480, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   500, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   520, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   540, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   560, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   580, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   600, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   620, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   640, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   660, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   680, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   700, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   720, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   740, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   760, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   780, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   800, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   820, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   840, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   860, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   880, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   900, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   920, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   940, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   960, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:   980, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1000, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1020, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1040, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1060, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1080, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1100, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1120, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1140, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1160, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1180, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1200, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1220, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1240, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1260, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1280, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1300, loss: 0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1320, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1340, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1360, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1380, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1400, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1420, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1440, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1460, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1480, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1500, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1520, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1540, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1560, loss: -0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1580, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1600, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1620, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1640, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1660, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1680, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1700, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1720, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1740, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1760, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1780, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1800, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1820, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1840, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1860, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1880, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1900, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1920, loss: 0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1940, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1960, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  1980, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2000, loss: -0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2020, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2040, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2060, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2080, loss: -0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2100, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2120, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2140, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2160, loss: -0.00008, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2180, loss: -0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2200, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2220, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2240, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2260, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2280, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2300, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2320, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2340, loss: -0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2360, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2380, loss: -0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2400, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2420, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2440, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2460, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2480, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2500, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2520, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2540, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2560, loss: -0.00011, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2580, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2600, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2620, loss: -0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2640, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2660, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2680, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2700, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2720, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2740, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2760, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2780, loss: -0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2800, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2820, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2840, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2860, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2880, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2900, loss: -0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2920, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2940, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2960, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  2980, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3000, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3020, loss: -0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3040, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3060, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3080, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3100, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3120, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3140, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3160, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3180, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3200, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3220, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3240, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3260, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3280, loss: 0.00015, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3300, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3320, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3340, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3360, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3380, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3400, loss: 0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3420, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3440, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3460, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3480, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3500, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3520, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3540, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3560, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3580, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3600, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3620, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3640, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3660, loss: 0.00010, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3680, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3700, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3720, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3740, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3760, loss: 0.00012, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3780, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3800, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3820, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3840, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3860, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3880, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3900, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3920, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3940, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3960, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  3980, loss: 0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4000, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4020, loss: 0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4040, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4060, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4080, loss: -0.00000, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4100, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4120, loss: 0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4140, loss: 0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4160, loss: 0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4180, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4200, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4220, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4240, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4260, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4280, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4300, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4320, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4340, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4360, loss: -0.00005, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4380, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4400, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4420, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4440, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4460, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4480, loss: -0.00009, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4500, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4520, loss: -0.00006, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4540, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4560, loss: -0.00001, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4580, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4600, loss: -0.00007, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4620, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4640, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4660, loss: -0.00003, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4680, loss: -0.00002, precision: 1.000 recall: 1.000 f1: 1.000 
training batch:  4700, loss: -0.00004, precision: 1.000 recall: 1.000 f1: 1.000 
start evaluate engines...
label: ORG, precision: 0.768 recall: 0.776 f1: 0.768 
label: PERSON, precision: 0.827 recall: 0.823 f1: 0.823 
label: LOC, precision: 0.864 recall: 0.874 f1: 0.865 
time consumption:21.48(min), precision: 0.957 recall: 0.962 f1: 0.959 
loading data...
overall best f1 is 0.9600654728664302 at 27 epoch
total training time consumption: 743.877(min)
